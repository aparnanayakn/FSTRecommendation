{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(\"./optimalalphabeta/dearr_cont/dearr0.1_0.06.csv\")\n",
    "#df1 = pd.read_csv(\"bordascores_top3.csv\")\n",
    "df2 = pd.read_csv(\"df_mfeatures.csv\")\n",
    "df3 = pd.read_csv(\"BinnedMetaFeatures.csv\")\n",
    "df1 = df1[['File','FeatureAlgo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = df1.merge(df2, on='File').merge(df3, on='File')\n",
    "merged_df.to_csv(\"KnowledgeBase.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"majority_voted_labels_clusteredearr.csv\")\n",
    "merged_df = df1.merge(df2, on='File').merge(df3, on='File')\n",
    "merged_df.to_csv(\"KnowledgeBase_majorityvotecluster.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(\"./Datafiles/KnowledgeBase.csv\")\n",
    "#df2 = pd.read_csv(\"KnowledgeBase_majorityvotecluster.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['File', 'FeatureAlgo', 'Completeness', 'MissColumns', 'Conciseness',\n",
       "       'SyntaxAccuracy', 'InvalidColumns', 'cor.mean', 'cor.sd', 'cov.mean',\n",
       "       ...\n",
       "       'nr_inst_bins', 'nr_num_bins', 'nr_bin_bins', 'nUnique_bins',\n",
       "       'attr_conc.mean_bins', 'attr_conc.sd_bins', 'attr_ent.mean_bins',\n",
       "       'attr_ent.sd_bins', 'cEntropy_bins', 'ena_bins'],\n",
       "      dtype='object', length=136)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char_raw1 = df1[[ 'cor.mean', 'cov.mean', 'eigenvalues.mean', 'g_mean.mean', 'h_mean.mean', 'iq_range.mean', \n",
    " 'kurtosis.mean', 'mad.mean', 'max.mean', 'mean.mean', 'median.mean', 'min.mean', 'nr_cor_attr', 'nr_norm', 'nr_outliers', 'range.mean', 'sd.mean', 'skewness.mean', 'sparsity.mean',\n",
    " 't_mean.mean', 'var.mean', 'attr_to_inst', 'inst_to_attr', 'nr_attr', 'nr_bin', 'nr_inst', 'nr_num','attr_conc.mean', \n",
    " 'attr_ent.mean', 'nUnique', 'ena', 'snr.mean', 'cEntropy', 'FeatureAlgo']]\n",
    "\n",
    "df_quality_raw1 = df1[['Completeness', 'Conciseness', 'ClassImbRatio', 'ClassOverlapPerc', 'OutlierPerc', 'LabelIssuesPerc', 'FeatureAlgo']]\n",
    "\n",
    "df_raw1 = df1[['Completeness', 'Conciseness', 'cor.mean', 'cov.mean', 'eigenvalues.mean', 'g_mean.mean', 'h_mean.mean', 'iq_range.mean', \n",
    " 'kurtosis.mean', 'mad.mean', 'max.mean', 'mean.mean', 'median.mean', 'min.mean', 'nr_cor_attr', 'nr_norm', 'nr_outliers', 'range.mean', 'sd.mean', 'skewness.mean', 'sparsity.mean',\n",
    " 't_mean.mean', 'var.mean', 'ClassImbRatio', 'ClassOverlapPerc', 'OutlierPerc', 'attr_to_inst', 'inst_to_attr', 'nr_attr', 'nr_bin', 'nr_inst', 'nr_num','attr_conc.mean', \n",
    " 'attr_ent.mean', 'LabelIssuesPerc','nUnique', 'ena', 'snr.mean', 'cEntropy', 'FeatureAlgo']]\n",
    "\n",
    "\n",
    "df_char_bin1 = df1[['cor.mean_bins', 'cov.mean_bins',\n",
    "       'eigenvalues.mean_bins', 'g_mean.mean_bins', 'h_mean.mean_bins',\n",
    "       'iq_range.mean_bins', 'kurtosis.mean_bins', 'mad.mean_bins',\n",
    "       'max.mean_bins', 'mean.mean_bins', 'median.mean_bins', 'min.mean_bins',\n",
    "       'nr_cor_attr_bins', 'nr_norm_bins', 'nr_outliers_bins',\n",
    "       'range.mean_bins', 'sd.mean_bins', 'skewness.mean_bins',\n",
    "       'sparsity.mean_bins', 't_mean.mean_bins', 'var.mean_bins',\n",
    "       'attr_to_inst_bins', 'inst_to_attr_bins',\n",
    "       'nr_attr_bins', 'nr_inst_bins', 'nr_num_bins', 'nr_bin_bins',\n",
    "       'attr_conc.mean_bins', 'attr_ent.mean_bins', 'ena_bins',\n",
    "       'snr.mean_bins', 'cEntropy_bins','FeatureAlgo']]\n",
    "\n",
    "df_quality_bin1 = df1[['Completeness_bins',\n",
    "       'Conciseness_bins', 'LabelIssues_bins', 'ClassImbRatio_bins', 'OutlierPerc_bins',\n",
    "       'ClassOverlapPerc_bins','FeatureAlgo']]\n",
    "\n",
    "df_bin1 = df1[['Completeness_bins',\n",
    "       'Conciseness_bins', 'cor.mean_bins', 'cov.mean_bins',\n",
    "       'eigenvalues.mean_bins', 'g_mean.mean_bins', 'h_mean.mean_bins',\n",
    "       'iq_range.mean_bins', 'kurtosis.mean_bins', 'mad.mean_bins',\n",
    "       'max.mean_bins', 'mean.mean_bins', 'median.mean_bins', 'min.mean_bins',\n",
    "       'nr_cor_attr_bins', 'nr_norm_bins', 'nr_outliers_bins',\n",
    "       'range.mean_bins', 'sd.mean_bins', 'skewness.mean_bins',\n",
    "       'sparsity.mean_bins', 't_mean.mean_bins', 'var.mean_bins',\n",
    "       'LabelIssues_bins', 'ClassImbRatio_bins', 'OutlierPerc_bins',\n",
    "       'ClassOverlapPerc_bins', 'attr_to_inst_bins', 'inst_to_attr_bins',\n",
    "       'nr_attr_bins', 'nr_inst_bins', 'nr_num_bins', 'nr_bin_bins',\n",
    "       'attr_conc.mean_bins', 'attr_ent.mean_bins', 'ena_bins',\n",
    "       'snr.mean_bins', 'cEntropy_bins', 'FeatureAlgo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"df_char_raw2 = df2[[ 'cor.mean', 'cov.mean', 'eigenvalues.mean', 'g_mean.mean', 'h_mean.mean', 'iq_range.mean', \\n 'kurtosis.mean', 'mad.mean', 'max.mean', 'mean.mean', 'median.mean', 'min.mean', 'nr_cor_attr', 'nr_norm', 'nr_outliers', 'range.mean', 'sd.mean', 'skewness.mean', 'sparsity.mean',\\n 't_mean.mean', 'var.mean', 'attr_to_inst', 'inst_to_attr', 'nr_attr', 'nr_bin', 'nr_inst', 'nr_num','attr_conc.mean', \\n 'attr_ent.mean', 'nUnique', 'ena', 'snr.mean', 'cEntropy', 'FeatureAlgo']]\\n\\ndf_quality_raw2 = df2[['Completeness', 'Conciseness', 'ClassImbRatio', 'ClassOverlapPerc', 'OutlierPerc', 'LabelIssuesPerc', 'FeatureAlgo']]\\n\\ndf_raw2 = df2[['Completeness', 'Conciseness', 'cor.mean', 'cov.mean', 'eigenvalues.mean', 'g_mean.mean', 'h_mean.mean', 'iq_range.mean', \\n 'kurtosis.mean', 'mad.mean', 'max.mean', 'mean.mean', 'median.mean', 'min.mean', 'nr_cor_attr', 'nr_norm', 'nr_outliers', 'range.mean', 'sd.mean', 'skewness.mean', 'sparsity.mean',\\n 't_mean.mean', 'var.mean', 'ClassImbRatio', 'ClassOverlapPerc', 'OutlierPerc', 'attr_to_inst', 'inst_to_attr', 'nr_attr', 'nr_bin', 'nr_inst', 'nr_num','attr_conc.mean', \\n 'attr_ent.mean', 'LabelIssuesPerc','nUnique', 'ena', 'snr.mean', 'cEntropy', 'FeatureAlgo']]\\n\\n\\ndf_char_bin2 = df2[['cor.mean_bins', 'cov.mean_bins',\\n       'eigenvalues.mean_bins', 'g_mean.mean_bins', 'h_mean.mean_bins',\\n       'iq_range.mean_bins', 'kurtosis.mean_bins', 'mad.mean_bins',\\n       'max.mean_bins', 'mean.mean_bins', 'median.mean_bins', 'min.mean_bins',\\n       'nr_cor_attr_bins', 'nr_norm_bins', 'nr_outliers_bins',\\n       'range.mean_bins', 'sd.mean_bins', 'skewness.mean_bins',\\n       'sparsity.mean_bins', 't_mean.mean_bins', 'var.mean_bins',\\n       'attr_to_inst_bins', 'inst_to_attr_bins',\\n       'nr_attr_bins', 'nr_inst_bins', 'nr_num_bins', 'nr_bin_bins',\\n       'attr_conc.mean_bins', 'attr_ent.mean_bins', 'ena_bins',\\n       'snr.mean_bins', 'cEntropy_bins','FeatureAlgo']]\\n\\ndf_quality_bin2 = df2[['Completeness_bins',\\n       'Conciseness_bins', 'LabelIssues_bins', 'ClassImbRatio_bins', 'OutlierPerc_bins',\\n       'ClassOverlapPerc_bins','FeatureAlgo']]\\n\\ndf_bin2 = df2[['Completeness_bins',\\n       'Conciseness_bins', 'cor.mean_bins', 'cov.mean_bins',\\n       'eigenvalues.mean_bins', 'g_mean.mean_bins', 'h_mean.mean_bins',\\n       'iq_range.mean_bins', 'kurtosis.mean_bins', 'mad.mean_bins',\\n       'max.mean_bins', 'mean.mean_bins', 'median.mean_bins', 'min.mean_bins',\\n       'nr_cor_attr_bins', 'nr_norm_bins', 'nr_outliers_bins',\\n       'range.mean_bins', 'sd.mean_bins', 'skewness.mean_bins',\\n       'sparsity.mean_bins', 't_mean.mean_bins', 'var.mean_bins',\\n       'LabelIssues_bins', 'ClassImbRatio_bins', 'OutlierPerc_bins',\\n       'ClassOverlapPerc_bins', 'attr_to_inst_bins', 'inst_to_attr_bins',\\n       'nr_attr_bins', 'nr_inst_bins', 'nr_num_bins', 'nr_bin_bins',\\n       'attr_conc.mean_bins', 'attr_ent.mean_bins', 'ena_bins',\\n       'snr.mean_bins', 'cEntropy_bins', 'FeatureAlgo']]\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''df_char_raw2 = df2[[ 'cor.mean', 'cov.mean', 'eigenvalues.mean', 'g_mean.mean', 'h_mean.mean', 'iq_range.mean', \n",
    " 'kurtosis.mean', 'mad.mean', 'max.mean', 'mean.mean', 'median.mean', 'min.mean', 'nr_cor_attr', 'nr_norm', 'nr_outliers', 'range.mean', 'sd.mean', 'skewness.mean', 'sparsity.mean',\n",
    " 't_mean.mean', 'var.mean', 'attr_to_inst', 'inst_to_attr', 'nr_attr', 'nr_bin', 'nr_inst', 'nr_num','attr_conc.mean', \n",
    " 'attr_ent.mean', 'nUnique', 'ena', 'snr.mean', 'cEntropy', 'FeatureAlgo']]\n",
    "\n",
    "df_quality_raw2 = df2[['Completeness', 'Conciseness', 'ClassImbRatio', 'ClassOverlapPerc', 'OutlierPerc', 'LabelIssuesPerc', 'FeatureAlgo']]\n",
    "\n",
    "df_raw2 = df2[['Completeness', 'Conciseness', 'cor.mean', 'cov.mean', 'eigenvalues.mean', 'g_mean.mean', 'h_mean.mean', 'iq_range.mean', \n",
    " 'kurtosis.mean', 'mad.mean', 'max.mean', 'mean.mean', 'median.mean', 'min.mean', 'nr_cor_attr', 'nr_norm', 'nr_outliers', 'range.mean', 'sd.mean', 'skewness.mean', 'sparsity.mean',\n",
    " 't_mean.mean', 'var.mean', 'ClassImbRatio', 'ClassOverlapPerc', 'OutlierPerc', 'attr_to_inst', 'inst_to_attr', 'nr_attr', 'nr_bin', 'nr_inst', 'nr_num','attr_conc.mean', \n",
    " 'attr_ent.mean', 'LabelIssuesPerc','nUnique', 'ena', 'snr.mean', 'cEntropy', 'FeatureAlgo']]\n",
    "\n",
    "\n",
    "df_char_bin2 = df2[['cor.mean_bins', 'cov.mean_bins',\n",
    "       'eigenvalues.mean_bins', 'g_mean.mean_bins', 'h_mean.mean_bins',\n",
    "       'iq_range.mean_bins', 'kurtosis.mean_bins', 'mad.mean_bins',\n",
    "       'max.mean_bins', 'mean.mean_bins', 'median.mean_bins', 'min.mean_bins',\n",
    "       'nr_cor_attr_bins', 'nr_norm_bins', 'nr_outliers_bins',\n",
    "       'range.mean_bins', 'sd.mean_bins', 'skewness.mean_bins',\n",
    "       'sparsity.mean_bins', 't_mean.mean_bins', 'var.mean_bins',\n",
    "       'attr_to_inst_bins', 'inst_to_attr_bins',\n",
    "       'nr_attr_bins', 'nr_inst_bins', 'nr_num_bins', 'nr_bin_bins',\n",
    "       'attr_conc.mean_bins', 'attr_ent.mean_bins', 'ena_bins',\n",
    "       'snr.mean_bins', 'cEntropy_bins','FeatureAlgo']]\n",
    "\n",
    "df_quality_bin2 = df2[['Completeness_bins',\n",
    "       'Conciseness_bins', 'LabelIssues_bins', 'ClassImbRatio_bins', 'OutlierPerc_bins',\n",
    "       'ClassOverlapPerc_bins','FeatureAlgo']]\n",
    "\n",
    "df_bin2 = df2[['Completeness_bins',\n",
    "       'Conciseness_bins', 'cor.mean_bins', 'cov.mean_bins',\n",
    "       'eigenvalues.mean_bins', 'g_mean.mean_bins', 'h_mean.mean_bins',\n",
    "       'iq_range.mean_bins', 'kurtosis.mean_bins', 'mad.mean_bins',\n",
    "       'max.mean_bins', 'mean.mean_bins', 'median.mean_bins', 'min.mean_bins',\n",
    "       'nr_cor_attr_bins', 'nr_norm_bins', 'nr_outliers_bins',\n",
    "       'range.mean_bins', 'sd.mean_bins', 'skewness.mean_bins',\n",
    "       'sparsity.mean_bins', 't_mean.mean_bins', 'var.mean_bins',\n",
    "       'LabelIssues_bins', 'ClassImbRatio_bins', 'OutlierPerc_bins',\n",
    "       'ClassOverlapPerc_bins', 'attr_to_inst_bins', 'inst_to_attr_bins',\n",
    "       'nr_attr_bins', 'nr_inst_bins', 'nr_num_bins', 'nr_bin_bins',\n",
    "       'attr_conc.mean_bins', 'attr_ent.mean_bins', 'ena_bins',\n",
    "       'snr.mean_bins', 'cEntropy_bins', 'FeatureAlgo']]'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAW DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw1 = df1[['Completeness', 'Conciseness', 'cor.mean', 'cov.mean', 'eigenvalues.mean', 'g_mean.mean', 'h_mean.mean', 'iq_range.mean', \n",
    " 'kurtosis.mean', 'mad.mean', 'max.mean', 'mean.mean', 'median.mean', 'min.mean', 'nr_cor_attr', 'nr_norm', 'nr_outliers', 'range.mean', 'sd.mean', 'skewness.mean', 'sparsity.mean',\n",
    " 't_mean.mean', 'var.mean', 'ClassImbRatio', 'ClassOverlapPerc', 'OutlierPerc', 'attr_to_inst', 'inst_to_attr', 'nr_attr', 'nr_bin', 'nr_inst', 'nr_num','attr_conc.mean', \n",
    " 'attr_ent.mean', 'LabelIssuesPerc','nUnique', 'ena', 'snr.mean', 'cEntropy','File', 'FeatureAlgo']]\n",
    "\n",
    "\n",
    "X = df_raw1.iloc[:, :-1]\n",
    "y = df_raw1.iloc[:, -1]\n",
    "\n",
    "    \n",
    "#label_encoder = LabelEncoder()\n",
    "#y_encoded = label_encoder.fit_transform(y)  \n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Ensure X_train/X_test are DataFrames\n",
    "X_train_df = pd.DataFrame(X_train)\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "\n",
    "# Concatenate features and target\n",
    "train_data = pd.concat([X_train_df, y_train], axis=1)\n",
    "test_data = pd.concat([X_test_df, y_test], axis=1)\n",
    "\n",
    "# Save to CSV\n",
    "train_data.to_csv(\"train_dataset.csv\", index=False)\n",
    "test_data.to_csv(\"test_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Completeness</th>\n",
       "      <th>Conciseness</th>\n",
       "      <th>cor.mean</th>\n",
       "      <th>cov.mean</th>\n",
       "      <th>eigenvalues.mean</th>\n",
       "      <th>g_mean.mean</th>\n",
       "      <th>h_mean.mean</th>\n",
       "      <th>iq_range.mean</th>\n",
       "      <th>kurtosis.mean</th>\n",
       "      <th>mad.mean</th>\n",
       "      <th>...</th>\n",
       "      <th>nr_num</th>\n",
       "      <th>attr_conc.mean</th>\n",
       "      <th>attr_ent.mean</th>\n",
       "      <th>LabelIssuesPerc</th>\n",
       "      <th>nUnique</th>\n",
       "      <th>ena</th>\n",
       "      <th>snr.mean</th>\n",
       "      <th>cEntropy</th>\n",
       "      <th>File</th>\n",
       "      <th>FeatureAlgo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048638</td>\n",
       "      <td>0.115693</td>\n",
       "      <td>1.013405e-03</td>\n",
       "      <td>7.934819e-03</td>\n",
       "      <td>0.309457</td>\n",
       "      <td>0.304097</td>\n",
       "      <td>0.073750</td>\n",
       "      <td>79.354226</td>\n",
       "      <td>0.050038</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1.549655e-02</td>\n",
       "      <td>2.069835</td>\n",
       "      <td>0.003891</td>\n",
       "      <td>2</td>\n",
       "      <td>5.354341</td>\n",
       "      <td>4.303351</td>\n",
       "      <td>0.466524</td>\n",
       "      <td>yeast-2_vs_4.csv</td>\n",
       "      <td>chisquare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.217669</td>\n",
       "      <td>2.134052e-01</td>\n",
       "      <td>1.860135e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>4.268611</td>\n",
       "      <td>0.688350</td>\n",
       "      <td>...</td>\n",
       "      <td>28</td>\n",
       "      <td>6.020831e-02</td>\n",
       "      <td>1.109183</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>10</td>\n",
       "      <td>-5.708116</td>\n",
       "      <td>1.831846</td>\n",
       "      <td>0.886403</td>\n",
       "      <td>ckd-dataset-v2.csv</td>\n",
       "      <td>chisquare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.432232</td>\n",
       "      <td>1.705113e+02</td>\n",
       "      <td>1.669686e+03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.396031</td>\n",
       "      <td>10.167753</td>\n",
       "      <td>14.488797</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>8.278827e-02</td>\n",
       "      <td>4.700437</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>2</td>\n",
       "      <td>0.283453</td>\n",
       "      <td>1.865977</td>\n",
       "      <td>0.441703</td>\n",
       "      <td>HTRU_2.csv</td>\n",
       "      <td>chisquare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.185636</td>\n",
       "      <td>5.097720e+01</td>\n",
       "      <td>2.084464e+03</td>\n",
       "      <td>0.213323</td>\n",
       "      <td>0.203318</td>\n",
       "      <td>37.265000</td>\n",
       "      <td>39.166774</td>\n",
       "      <td>27.777335</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>5.241064e-02</td>\n",
       "      <td>2.033069</td>\n",
       "      <td>0.059701</td>\n",
       "      <td>8</td>\n",
       "      <td>-25.495714</td>\n",
       "      <td>4.302230</td>\n",
       "      <td>0.730531</td>\n",
       "      <td>ecoli.data</td>\n",
       "      <td>GR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012061</td>\n",
       "      <td>0.289153</td>\n",
       "      <td>3.424150e+05</td>\n",
       "      <td>2.872231e+06</td>\n",
       "      <td>81.534251</td>\n",
       "      <td>29.931656</td>\n",
       "      <td>194.626625</td>\n",
       "      <td>652.389857</td>\n",
       "      <td>89.173423</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>8.867034e-02</td>\n",
       "      <td>3.991853</td>\n",
       "      <td>0.049708</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.629243</td>\n",
       "      <td>1.070543</td>\n",
       "      <td>0.273259</td>\n",
       "      <td>pageblocks5cn01-nc.csv</td>\n",
       "      <td>chisquare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087443</td>\n",
       "      <td>1.996062e-01</td>\n",
       "      <td>4.047913e+00</td>\n",
       "      <td>1.450287</td>\n",
       "      <td>1.375516</td>\n",
       "      <td>1.562500</td>\n",
       "      <td>1.283006</td>\n",
       "      <td>1.019287</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>1.428959e-02</td>\n",
       "      <td>1.432062</td>\n",
       "      <td>0.005063</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.763742</td>\n",
       "      <td>2.275673</td>\n",
       "      <td>0.519086</td>\n",
       "      <td>student-mat.csv</td>\n",
       "      <td>GR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080618</td>\n",
       "      <td>1.070540e-02</td>\n",
       "      <td>1.326280e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>110.887929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>36</td>\n",
       "      <td>1.049705e-02</td>\n",
       "      <td>0.580495</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>2</td>\n",
       "      <td>0.874819</td>\n",
       "      <td>0.629320</td>\n",
       "      <td>0.998595</td>\n",
       "      <td>chesskr-vs-kp.data</td>\n",
       "      <td>chisquare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.446594e+01</td>\n",
       "      <td>4.544194e+02</td>\n",
       "      <td>172.050086</td>\n",
       "      <td>170.708181</td>\n",
       "      <td>21.450000</td>\n",
       "      <td>1.890783</td>\n",
       "      <td>15.190948</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>1.228174e-01</td>\n",
       "      <td>2.261302</td>\n",
       "      <td>0.295082</td>\n",
       "      <td>9</td>\n",
       "      <td>-6.135210</td>\n",
       "      <td>2.945184</td>\n",
       "      <td>0.425673</td>\n",
       "      <td>Algerian_forest_fires_dataset_UPDATE.csv</td>\n",
       "      <td>chisquare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001825</td>\n",
       "      <td>0.346109</td>\n",
       "      <td>8.209154e+05</td>\n",
       "      <td>7.524909e+06</td>\n",
       "      <td>80.951586</td>\n",
       "      <td>31.132827</td>\n",
       "      <td>191.185450</td>\n",
       "      <td>86.979351</td>\n",
       "      <td>82.740125</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>1.468748e-01</td>\n",
       "      <td>2.948370</td>\n",
       "      <td>0.009124</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.647301</td>\n",
       "      <td>1.065389</td>\n",
       "      <td>0.273311</td>\n",
       "      <td>pageblocks_imbalanced.csv</td>\n",
       "      <td>chisquare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.413590</td>\n",
       "      <td>1.570798e+01</td>\n",
       "      <td>5.988930e+01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.760000</td>\n",
       "      <td>13.952040</td>\n",
       "      <td>3.469284</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>9.479532e-02</td>\n",
       "      <td>2.313665</td>\n",
       "      <td>0.009302</td>\n",
       "      <td>3</td>\n",
       "      <td>1.454436</td>\n",
       "      <td>2.479741</td>\n",
       "      <td>0.747746</td>\n",
       "      <td>newthyroid.csv</td>\n",
       "      <td>GR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.563946</td>\n",
       "      <td>7.700474e+07</td>\n",
       "      <td>4.547701e+08</td>\n",
       "      <td>23651.279434</td>\n",
       "      <td>21771.395297</td>\n",
       "      <td>13299.202213</td>\n",
       "      <td>1.738750</td>\n",
       "      <td>9436.994317</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1.872784e-01</td>\n",
       "      <td>3.169925</td>\n",
       "      <td>0.058889</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.038638</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Raisin_Dataset.csv</td>\n",
       "      <td>chisquare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.035852</td>\n",
       "      <td>2.903077e-02</td>\n",
       "      <td>8.419500e-01</td>\n",
       "      <td>1.760787</td>\n",
       "      <td>1.578382</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>-0.692739</td>\n",
       "      <td>1.482600</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>5.074699e-02</td>\n",
       "      <td>1.737242</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>3</td>\n",
       "      <td>6.557265</td>\n",
       "      <td>2.211693</td>\n",
       "      <td>0.975388</td>\n",
       "      <td>Hayes-roth imbalance.csv</td>\n",
       "      <td>GR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004673</td>\n",
       "      <td>0.214501</td>\n",
       "      <td>1.119574e-01</td>\n",
       "      <td>7.027958e-01</td>\n",
       "      <td>10.864354</td>\n",
       "      <td>10.840340</td>\n",
       "      <td>0.571935</td>\n",
       "      <td>10.529736</td>\n",
       "      <td>0.300909</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>8.650549e-02</td>\n",
       "      <td>2.017412</td>\n",
       "      <td>0.009346</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.575624</td>\n",
       "      <td>62.508342</td>\n",
       "      <td>0.841998</td>\n",
       "      <td>glass5cn10.csv</td>\n",
       "      <td>GR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.220142</td>\n",
       "      <td>2.768919e+00</td>\n",
       "      <td>1.159489e+01</td>\n",
       "      <td>8.304728</td>\n",
       "      <td>7.396481</td>\n",
       "      <td>5.592105</td>\n",
       "      <td>0.582838</td>\n",
       "      <td>3.277326</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>1.151373e-01</td>\n",
       "      <td>1.585979</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>2</td>\n",
       "      <td>4.633771</td>\n",
       "      <td>3.193399</td>\n",
       "      <td>0.870864</td>\n",
       "      <td>cervical_behaviour.csv</td>\n",
       "      <td>fcbf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.372531</td>\n",
       "      <td>2.968657e+07</td>\n",
       "      <td>5.050378e+14</td>\n",
       "      <td>73132.176443</td>\n",
       "      <td>14539.661032</td>\n",
       "      <td>256672.434473</td>\n",
       "      <td>8.887775</td>\n",
       "      <td>2140.176953</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>8.795676e-02</td>\n",
       "      <td>2.999953</td>\n",
       "      <td>0.052817</td>\n",
       "      <td>2</td>\n",
       "      <td>0.908034</td>\n",
       "      <td>2.836751</td>\n",
       "      <td>0.951801</td>\n",
       "      <td>breastcancerwdbc.data</td>\n",
       "      <td>chisquare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.013788</td>\n",
       "      <td>0.008374</td>\n",
       "      <td>0.084706</td>\n",
       "      <td>1.564483e+06</td>\n",
       "      <td>5.199317e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>116.409511</td>\n",
       "      <td>4954.750674</td>\n",
       "      <td>59.170150</td>\n",
       "      <td>...</td>\n",
       "      <td>64</td>\n",
       "      <td>3.130740e-02</td>\n",
       "      <td>4.313838</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>2</td>\n",
       "      <td>0.174616</td>\n",
       "      <td>0.125507</td>\n",
       "      <td>0.297315</td>\n",
       "      <td>4year.csv</td>\n",
       "      <td>chisquare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011369</td>\n",
       "      <td>0.105500</td>\n",
       "      <td>6.088329e-01</td>\n",
       "      <td>4.570376e+01</td>\n",
       "      <td>7.163086</td>\n",
       "      <td>6.842736</td>\n",
       "      <td>3.432791</td>\n",
       "      <td>4.155480</td>\n",
       "      <td>2.516214</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>3.073268e-02</td>\n",
       "      <td>1.840552</td>\n",
       "      <td>0.000947</td>\n",
       "      <td>7</td>\n",
       "      <td>-11.310228</td>\n",
       "      <td>3.350330</td>\n",
       "      <td>0.998295</td>\n",
       "      <td>ObesityDataSet_raw_and_data_sinthetic.csv</td>\n",
       "      <td>relief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.722547</td>\n",
       "      <td>1.735490e+00</td>\n",
       "      <td>2.370735e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.111111</td>\n",
       "      <td>-1.316729</td>\n",
       "      <td>1.619878</td>\n",
       "      <td>...</td>\n",
       "      <td>54</td>\n",
       "      <td>3.057951e-01</td>\n",
       "      <td>1.797568</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.322282</td>\n",
       "      <td>1.175264</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>divorce.csv</td>\n",
       "      <td>chisquare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.003205e+00</td>\n",
       "      <td>2.605171</td>\n",
       "      <td>2.189781</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.305436</td>\n",
       "      <td>1.482600</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>-6.250000e-08</td>\n",
       "      <td>2.321928</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.677040</td>\n",
       "      <td>2.138102</td>\n",
       "      <td>0.831639</td>\n",
       "      <td>balancescaleimbalanced.csv</td>\n",
       "      <td>chisquare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.407057</td>\n",
       "      <td>2.784462e+02</td>\n",
       "      <td>2.025589e+03</td>\n",
       "      <td>113.838671</td>\n",
       "      <td>111.362872</td>\n",
       "      <td>34.569444</td>\n",
       "      <td>5.096477</td>\n",
       "      <td>21.168233</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>1.212231e-01</td>\n",
       "      <td>3.092930</td>\n",
       "      <td>0.002364</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.404616</td>\n",
       "      <td>7.985474</td>\n",
       "      <td>0.950746</td>\n",
       "      <td>vehicle3.csv</td>\n",
       "      <td>chisquare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103419</td>\n",
       "      <td>0.069474</td>\n",
       "      <td>2.049481e-01</td>\n",
       "      <td>3.625483e+00</td>\n",
       "      <td>0.395078</td>\n",
       "      <td>0.276014</td>\n",
       "      <td>1.541176</td>\n",
       "      <td>169.794926</td>\n",
       "      <td>0.941887</td>\n",
       "      <td>...</td>\n",
       "      <td>85</td>\n",
       "      <td>8.286126e-03</td>\n",
       "      <td>1.335823</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>2</td>\n",
       "      <td>-8.377972</td>\n",
       "      <td>0.957338</td>\n",
       "      <td>0.326585</td>\n",
       "      <td>insurancedata2000.data</td>\n",
       "      <td>chisquare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.405163</td>\n",
       "      <td>1.412261e+02</td>\n",
       "      <td>4.182884e+02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.948599</td>\n",
       "      <td>6.979498</td>\n",
       "      <td>15.649056</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>8.208582e-02</td>\n",
       "      <td>2.584843</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>3</td>\n",
       "      <td>2.935926</td>\n",
       "      <td>3.217910</td>\n",
       "      <td>0.941259</td>\n",
       "      <td>column_3C_weka.csv</td>\n",
       "      <td>GR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.989096</td>\n",
       "      <td>0.185447</td>\n",
       "      <td>1.856629e-01</td>\n",
       "      <td>1.000920e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.629333</td>\n",
       "      <td>4.806175</td>\n",
       "      <td>0.466327</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>5.480887e-02</td>\n",
       "      <td>0.958873</td>\n",
       "      <td>0.000909</td>\n",
       "      <td>2</td>\n",
       "      <td>-2.034783</td>\n",
       "      <td>-0.094808</td>\n",
       "      <td>0.907651</td>\n",
       "      <td>titanic.csv</td>\n",
       "      <td>relief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.229355</td>\n",
       "      <td>1.806328e+02</td>\n",
       "      <td>1.345897e+04</td>\n",
       "      <td>73.550359</td>\n",
       "      <td>60.819170</td>\n",
       "      <td>59.074952</td>\n",
       "      <td>5.024238</td>\n",
       "      <td>41.522410</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>6.307469e-02</td>\n",
       "      <td>1.998564</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>2</td>\n",
       "      <td>0.984652</td>\n",
       "      <td>2.394588</td>\n",
       "      <td>0.992267</td>\n",
       "      <td>breastcancercoimbra.csv</td>\n",
       "      <td>MI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.000626</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.228674</td>\n",
       "      <td>2.565648e-01</td>\n",
       "      <td>8.297483e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.323529</td>\n",
       "      <td>2.409273</td>\n",
       "      <td>0.697694</td>\n",
       "      <td>...</td>\n",
       "      <td>34</td>\n",
       "      <td>7.928079e-02</td>\n",
       "      <td>1.184957</td>\n",
       "      <td>0.005479</td>\n",
       "      <td>6</td>\n",
       "      <td>-6.517973</td>\n",
       "      <td>0.954112</td>\n",
       "      <td>0.940885</td>\n",
       "      <td>dermatology.data</td>\n",
       "      <td>chisquare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060109</td>\n",
       "      <td>1.962652e-02</td>\n",
       "      <td>2.424374e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.867149</td>\n",
       "      <td>-1.115896</td>\n",
       "      <td>1.384179</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>3.440063e-03</td>\n",
       "      <td>4.392317</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>2</td>\n",
       "      <td>0.894533</td>\n",
       "      <td>0.956736</td>\n",
       "      <td>0.944331</td>\n",
       "      <td>Electical_stability.csv</td>\n",
       "      <td>fcbf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009861</td>\n",
       "      <td>0.049003</td>\n",
       "      <td>7.943880e-04</td>\n",
       "      <td>3.275375e-02</td>\n",
       "      <td>0.038153</td>\n",
       "      <td>0.032863</td>\n",
       "      <td>0.066285</td>\n",
       "      <td>388.707179</td>\n",
       "      <td>0.013887</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>2.477930e-03</td>\n",
       "      <td>1.348138</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.251306</td>\n",
       "      <td>1.265198</td>\n",
       "      <td>0.282405</td>\n",
       "      <td>thyroid.csv</td>\n",
       "      <td>chisquare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.007102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.624371e-01</td>\n",
       "      <td>2.953956e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>16.683670</td>\n",
       "      <td>1.334340</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>5.017971e-02</td>\n",
       "      <td>1.161564</td>\n",
       "      <td>0.002841</td>\n",
       "      <td>2</td>\n",
       "      <td>2.283909</td>\n",
       "      <td>1.146504</td>\n",
       "      <td>0.839255</td>\n",
       "      <td>Autism-Adult-Data.csv</td>\n",
       "      <td>chisquare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.479964</td>\n",
       "      <td>1.930821e-02</td>\n",
       "      <td>4.005634e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.241357</td>\n",
       "      <td>1.139800</td>\n",
       "      <td>0.156838</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1.880345e-01</td>\n",
       "      <td>1.996546</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>2</td>\n",
       "      <td>0.560231</td>\n",
       "      <td>1.867604</td>\n",
       "      <td>0.718138</td>\n",
       "      <td>appendicitis.csv</td>\n",
       "      <td>GR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.413590</td>\n",
       "      <td>1.570798e+01</td>\n",
       "      <td>5.988930e+01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.760000</td>\n",
       "      <td>13.952040</td>\n",
       "      <td>3.469284</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>9.479532e-02</td>\n",
       "      <td>2.313665</td>\n",
       "      <td>0.009302</td>\n",
       "      <td>3</td>\n",
       "      <td>1.454436</td>\n",
       "      <td>2.485844</td>\n",
       "      <td>0.747746</td>\n",
       "      <td>Thyroid Disease (New Thyroid) Multi-class Imba...</td>\n",
       "      <td>GR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.342606</td>\n",
       "      <td>0.601939</td>\n",
       "      <td>4.720494e+00</td>\n",
       "      <td>7.892321e+00</td>\n",
       "      <td>2.248067</td>\n",
       "      <td>1.741995</td>\n",
       "      <td>3.111111</td>\n",
       "      <td>1.589404</td>\n",
       "      <td>0.494200</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>2.071309e-01</td>\n",
       "      <td>1.847454</td>\n",
       "      <td>0.002928</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.747355</td>\n",
       "      <td>1.110718</td>\n",
       "      <td>0.934003</td>\n",
       "      <td>breastcancerimbalance.csv</td>\n",
       "      <td>fcbf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.016753</td>\n",
       "      <td>0.223743</td>\n",
       "      <td>3.724802e+01</td>\n",
       "      <td>6.133266e+02</td>\n",
       "      <td>1.906980</td>\n",
       "      <td>1.623274</td>\n",
       "      <td>6.625735</td>\n",
       "      <td>88.490771</td>\n",
       "      <td>1.307304</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>1.311544e-01</td>\n",
       "      <td>1.516487</td>\n",
       "      <td>0.002577</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.332986</td>\n",
       "      <td>2.580567</td>\n",
       "      <td>0.953479</td>\n",
       "      <td>audit_trial.csv</td>\n",
       "      <td>GR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.676480</td>\n",
       "      <td>1.296376e-02</td>\n",
       "      <td>1.197079e-01</td>\n",
       "      <td>0.327720</td>\n",
       "      <td>0.298163</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>0.567975</td>\n",
       "      <td>0.304767</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2.053920e-01</td>\n",
       "      <td>2.958876</td>\n",
       "      <td>0.002736</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.318823</td>\n",
       "      <td>3.196814</td>\n",
       "      <td>0.317260</td>\n",
       "      <td>abalone9-18.csv</td>\n",
       "      <td>chisquare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.164556</td>\n",
       "      <td>2.098385e+00</td>\n",
       "      <td>5.258301e+01</td>\n",
       "      <td>8.974724</td>\n",
       "      <td>7.166145</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>0.059317</td>\n",
       "      <td>5.485620</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>2.255776e-01</td>\n",
       "      <td>1.375906</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>3</td>\n",
       "      <td>2.329141</td>\n",
       "      <td>2.948538</td>\n",
       "      <td>0.999879</td>\n",
       "      <td>tae.data</td>\n",
       "      <td>MI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.465508</td>\n",
       "      <td>1.607919e+01</td>\n",
       "      <td>6.601716e+02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.734546</td>\n",
       "      <td>4.980780</td>\n",
       "      <td>9.750099</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>1.517935e-01</td>\n",
       "      <td>2.320058</td>\n",
       "      <td>0.010256</td>\n",
       "      <td>2</td>\n",
       "      <td>0.673815</td>\n",
       "      <td>2.356343</td>\n",
       "      <td>0.805125</td>\n",
       "      <td>parkinsons.data</td>\n",
       "      <td>GR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Completeness  Conciseness  cor.mean      cov.mean  eigenvalues.mean  \\\n",
       "162      0.000000     0.048638  0.115693  1.013405e-03      7.934819e-03   \n",
       "42       0.000000     0.000000  0.217669  2.134052e-01      1.860135e+00   \n",
       "90       0.000000     0.000000  0.432232  1.705113e+02      1.669686e+03   \n",
       "60       0.000000     0.000000  0.185636  5.097720e+01      2.084464e+03   \n",
       "114      0.000000     0.012061  0.289153  3.424150e+05      2.872231e+06   \n",
       "137      0.000000     0.000000  0.087443  1.996062e-01      4.047913e+00   \n",
       "41       0.000000     0.000000  0.080618  1.070540e-02      1.326280e-01   \n",
       "15       0.000293     0.000000       NaN  4.446594e+01      4.544194e+02   \n",
       "113      0.000000     0.001825  0.346109  8.209154e+05      7.524909e+06   \n",
       "108      0.000000     0.000000  0.413590  1.570798e+01      5.988930e+01   \n",
       "124      0.000000     0.000000  0.563946  7.700474e+07      4.547701e+08   \n",
       "82       0.000000     0.409091  0.035852  2.903077e-02      8.419500e-01   \n",
       "78       0.000000     0.004673  0.214501  1.119574e-01      7.027958e-01   \n",
       "38       0.000000     0.000000  0.220142  2.768919e+00      1.159489e+01   \n",
       "31       0.000000     0.000000  0.372531  2.968657e+07      5.050378e+14   \n",
       "9        0.013788     0.008374  0.084706  1.564483e+06      5.199317e+08   \n",
       "111      0.000000     0.011369  0.105500  6.088329e-01      4.570376e+01   \n",
       "56       0.000000     0.117647  0.722547  1.735490e+00      2.370735e+00   \n",
       "24       0.000000     0.000000  0.000000  0.000000e+00      2.003205e+00   \n",
       "153      0.000000     0.000000  0.407057  2.784462e+02      2.025589e+03   \n",
       "93       0.000000     0.103419  0.069474  2.049481e-01      3.625483e+00   \n",
       "45       0.000000     0.000000  0.405163  1.412261e+02      4.182884e+02   \n",
       "146      0.000000     0.989096  0.185447  1.856629e-01      1.000920e+00   \n",
       "29       0.000000     0.000000  0.229355  1.806328e+02      1.345897e+04   \n",
       "55       0.000626     0.000000  0.228674  2.565648e-01      8.297483e+00   \n",
       "65       0.000000     0.000000  0.060109  1.962652e-02      2.424374e+00   \n",
       "143      0.000000     0.009861  0.049003  7.943880e-04      3.275375e-02   \n",
       "19       0.012987     0.007102       NaN  3.624371e-01      2.953956e+01   \n",
       "16       0.000000     0.000000  0.479964  1.930821e-02      4.005634e-02   \n",
       "141      0.000000     0.000000  0.413590  1.570798e+01      5.988930e+01   \n",
       "30       0.000000     0.342606  0.601939  4.720494e+00      7.892321e+00   \n",
       "18       0.000072     0.016753  0.223743  3.724802e+01      6.133266e+02   \n",
       "12       0.000000     0.000000  0.676480  1.296376e-02      1.197079e-01   \n",
       "139      0.000000     0.266667  0.164556  2.098385e+00      5.258301e+01   \n",
       "115      0.000000     0.000000  0.465508  1.607919e+01      6.601716e+02   \n",
       "\n",
       "      g_mean.mean   h_mean.mean  iq_range.mean  kurtosis.mean     mad.mean  \\\n",
       "162      0.309457      0.304097       0.073750      79.354226     0.050038   \n",
       "42       0.000000      0.000000       1.250000       4.268611     0.688350   \n",
       "90            NaN           NaN      20.396031      10.167753    14.488797   \n",
       "60       0.213323      0.203318      37.265000      39.166774    27.777335   \n",
       "114     81.534251     29.931656     194.626625     652.389857    89.173423   \n",
       "137      1.450287      1.375516       1.562500       1.283006     1.019287   \n",
       "41       0.000000      0.000000       0.361111     110.887929     0.000000   \n",
       "15     172.050086    170.708181      21.450000       1.890783    15.190948   \n",
       "113     80.951586     31.132827     191.185450      86.979351    82.740125   \n",
       "108           NaN           NaN       4.760000      13.952040     3.469284   \n",
       "124  23651.279434  21771.395297   13299.202213       1.738750  9436.994317   \n",
       "82       1.760787      1.578382       1.250000      -0.692739     1.482600   \n",
       "78      10.864354     10.840340       0.571935      10.529736     0.300909   \n",
       "38       8.304728      7.396481       5.592105       0.582838     3.277326   \n",
       "31   73132.176443  14539.661032  256672.434473       8.887775  2140.176953   \n",
       "9             NaN           NaN     116.409511    4954.750674    59.170150   \n",
       "111      7.163086      6.842736       3.432791       4.155480     2.516214   \n",
       "56       0.000000      0.000000       3.111111      -1.316729     1.619878   \n",
       "24       2.605171      2.189781       2.000000      -1.305436     1.482600   \n",
       "153    113.838671    111.362872      34.569444       5.096477    21.168233   \n",
       "93       0.395078      0.276014       1.541176     169.794926     0.941887   \n",
       "45            NaN           NaN      22.948599       6.979498    15.649056   \n",
       "146           NaN           NaN       0.629333       4.806175     0.466327   \n",
       "29      73.550359     60.819170      59.074952       5.024238    41.522410   \n",
       "55       0.000000      0.000000       1.323529       2.409273     0.697694   \n",
       "65            NaN           NaN       1.867149      -1.115896     1.384179   \n",
       "143      0.038153      0.032863       0.066285     388.707179     0.013887   \n",
       "19       0.000000      0.000000       3.300000      16.683670     1.334340   \n",
       "16       0.000000      0.000000       0.241357       1.139800     0.156838   \n",
       "141           NaN           NaN       4.760000      13.952040     3.469284   \n",
       "30       2.248067      1.741995       3.111111       1.589404     0.494200   \n",
       "18       1.906980      1.623274       6.625735      88.490771     1.307304   \n",
       "12       0.327720      0.298163       0.410000       0.567975     0.304767   \n",
       "139      8.974724      7.166145       8.400000       0.059317     5.485620   \n",
       "115           NaN           NaN      13.734546       4.980780     9.750099   \n",
       "\n",
       "     ...  nr_num  attr_conc.mean  attr_ent.mean  LabelIssuesPerc  nUnique  \\\n",
       "162  ...       8    1.549655e-02       2.069835         0.003891        2   \n",
       "42   ...      28    6.020831e-02       1.109183         0.010000       10   \n",
       "90   ...       8    8.278827e-02       4.700437         0.000112        2   \n",
       "60   ...       9    5.241064e-02       2.033069         0.059701        8   \n",
       "114  ...      10    8.867034e-02       3.991853         0.049708        5   \n",
       "137  ...      32    1.428959e-02       1.432062         0.005063        2   \n",
       "41   ...      36    1.049705e-02       0.580495         0.000626        2   \n",
       "15   ...      13    1.228174e-01       2.261302         0.295082        9   \n",
       "113  ...      10    1.468748e-01       2.948370         0.009124        5   \n",
       "108  ...       5    9.479532e-02       2.313665         0.009302        3   \n",
       "124  ...       7    1.872784e-01       3.169925         0.058889        2   \n",
       "82   ...       4    5.074699e-02       1.737242         0.015152        3   \n",
       "78   ...       9    8.650549e-02       2.017412         0.009346        6   \n",
       "38   ...      19    1.151373e-01       1.585979         0.027778        2   \n",
       "31   ...      31    8.795676e-02       2.999953         0.052817        2   \n",
       "9    ...      64    3.130740e-02       4.313838         0.000204        2   \n",
       "111  ...      16    3.073268e-02       1.840552         0.000947        7   \n",
       "56   ...      54    3.057951e-01       1.797568         0.011765        2   \n",
       "24   ...       4   -6.250000e-08       2.321928         0.003200        3   \n",
       "153  ...      18    1.212231e-01       3.092930         0.002364        3   \n",
       "93   ...      85    8.286126e-03       1.335823         0.000344        2   \n",
       "45   ...       6    8.208582e-02       2.584843         0.006452        3   \n",
       "146  ...       3    5.480887e-02       0.958873         0.000909        2   \n",
       "29   ...       9    6.307469e-02       1.998564         0.017241        2   \n",
       "55   ...      34    7.928079e-02       1.184957         0.005479        6   \n",
       "65   ...      13    3.440063e-03       4.392317         0.000200        2   \n",
       "143  ...      21    2.477930e-03       1.348138         0.000278        3   \n",
       "19   ...      20    5.017971e-02       1.161564         0.002841        2   \n",
       "16   ...       7    1.880345e-01       1.996546         0.018868        2   \n",
       "141  ...       5    9.479532e-02       2.313665         0.009302        3   \n",
       "30   ...       9    2.071309e-01       1.847454         0.002928        2   \n",
       "18   ...      17    1.311544e-01       1.516487         0.002577        2   \n",
       "12   ...       8    2.053920e-01       2.958876         0.002736        2   \n",
       "139  ...       5    2.255776e-01       1.375906         0.013333        3   \n",
       "115  ...      23    1.517935e-01       2.320058         0.010256        2   \n",
       "\n",
       "           ena   snr.mean  cEntropy  \\\n",
       "162   5.354341   4.303351  0.466524   \n",
       "42   -5.708116   1.831846  0.886403   \n",
       "90    0.283453   1.865977  0.441703   \n",
       "60  -25.495714   4.302230  0.730531   \n",
       "114  -0.629243   1.070543  0.273259   \n",
       "137  -0.763742   2.275673  0.519086   \n",
       "41    0.874819   0.629320  0.998595   \n",
       "15   -6.135210   2.945184  0.425673   \n",
       "113  -0.647301   1.065389  0.273311   \n",
       "108   1.454436   2.479741  0.747746   \n",
       "124   1.000000   5.038638  1.000000   \n",
       "82    6.557265   2.211693  0.975388   \n",
       "78   -1.575624  62.508342  0.841998   \n",
       "38    4.633771   3.193399  0.870864   \n",
       "31    0.908034   2.836751  0.951801   \n",
       "9     0.174616   0.125507  0.297315   \n",
       "111 -11.310228   3.350330  0.998295   \n",
       "56   -1.322282   1.175264  0.999900   \n",
       "24   -1.677040   2.138102  0.831639   \n",
       "153  -0.404616   7.985474  0.950746   \n",
       "93   -8.377972   0.957338  0.326585   \n",
       "45    2.935926   3.217910  0.941259   \n",
       "146  -2.034783  -0.094808  0.907651   \n",
       "29    0.984652   2.394588  0.992267   \n",
       "55   -6.517973   0.954112  0.940885   \n",
       "65    0.894533   0.956736  0.944331   \n",
       "143  -0.251306   1.265198  0.282405   \n",
       "19    2.283909   1.146504  0.839255   \n",
       "16    0.560231   1.867604  0.718138   \n",
       "141   1.454436   2.485844  0.747746   \n",
       "30   -0.747355   1.110718  0.934003   \n",
       "18   -1.332986   2.580567  0.953479   \n",
       "12   -0.318823   3.196814  0.317260   \n",
       "139   2.329141   2.948538  0.999879   \n",
       "115   0.673815   2.356343  0.805125   \n",
       "\n",
       "                                                  File  FeatureAlgo  \n",
       "162                                   yeast-2_vs_4.csv    chisquare  \n",
       "42                                  ckd-dataset-v2.csv    chisquare  \n",
       "90                                          HTRU_2.csv    chisquare  \n",
       "60                                          ecoli.data           GR  \n",
       "114                             pageblocks5cn01-nc.csv    chisquare  \n",
       "137                                    student-mat.csv           GR  \n",
       "41                                  chesskr-vs-kp.data    chisquare  \n",
       "15            Algerian_forest_fires_dataset_UPDATE.csv    chisquare  \n",
       "113                          pageblocks_imbalanced.csv    chisquare  \n",
       "108                                     newthyroid.csv           GR  \n",
       "124                                 Raisin_Dataset.csv    chisquare  \n",
       "82                            Hayes-roth imbalance.csv           GR  \n",
       "78                                      glass5cn10.csv           GR  \n",
       "38                              cervical_behaviour.csv         fcbf  \n",
       "31                               breastcancerwdbc.data    chisquare  \n",
       "9                                            4year.csv    chisquare  \n",
       "111          ObesityDataSet_raw_and_data_sinthetic.csv       relief  \n",
       "56                                         divorce.csv    chisquare  \n",
       "24                          balancescaleimbalanced.csv    chisquare  \n",
       "153                                       vehicle3.csv    chisquare  \n",
       "93                              insurancedata2000.data    chisquare  \n",
       "45                                  column_3C_weka.csv           GR  \n",
       "146                                        titanic.csv       relief  \n",
       "29                             breastcancercoimbra.csv           MI  \n",
       "55                                    dermatology.data    chisquare  \n",
       "65                             Electical_stability.csv         fcbf  \n",
       "143                                        thyroid.csv    chisquare  \n",
       "19                               Autism-Adult-Data.csv    chisquare  \n",
       "16                                    appendicitis.csv           GR  \n",
       "141  Thyroid Disease (New Thyroid) Multi-class Imba...           GR  \n",
       "30                           breastcancerimbalance.csv         fcbf  \n",
       "18                                     audit_trial.csv           GR  \n",
       "12                                     abalone9-18.csv    chisquare  \n",
       "139                                           tae.data           MI  \n",
       "115                                    parkinsons.data           GR  \n",
       "\n",
       "[35 rows x 41 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-NN\n",
    "def evaluate_knn(df, random_state=42):\n",
    "    df = df.copy()\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)  \n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=random_state)\n",
    "    \n",
    "    knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "    \n",
    "    cross_val_scores = cross_val_score(knn_classifier, X_train, y_train, cv=kf, scoring='accuracy')\n",
    "    error_rates = 1 - cross_val_scores\n",
    "    \n",
    "    knn_classifier.fit(X_train, y_train)\n",
    "    y_pred = knn_classifier.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'Cross-Validation Accuracy': cross_val_scores.mean(),\n",
    "        'Cross-Validation Error Rate': error_rates.mean(),\n",
    "        'Test Accuracy': accuracy,\n",
    "        'Test Error Rate': 1 - accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "datasets1 = {\n",
    "    'df_raw': df_raw1,\n",
    "    'df_char_raw': df_char_raw1,\n",
    "    'df_quality_raw': df_quality_raw1\n",
    "}\n",
    "\n",
    "results1 = {}\n",
    "\n",
    "for name, df in datasets1.items():\n",
    "    results1[name] = evaluate_knn(df)\n",
    "\n",
    "comparison_df1 = pd.DataFrame(results1).transpose()\n",
    "print(\"**************KNN - 0.1_0.06**************\\n\")\n",
    "print(comparison_df1)\n",
    "\n",
    "datasets2 = {\n",
    "    'df_raw': df_raw2,\n",
    "    'df_char_raw': df_char_raw2,\n",
    "    'df_quality_raw': df_quality_raw2\n",
    "}\n",
    "\n",
    "results2 = {}\n",
    "\n",
    "for name, df in datasets2.items():\n",
    "    results2[name] = evaluate_knn(df)\n",
    "\n",
    "comparison_df2 = pd.DataFrame(results2).transpose()\n",
    "print(\"\\n\\n**************KNN - Majority clustering**************\\n\")\n",
    "print(comparison_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decision\n",
    "\n",
    "def evaluate_decision_tree(df, random_state=33):\n",
    "    df = df.copy()\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    \n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)  \n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=random_state)\n",
    "    \n",
    "    tree_classifier = DecisionTreeClassifier(random_state=random_state)\n",
    "    \n",
    "    cross_val_scores = cross_val_score(tree_classifier, X_train, y_train, cv=kf, scoring='accuracy')\n",
    "    error_rates = 1 - cross_val_scores\n",
    "    \n",
    "    tree_classifier.fit(X_train, y_train)\n",
    "    y_pred = tree_classifier.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'Cross-Validation Accuracy': cross_val_scores.mean(),\n",
    "        'Cross-Validation Error Rate': error_rates.mean(),\n",
    "        'Test Accuracy': accuracy,\n",
    "        'Test Error Rate': 1 - accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "\n",
    "results1 = {}\n",
    "\n",
    "for name, df in datasets1.items():\n",
    "    results1[name] = evaluate_decision_tree(df)\n",
    "\n",
    "comparison_df1 = pd.DataFrame(results1).transpose()\n",
    "print(\"**************DT - 0.1_0.06**************\\n\")\n",
    "print(comparison_df1)\n",
    "\n",
    "results2 = {}\n",
    "\n",
    "for name, df in datasets2.items():\n",
    "    results2[name] = evaluate_decision_tree(df)\n",
    "\n",
    "comparison_df2 = pd.DataFrame(results2).transpose()\n",
    "print(\"\\n\\n**************DT - Majority clustering**************\\n\")\n",
    "print(comparison_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results1 = {}\n",
    "results2 = {}\n",
    "\n",
    "metric_name = 'Euclidean'\n",
    "metric_func = lambda X_train, X_test: np.sqrt(((X_train.values[:, np.newaxis] - X_test.values) ** 2).sum(axis=2))\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, df in datasets1.items():\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    cv_accuracies = []\n",
    "    test_accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    confusion_matrices = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y_encoded[train_index], y_encoded[test_index]\n",
    "        \n",
    "        distances = metric_func(X_train, X_test)\n",
    "        \n",
    "        nearest_indices = np.argmin(distances, axis=0)\n",
    "        \n",
    "        predicted_labels = y_train[nearest_indices]\n",
    "        \n",
    "        cv_accuracy = accuracy_score(y_test, predicted_labels)\n",
    "        cv_error_rate = 1 - cv_accuracy\n",
    "        \n",
    "        cv_accuracies.append(cv_accuracy)\n",
    "      \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=33)\n",
    "    \n",
    "    distances = metric_func(X_train, X_test)\n",
    "    nearest_indices = np.argmin(distances, axis=0)\n",
    "    nearest_labels = y_encoded[nearest_indices]\n",
    "    predicted_labels = y_train[nearest_indices] \n",
    "    precision = precision_score(y_test, predicted_labels, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, predicted_labels, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, predicted_labels, average='weighted', zero_division=0)\n",
    "    confusion_matrix_result = confusion_matrix(y_test, predicted_labels)\n",
    "        \n",
    "    test_accuracy =accuracy_score(y_test, predicted_labels)\n",
    "\n",
    "    results1[name] = {\n",
    "        'Cross-Validation Accuracy': np.mean(cv_accuracies),\n",
    "        'Cross-Validation Error Rate': np.mean([1 - acc for acc in cv_accuracies]),\n",
    "        'Test Accuracy': (test_accuracy),\n",
    "        'Test Error Rate': (1 - test_accuracy),\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score':f1,\n",
    "        'Confusion Matrix': confusion_matrix_result\n",
    "    }\n",
    "\n",
    "\n",
    "for name, df in datasets2.items():\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    cv_accuracies = []\n",
    "    test_accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    confusion_matrices = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y_encoded[train_index], y_encoded[test_index]\n",
    "        \n",
    "        distances = metric_func(X_train, X_test)\n",
    "        \n",
    "        nearest_indices = np.argmin(distances, axis=0)\n",
    "        \n",
    "        predicted_labels = y_train[nearest_indices]\n",
    "        \n",
    "        cv_accuracy = accuracy_score(y_test, predicted_labels)\n",
    "        cv_error_rate = 1 - cv_accuracy\n",
    "        \n",
    "        cv_accuracies.append(cv_accuracy)\n",
    "      \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=33)\n",
    "    \n",
    "    distances = metric_func(X_train, X_test)\n",
    "    nearest_indices = np.argmin(distances, axis=0)\n",
    "    nearest_labels = y_encoded[nearest_indices]\n",
    "    predicted_labels = y_train[nearest_indices] \n",
    "    precision = precision_score(y_test, predicted_labels, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, predicted_labels, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, predicted_labels, average='weighted', zero_division=0)\n",
    "    confusion_matrix_result = confusion_matrix(y_test, predicted_labels)\n",
    "        \n",
    "    test_accuracy =accuracy_score(y_test, predicted_labels)\n",
    "\n",
    "    results2[name] = {\n",
    "        'Cross-Validation Accuracy': np.mean(cv_accuracies),\n",
    "        'Cross-Validation Error Rate': np.mean([1 - acc for acc in cv_accuracies]),\n",
    "        'Test Accuracy': (test_accuracy),\n",
    "        'Test Error Rate': (1 - test_accuracy),\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score':f1,\n",
    "        'Confusion Matrix': confusion_matrix_result\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "results_df1 = pd.DataFrame(results1).transpose()\n",
    "results_df2 = pd.DataFrame(results2).transpose()\n",
    "\n",
    "print(\"**************Distance based - 0.1_0.06**************\\n\")\n",
    "print(results_df1)\n",
    "\n",
    "print(\"\\n\\n**************Distance based - Majority clustering**************\\n\")\n",
    "print(results_df2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unify_features(test_sample, train_data, train_labels):\n",
    "    test_features = test_sample.values\n",
    "    matching_indices = np.all(train_data == test_features, axis=1)\n",
    "    if matching_indices.any():\n",
    "        matching_labels = train_labels[matching_indices]\n",
    "        return np.unique(matching_labels)\n",
    "    return [\"unknown\"]\n",
    "\n",
    "def evaluate_unification(df, random_state=42):\n",
    "    df = df.copy()\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    cv_accuracies = []\n",
    "    test_accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    confusion_matrices = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y_encoded[train_index], y_encoded[test_index]\n",
    "        \n",
    "        predicted_labels = []\n",
    "        for i in range(X_test.shape[0]):\n",
    "            label = unify_features(X_test.iloc[i], X_train, y_train)\n",
    "            predicted_labels.append(label)\n",
    "        \n",
    "        predicted_labels = [item for sublist in predicted_labels for item in sublist]\n",
    "        \n",
    "        valid_indices = [i for i, label in enumerate(predicted_labels) if label != \"unknown\" and i < len(y_test)]\n",
    "        \n",
    "        if not valid_indices:\n",
    "            continue\n",
    "        \n",
    "        valid_predictions = np.array([predicted_labels[i] for i in valid_indices])\n",
    "        valid_truth = np.array([y_test[i] for i in valid_indices])\n",
    "        \n",
    "        cv_accuracy = accuracy_score(valid_truth, valid_predictions)\n",
    "       \n",
    "        cv_accuracies.append(cv_accuracy)\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=2)\n",
    "\n",
    "    predicted_labels = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        label = unify_features(X_test.iloc[i], X_train, y_train)\n",
    "        predicted_labels.append(label)\n",
    "    predicted_labels = [item for sublist in predicted_labels for item in sublist]\n",
    "        \n",
    "    valid_indices = [i for i, label in enumerate(predicted_labels) if label != \"unknown\" and i < len(y_test)]\n",
    "        \n",
    "    valid_predictions = np.array([predicted_labels[i] for i in valid_indices])\n",
    "    valid_truth = np.array([y_test[i] for i in valid_indices])\n",
    "        \n",
    "    precision = precision_score(valid_truth, valid_predictions, average='weighted', zero_division=0)\n",
    "    recall = recall_score(valid_truth, valid_predictions, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(valid_truth, valid_predictions, average='weighted', zero_division=0)\n",
    "    confusion_matrix_result = confusion_matrix(valid_truth, valid_predictions)    \n",
    "    test_accuracy = accuracy_score(valid_truth, valid_predictions)\n",
    "\n",
    "    return {\n",
    "        'Cross-Validation Accuracy': np.mean(cv_accuracies),\n",
    "        'Cross-Validation Error Rate': np.mean([1 - acc for acc in cv_accuracies]),\n",
    "        'Test Accuracy': test_accuracy,\n",
    "        'Test Error Rate': 1 - test_accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': confusion_matrix_result\n",
    "    }\n",
    "\n",
    "\n",
    "results1 = {}\n",
    "results2 = {}\n",
    "\n",
    "for name, df in datasets1.items():\n",
    "    results1[name] = evaluate_unification(df)\n",
    "\n",
    "results_df1 = pd.DataFrame(results1).transpose()\n",
    "\n",
    "for name, df in datasets2.items():\n",
    "    results2[name] = evaluate_unification(df)\n",
    "\n",
    "results_df2 = pd.DataFrame(results2).transpose()\n",
    "\n",
    "print(\"**************Unification - 0.1_0.06**************\\n\")\n",
    "print(results_df1)\n",
    "\n",
    "print(\"\\n\\n**************Unification - Majority clustering**************\\n\")\n",
    "print(results_df2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIN data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_knn(df, random_state=33):\n",
    "    df = df.copy()\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    for col in X:\n",
    "        X[col] = label_encoder.fit_transform(X[col])\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)  \n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=random_state)\n",
    "    \n",
    "    knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "    \n",
    "    cross_val_scores = cross_val_score(knn_classifier, X_train, y_train, cv=kf, scoring='accuracy')\n",
    "    error_rates = 1 - cross_val_scores\n",
    "    \n",
    "    knn_classifier.fit(X_train, y_train)\n",
    "    y_pred = knn_classifier.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'Cross-Validation Accuracy': cross_val_scores.mean(),\n",
    "        'Cross-Validation Error Rate': error_rates.mean(),\n",
    "        'Test Accuracy': accuracy,\n",
    "        'Test Error Rate': 1 - accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "results1 = {}\n",
    "\n",
    "datasets1 = {\n",
    "    'df_bin': df_bin1,\n",
    "    'df_char_bin': df_char_bin1,\n",
    "    'df_quality_bin': df_quality_bin1\n",
    "}\n",
    "datasets2 = {\n",
    "    'df_bin': df_bin2,\n",
    "    'df_char_bin': df_char_bin2,\n",
    "    'df_quality_raw': df_quality_bin2\n",
    "}\n",
    "\n",
    "for name, df in datasets1.items():\n",
    "    results1[name] = evaluate_knn(df)\n",
    "\n",
    "comparison_df1 = pd.DataFrame(results1).transpose()\n",
    "print(\"**************KNN - 0.1_0.06**************\\n\")\n",
    "print(comparison_df1)\n",
    "\n",
    "\n",
    "results2 = {}\n",
    "\n",
    "for name, df in datasets2.items():\n",
    "    results2[name] = evaluate_knn(df)\n",
    "\n",
    "comparison_df2 = pd.DataFrame(results2).transpose()\n",
    "print(\"\\n\\n**************KNN - Majority clustering**************\\n\")\n",
    "print(comparison_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_decision_tree(df, random_state=33):\n",
    "    df = df.copy()\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    for col in X:\n",
    "        X[col] = label_encoder.fit_transform(X[col])\n",
    "        \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)  \n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=random_state)\n",
    "    \n",
    "    tree_classifier = DecisionTreeClassifier(random_state=random_state)\n",
    "    \n",
    "    cross_val_scores = cross_val_score(tree_classifier, X_train, y_train, cv=kf, scoring='accuracy')\n",
    "    error_rates = 1 - cross_val_scores\n",
    "    \n",
    "    tree_classifier.fit(X_train, y_train)\n",
    "    y_pred = tree_classifier.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'Cross-Validation Accuracy': cross_val_scores.mean(),\n",
    "        'Cross-Validation Error Rate': error_rates.mean(),\n",
    "        'Test Accuracy': accuracy,\n",
    "        'Test Error Rate': 1 - accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "results1 = {}\n",
    "datasets1 = {\n",
    "    'df_bin': df_bin1,\n",
    "    'df_char_bin': df_char_bin1,\n",
    "    'df_quality_bin': df_quality_bin1\n",
    "}\n",
    "datasets2 = {\n",
    "    'df_bin': df_bin2,\n",
    "    'df_char_bin': df_char_bin2,\n",
    "    'df_quality_raw': df_quality_bin2\n",
    "}\n",
    "for name, df in datasets1.items():\n",
    "    results1[name] = evaluate_decision_tree(df)\n",
    "\n",
    "comparison_df1 = pd.DataFrame(results1).transpose()\n",
    "print(\"**************DT - 0.1_0.06**************\\n\")\n",
    "print(comparison_df1)\n",
    "\n",
    "results2 = {}\n",
    "\n",
    "for name, df in datasets2.items():\n",
    "    results2[name] = evaluate_decision_tree(df)\n",
    "\n",
    "comparison_df2 = pd.DataFrame(results2).transpose()\n",
    "print(\"\\n\\n**************DT - Majority clustering**************\\n\")\n",
    "print(comparison_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets1 = {\n",
    "    'df_bin': df_bin1,\n",
    "    'df_char_bin': df_char_bin1,\n",
    "    'df_quality_bin': df_quality_bin1\n",
    "}\n",
    "datasets2 = {\n",
    "    'df_bin': df_bin2,\n",
    "    'df_char_bin': df_char_bin2,\n",
    "    'df_quality_bin': df_quality_bin2\n",
    "}\n",
    "results1 = {}\n",
    "results2 = {}\n",
    "\n",
    "\n",
    "metric_name = 'Euclidean'\n",
    "metric_func = lambda X_train, X_test: np.sqrt(((X_train.values[:, np.newaxis] - X_test.values) ** 2).sum(axis=2))\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, df in datasets1.items():\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    for col in X:\n",
    "        X[col] = label_encoder.fit_transform(X[col])\n",
    "        \n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    cv_accuracies = []\n",
    "    test_accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    confusion_matrices = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y_encoded[train_index], y_encoded[test_index]\n",
    "        \n",
    "        distances = metric_func(X_train, X_test)\n",
    "        \n",
    "        nearest_indices = np.argmin(distances, axis=0)\n",
    "        \n",
    "        predicted_labels = y_train[nearest_indices]\n",
    "        \n",
    "        cv_accuracy = accuracy_score(y_test, predicted_labels)\n",
    "        cv_error_rate = 1 - cv_accuracy\n",
    "        \n",
    "        cv_accuracies.append(cv_accuracy)\n",
    "      \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=33)\n",
    "    \n",
    "    distances = metric_func(X_train, X_test)\n",
    "    nearest_indices = np.argmin(distances, axis=0)\n",
    "    nearest_labels = y_encoded[nearest_indices]\n",
    "    predicted_labels = y_train[nearest_indices] \n",
    "    precision = precision_score(y_test, predicted_labels, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, predicted_labels, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, predicted_labels, average='weighted', zero_division=0)\n",
    "    confusion_matrix_result = confusion_matrix(y_test, predicted_labels)\n",
    "        \n",
    "    test_accuracy =accuracy_score(y_test, predicted_labels)\n",
    "\n",
    "    results1[name] = {\n",
    "        'Cross-Validation Accuracy': np.mean(cv_accuracies),\n",
    "        'Cross-Validation Error Rate': np.mean([1 - acc for acc in cv_accuracies]),\n",
    "        'Test Accuracy': (test_accuracy),\n",
    "        'Test Error Rate': (1 - test_accuracy),\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score':f1,\n",
    "        'Confusion Matrix': confusion_matrix_result\n",
    "    }\n",
    "\n",
    "results_df1 = pd.DataFrame(results1).transpose()\n",
    "print(\"***************Distance based - 0.1_0.06*************\")\n",
    "print(results_df1)\n",
    "\n",
    "\n",
    "\n",
    "for name, df in datasets2.items():\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    for col in X:\n",
    "        X[col] = label_encoder.fit_transform(X[col])\n",
    "        \n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    cv_accuracies = []\n",
    "    test_accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    confusion_matrices = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y_encoded[train_index], y_encoded[test_index]\n",
    "        \n",
    "        distances = metric_func(X_train, X_test)\n",
    "        \n",
    "        nearest_indices = np.argmin(distances, axis=0)\n",
    "        \n",
    "        predicted_labels = y_train[nearest_indices]\n",
    "        \n",
    "        cv_accuracy = accuracy_score(y_test, predicted_labels)\n",
    "        cv_error_rate = 1 - cv_accuracy\n",
    "        \n",
    "        cv_accuracies.append(cv_accuracy)\n",
    "      \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=33)\n",
    "    \n",
    "    distances = metric_func(X_train, X_test)\n",
    "    nearest_indices = np.argmin(distances, axis=0)\n",
    "    nearest_labels = y_encoded[nearest_indices]\n",
    "    predicted_labels = y_train[nearest_indices] \n",
    "    precision = precision_score(y_test, predicted_labels, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, predicted_labels, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, predicted_labels, average='weighted', zero_division=0)\n",
    "    confusion_matrix_result = confusion_matrix(y_test, predicted_labels)\n",
    "        \n",
    "    test_accuracy =accuracy_score(y_test, predicted_labels)\n",
    "\n",
    "    results2[name] = {\n",
    "        'Cross-Validation Accuracy': np.mean(cv_accuracies),\n",
    "        'Cross-Validation Error Rate': np.mean([1 - acc for acc in cv_accuracies]),\n",
    "        'Test Accuracy': (test_accuracy),\n",
    "        'Test Error Rate': (1 - test_accuracy),\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score':f1,\n",
    "        'Confusion Matrix': confusion_matrix_result\n",
    "    }\n",
    "\n",
    "results_df2 = pd.DataFrame(results2).transpose()\n",
    "print(\"\\n\\n***************Distance based - Majority cluster*************\")\n",
    "\n",
    "print(results_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unify_features(test_sample, train_data, train_labels):\n",
    "    test_features = test_sample.values\n",
    "    matching_indices = np.all(train_data == test_features, axis=1)\n",
    "    if matching_indices.any():\n",
    "        matching_labels = train_labels[matching_indices]\n",
    "        return np.unique(matching_labels)\n",
    "    return [\"unknown\"]\n",
    "\n",
    "def evaluate_unification(df, random_state=42):\n",
    "    df = df.copy()\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    cv_accuracies = []\n",
    "    test_accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    confusion_matrices = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y_encoded[train_index], y_encoded[test_index]\n",
    "        \n",
    "        predicted_labels = []\n",
    "        for i in range(X_test.shape[0]):\n",
    "            label = unify_features(X_test.iloc[i], X_train, y_train)\n",
    "            predicted_labels.append(label)\n",
    "        \n",
    "        predicted_labels = [item for sublist in predicted_labels for item in sublist]\n",
    "        \n",
    "        valid_indices = [i for i, label in enumerate(predicted_labels) if label != \"unknown\" and i < len(y_test)]\n",
    "        \n",
    "        if not valid_indices:\n",
    "            continue\n",
    "        \n",
    "        valid_predictions = np.array([predicted_labels[i] for i in valid_indices])\n",
    "        valid_truth = np.array([y_test[i] for i in valid_indices])\n",
    "        \n",
    "        cv_accuracy = accuracy_score(valid_truth, valid_predictions)\n",
    "       \n",
    "        cv_accuracies.append(cv_accuracy)\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=2)\n",
    "    predicted_labels = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        label = unify_features(X_test.iloc[i], X_train, y_train)\n",
    "        predicted_labels.append(label)\n",
    "    predicted_labels = [item for sublist in predicted_labels for item in sublist]\n",
    "        \n",
    "    valid_indices = [i for i, label in enumerate(predicted_labels) if label != \"unknown\" and i < len(y_test)]\n",
    "        \n",
    "    valid_predictions = np.array([predicted_labels[i] for i in valid_indices])\n",
    "    valid_truth = np.array([y_test[i] for i in valid_indices])\n",
    "    precision = precision_score(valid_truth, valid_predictions, average='weighted', zero_division=0)\n",
    "    recall = recall_score(valid_truth, valid_predictions, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(valid_truth, valid_predictions, average='weighted', zero_division=0)\n",
    "    confusion_matrix_result = confusion_matrix(valid_truth, valid_predictions)    \n",
    "    test_accuracy = accuracy_score(valid_truth, valid_predictions)\n",
    "\n",
    "    return {\n",
    "        'Cross-Validation Accuracy': np.mean(cv_accuracies),\n",
    "        'Cross-Validation Error Rate': np.mean([1 - acc for acc in cv_accuracies]),\n",
    "        'Test Accuracy': test_accuracy,\n",
    "        'Test Error Rate': 1 - test_accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': confusion_matrix_result\n",
    "    }\n",
    "\n",
    "datasets1 = {\n",
    "    'df_bin': df_bin1,\n",
    "    'df_char_bin': df_char_bin1,\n",
    "    'df_quality_bin': df_quality_bin1\n",
    "}\n",
    "datasets2 = {\n",
    "    'df_bin': df_bin2,\n",
    "    'df_char_bin': df_char_bin2,\n",
    "    'df_quality_bin': df_quality_bin2\n",
    "}\n",
    "results1 = {}\n",
    "results2 = {}\n",
    "\n",
    "for name, df in datasets1.items():\n",
    "    results1[name] = evaluate_unification(df)\n",
    "for name, df in datasets2.items():\n",
    "    results2[name] = evaluate_unification(df)\n",
    "\n",
    "results_df1 = pd.DataFrame(results1).transpose()\n",
    "results_df2 = pd.DataFrame(results2).transpose()\n",
    "\n",
    "print(\"***************Unification - 0.1_0.06**********************\")\n",
    "print(results_df1)\n",
    "print(\"***************Unification - majority voting**********************\")\n",
    "print(results_df2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_normalised1 = pd.read_csv(\"NormalisedDataset_01006.csv\")\n",
    "df_normalised2 = pd.read_csv(\"NormalisedDataset_majoritycluster.csv\")\n",
    "\n",
    "df_normalised_quality1 = df_normalised1[['Completeness_normalized', 'Conciseness_normalized',  'ClassImbRatio_normalized', 'ClassOverlapPerc_normalized',\n",
    "       'OutlierPerc_normalized',  'LabelIssuesPerc_normalized', 'FeatureAlgo_normalized']]\n",
    "df_normalised_char1 = df_normalised1[[ 'cor.mean_normalized', 'cov.mean_normalized',\n",
    "       'eigenvalues.mean_normalized', 'g_mean.mean_normalized',\n",
    "       'h_mean.mean_normalized', 'iq_range.mean_normalized',\n",
    "       'kurtosis.mean_normalized', 'mad.mean_normalized',\n",
    "       'max.mean_normalized', 'mean.mean_normalized', 'median.mean_normalized',\n",
    "       'min.mean_normalized', 'nr_cor_attr_normalized', 'nr_norm_normalized',\n",
    "       'nr_outliers_normalized', 'range.mean_normalized', 'sd.mean_normalized',\n",
    "       'skewness.mean_normalized', 'sparsity.mean_normalized',\n",
    "       't_mean.mean_normalized', 'var.mean_normalized',\n",
    "        'attr_to_inst_normalized',\n",
    "       'inst_to_attr_normalized', 'nr_attr_normalized', 'nr_bin_normalized',\n",
    "       'nr_inst_normalized', 'nr_num_normalized', 'attr_conc.mean_normalized',\n",
    "       'attr_ent.mean_normalized', 'nUnique_normalized', 'ena_normalized', 'snr.mean_normalized',\n",
    "       'cEntropy_normalized', 'FeatureAlgo_normalized']]\n",
    "df_normalised_quality2 = df_normalised2[['Completeness_normalized', 'Conciseness_normalized',  'ClassImbRatio_normalized', 'ClassOverlapPerc_normalized',\n",
    "       'OutlierPerc_normalized',  'LabelIssuesPerc_normalized', 'FeatureAlgo_normalized']]\n",
    "df_normalised_char2 = df_normalised2[[ 'cor.mean_normalized', 'cov.mean_normalized',\n",
    "       'eigenvalues.mean_normalized', 'g_mean.mean_normalized',\n",
    "       'h_mean.mean_normalized', 'iq_range.mean_normalized',\n",
    "       'kurtosis.mean_normalized', 'mad.mean_normalized',\n",
    "       'max.mean_normalized', 'mean.mean_normalized', 'median.mean_normalized',\n",
    "       'min.mean_normalized', 'nr_cor_attr_normalized', 'nr_norm_normalized',\n",
    "       'nr_outliers_normalized', 'range.mean_normalized', 'sd.mean_normalized',\n",
    "       'skewness.mean_normalized', 'sparsity.mean_normalized',\n",
    "       't_mean.mean_normalized', 'var.mean_normalized',\n",
    "        'attr_to_inst_normalized',\n",
    "       'inst_to_attr_normalized', 'nr_attr_normalized', 'nr_bin_normalized',\n",
    "       'nr_inst_normalized', 'nr_num_normalized', 'attr_conc.mean_normalized',\n",
    "       'attr_ent.mean_normalized', 'nUnique_normalized', 'ena_normalized', 'snr.mean_normalized',\n",
    "       'cEntropy_normalized', 'FeatureAlgo_normalized']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_knn(df, random_state=33):\n",
    "    df = df.copy()\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)  \n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=random_state)\n",
    "    \n",
    "    knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "    \n",
    "    cross_val_scores = cross_val_score(knn_classifier, X_train, y_train, cv=kf, scoring='accuracy')\n",
    "    error_rates = 1 - cross_val_scores\n",
    "    \n",
    "    knn_classifier.fit(X_train, y_train)\n",
    "    y_pred = knn_classifier.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'Cross-Validation Accuracy': cross_val_scores.mean(),\n",
    "        'Cross-Validation Error Rate': error_rates.mean(),\n",
    "        'Test Accuracy': accuracy,\n",
    "        'Test Error Rate': 1 - accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "datasets1 = {\n",
    "    'df_normalised': df_normalised1,\n",
    "    'df_normalised_char': df_normalised_char1,\n",
    "    'df_normalised_quality': df_normalised_quality1\n",
    "}\n",
    "\n",
    "results1 = {}\n",
    "\n",
    "for name, df in datasets1.items():\n",
    "    results1[name] = evaluate_knn(df)\n",
    "\n",
    "comparison_df1 = pd.DataFrame(results1).transpose()\n",
    "print(\"**************KNN - 0.1_0.06**************\\n\")\n",
    "print(comparison_df1)\n",
    "\n",
    "datasets2 = {\n",
    "    'df_normalised': df_normalised2,\n",
    "    'df_normalised_char': df_normalised_char2,\n",
    "    'df_normalised_quality': df_normalised_quality2\n",
    "}\n",
    "\n",
    "results2 = {}\n",
    "\n",
    "for name, df in datasets2.items():\n",
    "    results2[name] = evaluate_knn(df)\n",
    "\n",
    "comparison_df2 = pd.DataFrame(results2).transpose()\n",
    "print(\"\\n\\n**************KNN - Majority clustering**************\\n\")\n",
    "print(comparison_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_decision_tree(df, random_state=33):\n",
    "    df = df.copy()\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)  \n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=random_state)\n",
    "    \n",
    "    tree_classifier = DecisionTreeClassifier(random_state=random_state)\n",
    "    \n",
    "    cross_val_scores = cross_val_score(tree_classifier, X_train, y_train, cv=kf, scoring='accuracy')\n",
    "    error_rates = 1 - cross_val_scores\n",
    "    \n",
    "    tree_classifier.fit(X_train, y_train)\n",
    "    y_pred = tree_classifier.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'Cross-Validation Accuracy': cross_val_scores.mean(),\n",
    "        'Cross-Validation Error Rate': error_rates.mean(),\n",
    "        'Test Accuracy': accuracy,\n",
    "        'Test Error Rate': 1 - accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "\n",
    "results1 = {}\n",
    "\n",
    "for name, df in datasets1.items():\n",
    "    results1[name] = evaluate_decision_tree(df)\n",
    "\n",
    "comparison_df1 = pd.DataFrame(results1).transpose()\n",
    "print(\"**************DT - 0.1_0.06**************\\n\")\n",
    "print(comparison_df1)\n",
    "\n",
    "results2 = {}\n",
    "\n",
    "for name, df in datasets2.items():\n",
    "    results2[name] = evaluate_decision_tree(df)\n",
    "\n",
    "comparison_df2 = pd.DataFrame(results2).transpose()\n",
    "print(\"\\n\\n**************DT - Majority clustering**************\\n\")\n",
    "print(comparison_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets1 = {\n",
    "    'df_normalised': df_normalised1,\n",
    "    'df_normalised_char': df_normalised_char1,\n",
    "    'df_normalised_quality': df_normalised_quality1\n",
    "}\n",
    "\n",
    "results1 = {}\n",
    "results2 = {}\n",
    "\n",
    "datasets2 = {\n",
    "    'df_normalised': df_normalised2,\n",
    "    'df_normalised_char': df_normalised_char2,\n",
    "    'df_normalised_quality': df_normalised_quality2\n",
    "}\n",
    "\n",
    "\n",
    "metric_name = 'Euclidean'\n",
    "metric_func = lambda X_train, X_test: np.sqrt(((X_train.values[:, np.newaxis] - X_test.values) ** 2).sum(axis=2))\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=33)\n",
    "\n",
    "for name, df in datasets1.items():\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    cv_accuracies = []\n",
    "    test_accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    confusion_matrices = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y_encoded[train_index], y_encoded[test_index]\n",
    "        \n",
    "        distances = metric_func(X_train, X_test)\n",
    "        \n",
    "        nearest_indices = np.argmin(distances, axis=0)\n",
    "        \n",
    "        predicted_labels = y_train[nearest_indices]\n",
    "        \n",
    "        cv_accuracy = accuracy_score(y_test, predicted_labels)\n",
    "        cv_error_rate = 1 - cv_accuracy\n",
    "        \n",
    "        cv_accuracies.append(cv_accuracy)\n",
    "      \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=33)\n",
    "    \n",
    "    distances = metric_func(X_train, X_test)\n",
    "    nearest_indices = np.argmin(distances, axis=0)\n",
    "    nearest_labels = y_encoded[nearest_indices]\n",
    "    predicted_labels = y_train[nearest_indices] \n",
    "    precision = precision_score(y_test, predicted_labels, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, predicted_labels, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, predicted_labels, average='weighted', zero_division=0)\n",
    "    confusion_matrix_result = confusion_matrix(y_test, predicted_labels)\n",
    "        \n",
    "    test_accuracy =accuracy_score(y_test, predicted_labels)\n",
    "\n",
    "    results1[name] = {\n",
    "        'Cross-Validation Accuracy': np.mean(cv_accuracies),\n",
    "        'Cross-Validation Error Rate': np.mean([1 - acc for acc in cv_accuracies]),\n",
    "        'Test Accuracy': (test_accuracy),\n",
    "        'Test Error Rate': (1 - test_accuracy),\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score':f1,\n",
    "        'Confusion Matrix': confusion_matrix_result\n",
    "    }\n",
    "\n",
    "for name, df in datasets2.items():\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    cv_accuracies = []\n",
    "    test_accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    confusion_matrices = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y_encoded[train_index], y_encoded[test_index]\n",
    "        \n",
    "        distances = metric_func(X_train, X_test)\n",
    "        \n",
    "        nearest_indices = np.argmin(distances, axis=0)\n",
    "        \n",
    "        predicted_labels = y_train[nearest_indices]\n",
    "        \n",
    "        cv_accuracy = accuracy_score(y_test, predicted_labels)\n",
    "        cv_error_rate = 1 - cv_accuracy\n",
    "        \n",
    "        cv_accuracies.append(cv_accuracy)\n",
    "      \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=33)\n",
    "    \n",
    "    distances = metric_func(X_train, X_test)\n",
    "    nearest_indices = np.argmin(distances, axis=0)\n",
    "    nearest_labels = y_encoded[nearest_indices]\n",
    "    predicted_labels = y_train[nearest_indices] \n",
    "    precision = precision_score(y_test, predicted_labels, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, predicted_labels, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, predicted_labels, average='weighted', zero_division=0)\n",
    "    confusion_matrix_result = confusion_matrix(y_test, predicted_labels)\n",
    "        \n",
    "    test_accuracy =accuracy_score(y_test, predicted_labels)\n",
    "\n",
    "    results2[name] = {\n",
    "        'Cross-Validation Accuracy': np.mean(cv_accuracies),\n",
    "        'Cross-Validation Error Rate': np.mean([1 - acc for acc in cv_accuracies]),\n",
    "        'Test Accuracy': (test_accuracy),\n",
    "        'Test Error Rate': (1 - test_accuracy),\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score':f1,\n",
    "        'Confusion Matrix': confusion_matrix_result\n",
    "    }\n",
    "\n",
    "\n",
    "results_df1 = pd.DataFrame(results1).transpose()\n",
    "\n",
    "print(results_df1)\n",
    "\n",
    "\n",
    "results_df2 = pd.DataFrame(results2).transpose()\n",
    "\n",
    "print(results_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unify_features(test_sample, train_data, train_labels):\n",
    "    test_features = test_sample.values\n",
    "    matching_indices = np.all(train_data == test_features, axis=1)\n",
    "    if matching_indices.any():\n",
    "        matching_labels = train_labels[matching_indices]\n",
    "        return np.unique(matching_labels)\n",
    "    return [\"unknown\"]\n",
    "\n",
    "def evaluate_unification(df, random_state=33):\n",
    "    df = df.copy()\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    cv_accuracies = []\n",
    "    test_accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    confusion_matrices = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y_encoded[train_index], y_encoded[test_index]\n",
    "        \n",
    "        predicted_labels = []\n",
    "        for i in range(X_test.shape[0]):\n",
    "            label = unify_features(X_test.iloc[i], X_train, y_train)\n",
    "            predicted_labels.append(label)\n",
    "        \n",
    "        predicted_labels = [item for sublist in predicted_labels for item in sublist]\n",
    "        \n",
    "        valid_indices = [i for i, label in enumerate(predicted_labels) if label != \"unknown\" and i < len(y_test)]\n",
    "        \n",
    "        if not valid_indices:\n",
    "            continue\n",
    "        \n",
    "        valid_predictions = np.array([predicted_labels[i] for i in valid_indices])\n",
    "        valid_truth = np.array([y_test[i] for i in valid_indices])\n",
    "        \n",
    "        cv_accuracy = accuracy_score(valid_truth, valid_predictions)\n",
    "       \n",
    "        cv_accuracies.append(cv_accuracy)\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=2)\n",
    "    predicted_labels = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        label = unify_features(X_test.iloc[i], X_train, y_train)\n",
    "        predicted_labels.append(label)\n",
    "    predicted_labels = [item for sublist in predicted_labels for item in sublist]\n",
    "        \n",
    "    valid_indices = [i for i, label in enumerate(predicted_labels) if label != \"unknown\" and i < len(y_test)]\n",
    "        \n",
    "    valid_predictions = np.array([predicted_labels[i] for i in valid_indices])\n",
    "    valid_truth = np.array([y_test[i] for i in valid_indices])\n",
    "        \n",
    "    precision = precision_score(valid_truth, valid_predictions, average='weighted', zero_division=0)\n",
    "    recall = recall_score(valid_truth, valid_predictions, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(valid_truth, valid_predictions, average='weighted', zero_division=0)\n",
    "    confusion_matrix_result = confusion_matrix(valid_truth, valid_predictions)    \n",
    "    test_accuracy = accuracy_score(valid_truth, valid_predictions)\n",
    "\n",
    "    return {\n",
    "        'Cross-Validation Accuracy': np.mean(cv_accuracies),\n",
    "        'Cross-Validation Error Rate': np.mean([1 - acc for acc in cv_accuracies]),\n",
    "        'Test Accuracy': test_accuracy,\n",
    "        'Test Error Rate': 1 - test_accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': confusion_matrix_result\n",
    "    }\n",
    "\n",
    "\n",
    "results1 = {}\n",
    "results2 = {}\n",
    "\n",
    "for name, df in datasets1.items():\n",
    "    results1[name] = evaluate_unification(df)\n",
    "for name, df in datasets2.items():\n",
    "    results2[name] = evaluate_unification(df)\n",
    "\n",
    "results_df1 = pd.DataFrame(results1).transpose()\n",
    "results_df2 = pd.DataFrame(results2).transpose()\n",
    "\n",
    "print(\"***************Unification - 0.1_0.06**********************\")\n",
    "print(results_df1)\n",
    "print(\"***************Unification - majority voting**********************\")\n",
    "print(results_df2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".deeplearningClass",
   "language": "python",
   "name": ".deeplearningclass"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68ad885796f6f0b806db95b15cf2a015a244c9adabe8d6cffa3fc143090837a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
