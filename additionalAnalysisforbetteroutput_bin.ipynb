{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all imports\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import jenkspy\n",
    "from ast import literal_eval\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, _tree\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import cross_val_score, ShuffleSplit\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"df_mfeatures.csv\")\n",
    "df2 = pd.read_csv(\"df_ensemble_allfeatures.csv\")\n",
    "\n",
    "binDF = pd.read_csv(\"BinnedMetaFeatures.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mfeatures = df1[['File', 'Completeness', 'Conciseness', 'cor.mean', 'cov.mean',\n",
    "       'eigenvalues.mean', 'g_mean.mean', 'h_mean.mean', 'iq_range.mean', 'kurtosis.mean', \n",
    "       'mad.mean', 'max.mean', 'mean.mean', 'median.mean', 'min.mean', 'nr_cor_attr',\n",
    "       'nr_norm', 'nr_outliers', 'range.mean', 'sd.mean', 'skewness.mean',\n",
    "       'sparsity.mean', 't_mean.mean',   'var.mean', 'ClassImbRatio',\n",
    "       'ClassOverlapPerc', 'OutlierPerc', 'attr_to_inst', 'inst_to_attr', 'nr_attr',\n",
    "        'nr_bin',  'nr_inst', 'nr_num', 'attr_conc.mean', 'attr_ent.mean', 'ena', 'nUnique' ,'snr.mean', 'cEntropy','LabelIssuesPerc']] #SyntaxAccuracy removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Majority voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Needs to be run only one - writes majority voted labels into a file\n",
    "path = os.getcwd()+'/optimalalphabeta/dearr_cont/'  \n",
    "csv_files = glob(os.path.join(path, 'dearr*.csv'))\n",
    "\n",
    "labels_dict = {}\n",
    "\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        dataset_id = row['File']  \n",
    "        best_fs = row['FeatureAlgo']     \n",
    "        \n",
    "        if dataset_id not in labels_dict:\n",
    "            labels_dict[dataset_id] = []\n",
    "        labels_dict[dataset_id].append(best_fs)\n",
    "\n",
    "majority_labels = {\n",
    "    dataset: Counter(fs_list).most_common(1)[0][0]\n",
    "    for dataset, fs_list in labels_dict.items()\n",
    "}\n",
    "\n",
    "majority_df = pd.DataFrame(list(majority_labels.items()), columns=['File', 'FeatureAlgo'])\n",
    "majority_df.to_csv('majority_voted_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Majority voting based on a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of alpha-beta files in each cluster:\n",
      "0     5\n",
      "1    12\n",
      "2    18\n",
      "3     1\n",
      "Name: Cluster, dtype: int64\n",
      "\n",
      "Detailed files per cluster:\n",
      "\n",
      "Cluster 0 (5 files):\n",
      "['0.0_0.02', '0.0_0.04', '0.0_0.06', '0.0_0.08', '0.0_0.1']\n",
      "\n",
      "Cluster 1 (12 files):\n",
      "['0.02_0.0', '0.02_0.02', '0.02_0.04', '0.02_0.06', '0.02_0.08', '0.02_0.1', '0.04_0.0', '0.04_0.02', '0.04_0.04', '0.04_0.06', '0.04_0.08', '0.04_0.1']\n",
      "\n",
      "Cluster 2 (18 files):\n",
      "['0.06_0.0', '0.06_0.02', '0.06_0.04', '0.06_0.06', '0.06_0.08', '0.06_0.1', '0.08_0.0', '0.08_0.02', '0.08_0.04', '0.08_0.06', '0.08_0.08', '0.08_0.1', '0.1_0.0', '0.1_0.02', '0.1_0.04', '0.1_0.06', '0.1_0.08', '0.1_0.1']\n",
      "\n",
      "Cluster 3 (1 files):\n",
      "['0.0_0.0']\n"
     ]
    }
   ],
   "source": [
    "path = os.getcwd()+'/optimalalphabeta/dearr_cont/'  \n",
    "label_column = 'FeatureAlgo'        \n",
    "output_dim = 2                   \n",
    "n_clusters = 4                   \n",
    "\n",
    "csv_files = sorted(glob(os.path.join(path, 'dearr*.csv')))\n",
    "\n",
    "all_fs_algos = set()\n",
    "fs_distributions = []\n",
    "ab_values = []\n",
    "\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    fs_counts = Counter(df[label_column])\n",
    "    all_fs_algos.update(fs_counts.keys())\n",
    "    fs_distributions.append(fs_counts)\n",
    "    \n",
    "    # Extract alpha, beta from filename like 'dearr0.02_0.04.csv'\n",
    "    ab = file.split('/')[-1].replace('dearr', '').replace('.csv', '')\n",
    "    alpha, beta = map(float, ab.split('_'))\n",
    "    ab_values.append((alpha, beta))\n",
    "\n",
    "fs_algos = sorted(list(all_fs_algos))\n",
    "\n",
    "matrix = []\n",
    "for dist in fs_distributions:\n",
    "    vector = [dist.get(fs, 0) for fs in fs_algos]\n",
    "    matrix.append(vector)\n",
    "\n",
    "df_matrix = pd.DataFrame(matrix, columns=fs_algos)\n",
    "df_matrix.index = [f'{a}_{b}' for a, b in ab_values]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_matrix = scaler.fit_transform(df_matrix)\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "labels = kmeans.fit_predict(scaled_matrix)\n",
    "\n",
    "cluster_df = pd.DataFrame({\n",
    "    'File': df_matrix.index,  # e.g., \"0.02_0.04\"\n",
    "    'Cluster': labels\n",
    "})\n",
    "\n",
    "cluster_counts = cluster_df['Cluster'].value_counts().sort_index()\n",
    "\n",
    "print(\"Number of alpha-beta files in each cluster:\")\n",
    "print(cluster_counts)\n",
    "\n",
    "print(\"\\nDetailed files per cluster:\")\n",
    "for cluster_id, group in cluster_df.groupby('Cluster'):\n",
    "    print(f\"\\nCluster {cluster_id} ({len(group)} files):\")\n",
    "    print(group['File'].tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate majority vote based on cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files are copied into ./optimalalphabeta/clusteredearr/ directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = os.getcwd()+'/optimalalphabeta/clusteredearr/'  # replace with the path to your 36 CSVs\n",
    "csv_files = glob(os.path.join(path, 'dearr*.csv'))\n",
    "\n",
    "labels_dict = {}\n",
    "\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        dataset_id = row['File']  # replace 'Dataset' with your actual column name\n",
    "        best_fs = row['FeatureAlgo']     # replace 'Best_FS' with your actual column name\n",
    "        \n",
    "        if dataset_id not in labels_dict:\n",
    "            labels_dict[dataset_id] = []\n",
    "        labels_dict[dataset_id].append(best_fs)\n",
    "\n",
    "majority_labels = {\n",
    "    dataset: Counter(fs_list).most_common(1)[0][0]\n",
    "    for dataset, fs_list in labels_dict.items()\n",
    "}\n",
    "\n",
    "majority_df = pd.DataFrame(list(majority_labels.items()), columns=['File', 'FeatureAlgo'])\n",
    "majority_df.to_csv('majority_voted_labels_clusteredearr.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maptype(df):\n",
    "    fs_mapping = {\n",
    "    'MI': ('Uni', 'Information'),\n",
    "    'GR': ('Uni', 'Information'),\n",
    "    'fcbf': ('Multi', 'Information'),\n",
    "    'chisquare': ('Uni', 'Dependency'),\n",
    "    'mrmr': ('Multi', 'Dependency'),\n",
    "    'cfs': ('Multi', 'Dependency'),\n",
    "    'relief': ('Uni', 'Distance'),\n",
    "    'relieff': ('Uni', 'Distance'),\n",
    "    'multisurf': ('Multi', 'Distance')\n",
    "    }\n",
    "\n",
    "    def get_setting(algo):\n",
    "        return fs_mapping.get(algo, ('Unknown', 'Unknown'))  # default if not found\n",
    "\n",
    "    df[['FS_Setting', 'FS_Type']] = df['FeatureAlgo'].apply(lambda x: pd.Series(get_setting(x)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fst1_correlation(df1, df2, threshold=0.9):\n",
    "\n",
    "    def correlation_filter(df, threshold):\n",
    "        corr_matrix = df.corr().abs()\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "        return df.drop(columns=to_drop)\n",
    "    \n",
    "    file_col = df2['File']\n",
    "\n",
    "    X_filtered = correlation_filter(df2.drop(columns=['File']), threshold)\n",
    "\n",
    "    X_filtered['File'] = file_col\n",
    "\n",
    "    X_filtered = pd.merge(X_filtered, df1 , on='File', how='inner')  # use 'left', 'right', or 'outer' as needed\n",
    "    return X_filtered.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fst2_rf(df1):\n",
    "    le = LabelEncoder()\n",
    "    X = df1.drop(['File', 'FeatureAlgo','FS_Setting','FS_Type'], axis=1)\n",
    "    X = X.apply(le.fit_transform)\n",
    "    y= df1['FeatureAlgo']\n",
    "    clf = RandomForestClassifier(random_state=42)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    selector = SelectFromModel(clf, threshold='mean', prefit=True)\n",
    "    X_selected = selector.transform(X)\n",
    "\n",
    "    selected_features = X.columns[selector.get_support()]\n",
    "    return selected_features.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('majority_voted_labels_clusteredearr.csv')\n",
    "df2 = pd.read_csv('meta-features-cont.csv')\n",
    "fst1_features = fst1_correlation(df1, df2)\n",
    "\n",
    "\n",
    "merged_df = pd.merge(df1, df2, on='File', how='inner')  # use 'left', 'right', or 'outer' as needed\n",
    "merged_df.to_csv(\"majorityvoteclusteredearrdataset.csv\",index=False)\n",
    "merged_df = maptype(merged_df)\n",
    "fst2_features = fst2_rf(merged_df)\n",
    "\n",
    "common_features = list(set(fst1_features.tolist()).intersection(set(fst2_features)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rules_with_support_confidence(tree_model, feature_names, X, y):\n",
    "    tree_ = tree_model.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "    \n",
    "    paths = []\n",
    "    rules = []\n",
    "\n",
    "    def recurse(node, path, conds):\n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            left_path = path.copy()\n",
    "            left_path.append(f\"({name} <= {threshold:.3f})\")\n",
    "            recurse(tree_.children_left[node], left_path, conds)\n",
    "\n",
    "            right_path = path.copy()\n",
    "            right_path.append(f\"({name} > {threshold:.3f})\")\n",
    "            recurse(tree_.children_right[node], right_path, conds)\n",
    "        else:\n",
    "            path_str = \" and \".join(path)\n",
    "            support = tree_.n_node_samples[node] / tree_.n_node_samples[0]\n",
    "            value = tree_.value[node][0]\n",
    "            predicted_class = np.argmax(value)\n",
    "            confidence = value[predicted_class] / np.sum(value)\n",
    "            rules.append((path_str, predicted_class, support, confidence))\n",
    "\n",
    "    recurse(0, [], rules)\n",
    "\n",
    "    return pd.DataFrame(rules, columns=[\"Rule\", \"Predicted_Class\", \"Support\", \"Confidence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature_Algo Report (common features):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          GR       0.42      0.71      0.53         7\n",
      "          MI       0.00      0.00      0.00         3\n",
      "   chisquare       0.61      0.65      0.63        17\n",
      "        fcbf       1.00      0.40      0.57         5\n",
      "      relief       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.51        35\n",
      "   macro avg       0.41      0.35      0.35        35\n",
      "weighted avg       0.52      0.51      0.49        35\n",
      "\n",
      "                                                 Rule  Predicted_Class  \\\n",
      "0   (LabelIssuesPerc <= 27.500) and (cEntropy <= 5...                3   \n",
      "1   (LabelIssuesPerc <= 27.500) and (cEntropy <= 5...                4   \n",
      "29  (LabelIssuesPerc > 27.500) and (sparsity.mean ...                4   \n",
      "30  (LabelIssuesPerc > 27.500) and (sparsity.mean ...                0   \n",
      "31  (LabelIssuesPerc > 27.500) and (sparsity.mean ...                4   \n",
      "32  (LabelIssuesPerc > 27.500) and (sparsity.mean ...                4   \n",
      "33  (LabelIssuesPerc > 27.500) and (sparsity.mean ...                5   \n",
      "34  (LabelIssuesPerc > 27.500) and (sparsity.mean ...                1   \n",
      "35  (LabelIssuesPerc > 27.500) and (sparsity.mean ...                0   \n",
      "36  (LabelIssuesPerc > 27.500) and (sparsity.mean ...                4   \n",
      "\n",
      "     Support  Confidence  \n",
      "0   0.007246         1.0  \n",
      "1   0.007246         1.0  \n",
      "29  0.007246         1.0  \n",
      "30  0.007246         1.0  \n",
      "31  0.007246         1.0  \n",
      "32  0.021739         1.0  \n",
      "33  0.007246         1.0  \n",
      "34  0.007246         1.0  \n",
      "35  0.007246         1.0  \n",
      "36  0.014493         1.0  \n"
     ]
    }
   ],
   "source": [
    "#With clustering\n",
    "le = LabelEncoder()\n",
    "X = merged_df[common_features]\n",
    "X = X.apply(le.fit_transform)\n",
    "\n",
    "y = merged_df['FeatureAlgo'] # Encode both\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2, train_size=0.8)\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=20, random_state=0)\n",
    "clf.fit(X_train, y_train)  # y_train can be FS_Type or FS_Setting\n",
    "\n",
    "\n",
    "y_type_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Feature_Algo Report (common features):\")\n",
    "print(classification_report(y_test, y_type_pred))\n",
    "\n",
    "rule_df = get_rules_with_support_confidence(clf, feature_names=X_train.columns.tolist(), X=X_train, y=y_train)\n",
    "print(rule_df.sort_values(by='Confidence', ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_df.to_csv(\"rules.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature_Algo Report (common features):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Dependency       0.72      0.76      0.74        17\n",
      "    Distance       0.00      0.00      0.00         3\n",
      " Information       0.69      0.60      0.64        15\n",
      "\n",
      "    accuracy                           0.63        35\n",
      "   macro avg       0.47      0.45      0.46        35\n",
      "weighted avg       0.65      0.63      0.64        35\n",
      "\n",
      "                                                 Rule  Predicted_Class  \\\n",
      "0   (LabelIssuesPerc <= 27.500) and (cEntropy <= 5...                0   \n",
      "29  (LabelIssuesPerc > 27.500) and (sparsity.mean ...                1   \n",
      "22  (LabelIssuesPerc > 27.500) and (sparsity.mean ...                0   \n",
      "23  (LabelIssuesPerc > 27.500) and (sparsity.mean ...                2   \n",
      "24  (LabelIssuesPerc > 27.500) and (sparsity.mean ...                0   \n",
      "25  (LabelIssuesPerc > 27.500) and (sparsity.mean ...                2   \n",
      "26  (LabelIssuesPerc > 27.500) and (sparsity.mean ...                0   \n",
      "27  (LabelIssuesPerc > 27.500) and (sparsity.mean ...                2   \n",
      "28  (LabelIssuesPerc > 27.500) and (sparsity.mean ...                0   \n",
      "30  (LabelIssuesPerc > 27.500) and (sparsity.mean ...                2   \n",
      "\n",
      "     Support  Confidence  \n",
      "0   0.007246         1.0  \n",
      "29  0.007246         1.0  \n",
      "22  0.007246         1.0  \n",
      "23  0.007246         1.0  \n",
      "24  0.007246         1.0  \n",
      "25  0.297101         1.0  \n",
      "26  0.007246         1.0  \n",
      "27  0.036232         1.0  \n",
      "28  0.014493         1.0  \n",
      "30  0.036232         1.0  \n"
     ]
    }
   ],
   "source": [
    "#With clustering\n",
    "le = LabelEncoder()\n",
    "X = merged_df[common_features]\n",
    "X = X.apply(le.fit_transform)\n",
    "\n",
    "y = merged_df['FS_Type'] # Encode both\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2, train_size=0.8)\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=20, random_state=0)\n",
    "clf.fit(X_train, y_train)  # y_train can be FS_Type or FS_Setting\n",
    "\n",
    "\n",
    "y_type_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Feature_Algo Report (common features):\")\n",
    "print(classification_report(y_test, y_type_pred))\n",
    "\n",
    "rule_df = get_rules_with_support_confidence(clf, feature_names=X_train.columns.tolist(), X=X_train, y=y_train)\n",
    "print(rule_df.sort_values(by='Confidence', ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_df.to_csv(\"rules_fstype.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".deeplearningClass",
   "language": "python",
   "name": ".deeplearningclass"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36cf16204b8548560b1c020c4e8fb5b57f0e4c58016f52f2d4be01e192833930"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
