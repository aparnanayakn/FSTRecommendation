{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jenkspy\n",
    "import numpy as np\n",
    "from owlready2 import *\n",
    "import ast\n",
    "import rdflib\n",
    "import re\n",
    "\n",
    "\n",
    "owlready2.JAVA_EXE =\"/usr/lib/jvm/java-17-openjdk-amd64/bin/java\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"test_dataset.csv\")\n",
    "df.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_round(value):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except (ValueError, TypeError):\n",
    "        return value  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    df[col] = df[col].apply(convert_and_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onto = get_ontology(\"file:///home/d19125691/Documents/Experiments/ontologyDCQ/onto-DCQ-FS_git/onto/MtL_Enriched.owl\").load()\n",
    "dcat = get_namespace(\"http://www.w3.org/ns/dcat/\")\n",
    "dqv = get_namespace(\"http://www.w3.org/ns/dqv/\")\n",
    "mtl = get_ontology(\"https://purl.archive.org/domain/mtl#\")\n",
    "#onto.base_iri = \"https://purl.archive.org/domain/mtl#\"\n",
    "rdf=get_namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\n",
    "dmop = get_ontology(\"http://www.e-lico.eu/ontologies/dmop/DMOP/DMOP.owl\")\n",
    "dmop.base_iri = \"http://www.e-lico.eu/ontologies/dmo/DMOP/DMOP.owl#\"\n",
    "dmop1 = get_ontology(\"http://www.e-LICO.eu/ontologies/dmo/DMOP/DMOP.owl\")\n",
    "dmop1.base_iri = \"http://www.e-LICO.eu/ontologies/dmo/DMOP/DMOP.owl#\"\n",
    "\n",
    "#sync_reasoner_pellet([onto])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'[^a-zA-Z0-9]') #removing non-alpha numeric characters from file name\n",
    "\n",
    "fileObjects = {}\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with onto:\n",
    "    for i in range(len(df)):\n",
    "  #      i=34\n",
    "        fileName = df['File'][i]\n",
    "        fileName = pattern.sub('', fileName)\n",
    "\n",
    "        mtla = mtl.LearningFromTaskProperties(fileName, namespace=onto)\n",
    "        mtlt = mtl.FeatureSelectionRecommendationTask(fileName, namespace=onto)\n",
    "\n",
    "        fileObject = dmop.LabeledDataSet(fileName, namespace=onto)\n",
    "        fileObjects[fileName] = mtlt\n",
    "        fsr_output = mtl.FeatureSelectionRecommendation(fileName, namespace=onto)\n",
    "        mtla.addresses.append(mtlt)\n",
    "\n",
    "        mtlt.specifiesInputClass = [fileObject]\n",
    "        mtlt.specifiesOutputClass = [fsr_output]\n",
    "\n",
    "        nInstance = dmop1.NumberOfInstances(fileName, namespace=onto)\n",
    "        fileObject.hasQuality.append(nInstance)\n",
    "        q3 = mtl.Mean(fileName+\"mean1\")\n",
    "        nInstance.qlocation.append(q3) \n",
    "        q3.hasDatavalue = [int(df['nr_inst'][i])]\n",
    "\n",
    "        rRange = mtl.Range(fileName)\n",
    "        fileObject.hasQuality.append(rRange)\n",
    "        q2 = mtl.Mean(fileName+\"mean2\")\n",
    "        rRange.qlocation.append(q2)\n",
    "        q2.hasDatavalue  = [float(df['range.mean'][i])]\n",
    "\n",
    "        attrConc = mtl.AttributeConcentration(fileName)\n",
    "        fileObject.hasQuality.append(attrConc)\n",
    "        q4 = mtl.Mean(fileName+\"mean3\")\n",
    "        attrConc.qlocation.append(q4)\n",
    "        q4.hasDatavalue  = [float(df['attr_conc.mean'][i])]\n",
    "\n",
    "        covR = mtl.Covariance(fileName)\n",
    "        fileObject.hasQuality.append(covR)\n",
    "        q5 = mtl.Mean(fileName+\"mean4\")\n",
    "        covR.qlocation.append(q5)\n",
    "        q5.hasDatavalue = [float(df['cov.mean'][i])]\n",
    "\n",
    "        cEntr = mtl.ClassEntropy(fileName)\n",
    "        fileObject.hasQuality.append(cEntr)\n",
    "        q6 = mtl.Mean(fileName+\"mean5\")\n",
    "        cEntr.qlocation.append(q6)\n",
    "        q6.hasDatavalue  = [ float(df['cEntropy'][i])]\n",
    "\n",
    "        outDet = mtl.OutlierDetection(fileName)\n",
    "        fileObject.hasQuality.append(outDet)\n",
    "        q7 = mtl.Mean(fileName+\"mean6\")\n",
    "        outDet.qlocation.append(q7)\n",
    "        q7.hasDatavalue = [float(df['OutlierPerc'][i])]\n",
    "\n",
    "        nrOut = mtl.NumberOfOutliers(fileName)\n",
    "        fileObject.hasQuality.append(nrOut)\n",
    "        q8 = mtl.Mean(fileName+\"mean7\")\n",
    "        nrOut.qlocation.append(q8)\n",
    "        q8.hasDatavalue = [float(df['nr_outliers'][i])]\n",
    "\n",
    "        instFeat = dmop1.ProportionOfInstancesPerFeature(fileName, namespace=onto)\n",
    "        fileObject.hasQuality.append(instFeat)\n",
    "        q12 = mtl.Mean(fileName+\"mean8\")\n",
    "        instFeat.qlocation.append(q12)\n",
    "        q12.hasDatavalue  = [float(df['inst_to_attr'][i])]\n",
    "\n",
    "        attrEntr = mtl.AttributesEntropy(fileName)\n",
    "        fileObject.hasQuality.append(attrEntr)\n",
    "        q9 = mtl.Mean(fileName+\"mean9\")\n",
    "        attrEntr.qlocation.append(q9)\n",
    "        q9.hasDatavalue  = [float(df['attr_ent.mean'][i])]\n",
    "\n",
    "        corr = mtl.Correlation(\"corr\", namespace=onto)\n",
    "        fileObject.hasQuality.append(corr)\n",
    "        q10 = mtl.Mean(fileName+\"mean10\")\n",
    "        corr.qlocation.append(q10)\n",
    "        q10.hasDatavalue  = [float(df['cor.mean'][i])]\n",
    "\n",
    "        medAttr = mtl.MedianOfAttributes(fileName)\n",
    "        fileObject.hasQuality.append(medAttr)\n",
    "        q11 = mtl.Mean(fileName+\"mean11\")\n",
    "        medAttr.qlocation.append(q11)\n",
    "        q11.hasDatavalue  = [float(df['median.mean'][i])]\n",
    "\n",
    "        classImb = mtl.ClassImbalance(fileName)\n",
    "        fileObject.hasQuality.append(classImb)\n",
    "        q13 = mtl.Mean(fileName+\"mean12\")\n",
    "        classImb.qlocation.append(q13)\n",
    "        q13.hasDatavalue  = [float(df['ClassImbRatio'][i])]\n",
    "\n",
    "        ena = mtl.EqNumberOfAttributes(fileName)\n",
    "        fileObject.hasQuality.append(ena)\n",
    "        q14 = mtl.Mean(\"mean13\")\n",
    "        ena.qlocation.append(q14)\n",
    "        q14.hasDatavalue  = [float(df['ena'][i])]\n",
    "\n",
    "        labelNoise = mtl.LabelNoise(fileName)\n",
    "        fileObject.hasQuality.append(labelNoise)\n",
    "        q15 = mtl.Mean(fileName+\"mean14\")\n",
    "        labelNoise.qlocation.append(q15)\n",
    "        q15.hasDatavalue  = [float(df['LabelIssuesPerc'][i])]\n",
    "    # AllDifferent([q2,q3,q4,q5,q6,q7,q8,q9,q10,q11,q12,q13,q14,q15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:    \n",
    "    sync_reasoner_pellet([onto], infer_property_values=True, infer_data_property_values=True)\n",
    "    print(\"✅ Ontology is consistent.\")\n",
    "except OwlReadyInconsistentOntologyError:\n",
    "    print(\"❌ Ontology inconsistent! Checking problem individuals...\")\n",
    "    for ind in onto.individuals():\n",
    "        if ind.is_instance_of:\n",
    "            print(f\"Individual: {ind}, Classes: {ind.is_instance_of}\")\n",
    "except Exception as e:\n",
    "    print(\"Reasoner failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check classification again just to be safe\n",
    "for fileName, fileObject in fileObjects.items():\n",
    "    if onto.MIRelevantData in fileObject.is_a:\n",
    "        print(f\"{fileObject.name} satisfies MIRelevantData constraint\")\n",
    "        output = onto.MITask.specifiesOutputClass\n",
    "        print(\"Recommended feature selection technique is\", output[0].recommends)\n",
    "    if onto.FCBFRelevantData in fileObject.is_a:\n",
    "        print(f\"{fileObject.name} satisfies FCBFRelevantData constraint\")\n",
    "        output = onto.FCBFTask.specifiesOutputClass\n",
    "        print(\"Recommended feature selection technique is\", output[0].recommends)\n",
    "    if onto.GRRelevantData in fileObject.is_a:\n",
    "        print(f\"{fileObject.name} satisfies GRRelevantData constraint\")\n",
    "        output = onto.GRTask.specifiesOutputClass\n",
    "        print(\"Recommended feature selection technique is\", output[0].recommends)\n",
    "    if onto.ReliefRelevantData in fileObject.is_a:\n",
    "        print(f\"{fileObject.name} satisfies ReliefRelevantData constraint\")\n",
    "        output = onto.ReliefTask.specifiesOutputClass\n",
    "        print(\"Recommended feature selection technique is\", output[0].recommends)\n",
    "    if onto.ChisquareRelevantData in fileObject.is_a:\n",
    "        print(f\"{fileObject.name} satisfies ChisquareRelevantData constraint\")\n",
    "        output = onto.ChisquareTask.specifiesOutputClass\n",
    "        print(\"Recommended feature selection technique is\", output[0].recommends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming onto is your ontology and fileObject is your instance\n",
    "inferred_classes = mtla.is_a\n",
    "print(mtla.is_a)\n",
    "\n",
    "# Define a list of your final, mutually exclusive recommendation classes\n",
    "recommendation_classes = [mtl.MIRelevantData, mtl.ChisquareRelevantData, mtl.GRRelevantData, mtl.FCBFRelevantData, \n",
    "mtl.ReliefRelevantData]\n",
    "\n",
    "for cls in inferred_classes:\n",
    "    if cls in recommendation_classes:\n",
    "        # You found the single, most specific recommendation class\n",
    "        print(f\"Recommended feature selection technique is {cls.recommends}\") \n",
    "        break  # Exit the loop after finding the first one\n",
    "else:\n",
    "    # This else block runs if the loop completes without a break\n",
    "    print(\"No specific feature selection technique was recommended.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload triples to Fuseki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "fuseki_url = \"http://localhost:3030/KB/data\" #URL of the Fuseki dataset\n",
    "\n",
    "rdf_file = \"Populated.rdf\"\n",
    "\n",
    "with open(rdf_file, 'rb') as file:\n",
    "    rdf_data = file.read()\n",
    "\n",
    "response = requests.post(fuseki_url, data=rdf_data, headers={'Content-Type': 'application/rdf+xml'})\n",
    "\n",
    "if response.status_code == 200 or response.status_code == 204:\n",
    "    print(\"Data uploaded successfully\")\n",
    "else:\n",
    "    print(f\"Failed to upload data. Status code: {response.status_code}\")\n",
    "    print(f\"Response: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query to extract limits of correlation metric - simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "endpoint_url = \"http://localhost:3030/KB/sparql\" \n",
    "\n",
    "sparql = SPARQLWrapper(endpoint_url)\n",
    "query = \"\"\"\n",
    "    PREFIX dmop: <http://www.e-lico.eu/ontologies/dmo/DMOP/DMOP.owl#>\n",
    "    PREFIX dmop1: <http://www.e-LICO.eu/ontologies/dmo/DMOP/DMOP.owl#>\n",
    "    PREFIX mtl: <https://purl.archive.org/domain/mtl#>  \n",
    "    SELECT ?upperValue ?lowerValue ?mean ?stdDev\n",
    "    WHERE {  \n",
    "        ?metal a mtl:MetaLearningAlgorithm . \n",
    "        ?featureSelectionTask a dmop:FeatureSelectionTask .\n",
    "        ?metal mtl:hasMetaObjective ?featureSelectionTask .\n",
    "        ?cor a mtl:CorrelationParameter .\n",
    "        ?metal dmop:hasParameter ?cor .\n",
    "        ?cor mtl:hasLowerValue ?lowerValue .\n",
    "        ?cor mtl:hasUpperValue ?upperValue .\n",
    "        ?m a mtl:Mean .\n",
    "        ?std a mtl:StdDev .\n",
    "        ?cor mtl:hasQuality ?m .\n",
    "        ?cor mtl:hasQuality ?std .\n",
    "        ?m dmop:hasValue ?mean .\n",
    "  ?std dmop:hasValue ?stdDev . \n",
    "} \"\"\"\n",
    "\n",
    "sparql.setQuery(query)\n",
    "\n",
    "sparql.setReturnFormat(JSON)\n",
    "\n",
    "results = sparql.query().convert()\n",
    "\n",
    "if \"results\" in results and \"bindings\" in results[\"results\"]:\n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        print(result)\n",
    "        upperValue = result[\"upperValue\"][\"value\"]\n",
    "        lowerValue = result[\"lowerValue\"][\"value\"]\n",
    "        print(f\"Feature Selection Algorithm: {upperValue}, Feature Count: {lowerValue}\")\n",
    "else:\n",
    "    print(\"Unexpected response format. Please check the SPARQL query and endpoint.\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query to extract datasets and associated feature selection techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "endpoint_url = \"http://localhost:3030/KB/sparql\" \n",
    "\n",
    "sparql = SPARQLWrapper(endpoint_url)\n",
    "query = \"\"\"\n",
    "    PREFIX dmop: <http://www.e-lico.eu/ontologies/dmo/DMOP/DMOP.owl#>\n",
    "    PREFIX dmop1: <http://www.e-LICO.eu/ontologies/dmo/DMOP/DMOP.owl#>\n",
    "    PREFIX mtl: <https://purl.archive.org/domain/mtl#>  \n",
    "    SELECT ?featureAlgo ?dataset\n",
    "    WHERE {  \n",
    "        ?metal a mtl:MetaLearningAlgorithm . \n",
    "        ?featureSelectionTask a dmop:FeatureSelectionTask .\n",
    "        ?metal mtl:hasMetaObjective ?featureSelectionTask .\n",
    "  \t\t?dataset a dmop:DataSetClass .   \n",
    "\t\t?featureSelectionTask dmop:specifiesInputClass ?dataset .\n",
    "  \t\t?featureAlgoOutput a dmop:StructuredPredictionModelClass .\n",
    "  \t\t?featureSelectionTask dmop:specifiesOutputClass ?featureAlgoOutput .\n",
    "  \t\t?featureAlgoOutput dmop:hasValue ?featureAlgo .\n",
    "} \"\"\"\n",
    "\n",
    "sparql.setQuery(query)\n",
    "\n",
    "sparql.setReturnFormat(JSON)\n",
    "\n",
    "results = sparql.query().convert()\n",
    "\n",
    "if \"results\" in results and \"bindings\" in results[\"results\"]:\n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        featureAlgo = result[\"featureAlgo\"][\"value\"]\n",
    "        dataset = result[\"dataset\"][\"value\"]\n",
    "        print(f\"Feature Selection Algorithm: {featureAlgo}, Dataset: {dataset}\")\n",
    "else:\n",
    "    print(\"Unexpected response format. Please check the SPARQL query and endpoint.\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".deeplearningClass",
   "language": "python",
   "name": ".deeplearningclass"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
