{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"KnowledgeBase.csv\")\n",
    "df.fillna(0,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char_raw = df[[ 'cor.mean', 'cov.mean', 'eigenvalues.mean', 'g_mean.mean', 'h_mean.mean', 'iq_range.mean', \n",
    " 'kurtosis.mean', 'mad.mean', 'max.mean', 'mean.mean', 'median.mean', 'min.mean', 'nr_cor_attr', 'nr_norm', 'nr_outliers', 'range.mean', 'sd.mean', 'skewness.mean', 'sparsity.mean',\n",
    " 't_mean.mean', 'var.mean', 'attr_to_inst', 'inst_to_attr', 'nr_attr', 'nr_bin', 'nr_inst', 'nr_num','attr_conc.mean', \n",
    " 'attr_ent.mean', 'nUnique', 'ena', 'snr.mean', 'cEntropy', 'FeatureAlgo']]\n",
    "\n",
    "df_quality_raw = df[['Completeness', 'Conciseness', 'ClassImbRatio', 'ClassOverlapPerc', 'OutlierPerc', 'LabelIssuesPerc', 'FeatureAlgo']]\n",
    "\n",
    "df_raw = df[['Completeness', 'Conciseness', 'cor.mean', 'cov.mean', 'eigenvalues.mean', 'g_mean.mean', 'h_mean.mean', 'iq_range.mean', \n",
    " 'kurtosis.mean', 'mad.mean', 'max.mean', 'mean.mean', 'median.mean', 'min.mean', 'nr_cor_attr', 'nr_norm', 'nr_outliers', 'range.mean', 'sd.mean', 'skewness.mean', 'sparsity.mean',\n",
    " 't_mean.mean', 'var.mean', 'ClassImbRatio', 'ClassOverlapPerc', 'OutlierPerc', 'attr_to_inst', 'inst_to_attr', 'nr_attr', 'nr_bin', 'nr_inst', 'nr_num','attr_conc.mean', \n",
    " 'attr_ent.mean', 'LabelIssuesPerc','nUnique', 'ena', 'snr.mean', 'cEntropy', 'FeatureAlgo']]\n",
    "\n",
    "\n",
    "df_char_bin = df[['cor.mean_bins', 'cov.mean_bins',\n",
    "       'eigenvalues.mean_bins', 'g_mean.mean_bins', 'h_mean.mean_bins',\n",
    "       'iq_range.mean_bins', 'kurtosis.mean_bins', 'mad.mean_bins',\n",
    "       'max.mean_bins', 'mean.mean_bins', 'median.mean_bins', 'min.mean_bins',\n",
    "       'nr_cor_attr_bins', 'nr_norm_bins', 'nr_outliers_bins',\n",
    "       'range.mean_bins', 'sd.mean_bins', 'skewness.mean_bins',\n",
    "       'sparsity.mean_bins', 't_mean.mean_bins', 'var.mean_bins',\n",
    "       'attr_to_inst_bins', 'inst_to_attr_bins',\n",
    "       'nr_attr_bins', 'nr_inst_bins', 'nr_num_bins', 'nr_bin_bins',\n",
    "       'attr_conc.mean_bins', 'attr_ent.mean_bins', 'ena_bins',\n",
    "       'snr.mean_bins', 'cEntropy_bins','FeatureAlgo']]\n",
    "\n",
    "df_quality_bin = df[['Completeness_bins',\n",
    "       'Conciseness_bins', 'LabelIssues_bins', 'ClassImbRatio_bins', 'OutlierPerc_bins',\n",
    "       'ClassOverlapPerc_bins','FeatureAlgo']]\n",
    "\n",
    "df_bin = df[['Completeness_bins',\n",
    "       'Conciseness_bins', 'cor.mean_bins', 'cov.mean_bins',\n",
    "       'eigenvalues.mean_bins', 'g_mean.mean_bins', 'h_mean.mean_bins',\n",
    "       'iq_range.mean_bins', 'kurtosis.mean_bins', 'mad.mean_bins',\n",
    "       'max.mean_bins', 'mean.mean_bins', 'median.mean_bins', 'min.mean_bins',\n",
    "       'nr_cor_attr_bins', 'nr_norm_bins', 'nr_outliers_bins',\n",
    "       'range.mean_bins', 'sd.mean_bins', 'skewness.mean_bins',\n",
    "       'sparsity.mean_bins', 't_mean.mean_bins', 'var.mean_bins',\n",
    "       'LabelIssues_bins', 'ClassImbRatio_bins', 'OutlierPerc_bins',\n",
    "       'ClassOverlapPerc_bins', 'attr_to_inst_bins', 'inst_to_attr_bins',\n",
    "       'nr_attr_bins', 'nr_inst_bins', 'nr_num_bins', 'nr_bin_bins',\n",
    "       'attr_conc.mean_bins', 'attr_ent.mean_bins', 'ena_bins',\n",
    "       'snr.mean_bins', 'cEntropy_bins', 'FeatureAlgo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chisquare    102\n",
       "GR            35\n",
       "relief        20\n",
       "fcbf          12\n",
       "MI             4\n",
       "Name: FeatureAlgo, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['FeatureAlgo'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Cross-Validation Accuracy Cross-Validation Error Rate  \\\n",
      "df_raw                          0.752454                    0.247546   \n",
      "df_char_raw                     0.752454                    0.247546   \n",
      "df_quality_raw                  0.698796                    0.301204   \n",
      "\n",
      "               Test Accuracy Test Error Rate Precision    Recall  F1 Score  \\\n",
      "df_raw              0.833333        0.166667  0.819334  0.833333  0.810676   \n",
      "df_char_raw         0.833333        0.166667  0.819334  0.833333  0.810676   \n",
      "df_quality_raw      0.735294        0.264706  0.776197  0.735294  0.712222   \n",
      "\n",
      "                                                 Confusion Matrix  \n",
      "df_raw          [[12, 0, 1, 1, 0], [0, 20, 0, 0, 0], [4, 2, 3,...  \n",
      "df_char_raw     [[12, 0, 1, 1, 0], [0, 20, 0, 0, 0], [4, 2, 3,...  \n",
      "df_quality_raw  [[10, 1, 1, 2, 0], [0, 20, 0, 0, 0], [6, 2, 3,...  \n"
     ]
    }
   ],
   "source": [
    "def evaluate_knn(df, random_state=33):\n",
    "    df = df.copy()\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y) \n",
    "\n",
    "    ros = RandomOverSampler(random_state=33)\n",
    "\n",
    "    smote = SMOTE(random_state=33)\n",
    "\n",
    "    pipeline = Pipeline([('ros', ros), ('smote', smote)])\n",
    "\n",
    "    X_resampled, y_resampled = pipeline.fit_resample(X, y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=33)\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "        \n",
    "    knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "    \n",
    "    cross_val_scores = cross_val_score(knn_classifier, X_train, y_train, cv=kf, scoring='accuracy')\n",
    "    error_rates = 1 - cross_val_scores\n",
    "    \n",
    "    knn_classifier.fit(X_train, y_train)\n",
    "    y_pred = knn_classifier.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'Cross-Validation Accuracy': cross_val_scores.mean(),\n",
    "        'Cross-Validation Error Rate': error_rates.mean(),\n",
    "        'Test Accuracy': accuracy,\n",
    "        'Test Error Rate': 1 - accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "datasets = {\n",
    "    'df_raw': df_raw,\n",
    "    'df_char_raw': df_char_raw,\n",
    "    'df_quality_raw': df_quality_raw\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    results[name] = evaluate_knn(df)\n",
    "\n",
    "comparison_df = pd.DataFrame(results).transpose()\n",
    "print(comparison_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Cross-Validation Accuracy Cross-Validation Error Rate  \\\n",
      "df_raw                          0.914303                   0.0856971   \n",
      "df_char_raw                     0.906986                   0.0930142   \n",
      "df_quality_raw                  0.857964                    0.142036   \n",
      "\n",
      "               Test Accuracy Test Error Rate Precision    Recall  F1 Score  \\\n",
      "df_raw              0.960784       0.0392157  0.966301  0.960784  0.959023   \n",
      "df_char_raw         0.960784       0.0392157  0.966301  0.960784  0.959023   \n",
      "df_quality_raw      0.892157        0.107843  0.900767  0.892157  0.883325   \n",
      "\n",
      "                                                 Confusion Matrix  \n",
      "df_raw          [[14, 0, 0, 0, 0], [0, 20, 0, 0, 0], [3, 0, 10...  \n",
      "df_char_raw     [[14, 0, 0, 0, 0], [0, 20, 0, 0, 0], [3, 0, 10...  \n",
      "df_quality_raw  [[13, 1, 0, 0, 0], [0, 20, 0, 0, 0], [3, 1, 7,...  \n"
     ]
    }
   ],
   "source": [
    "#decision\n",
    "\n",
    "def evaluate_decision_tree(df, random_state=33):\n",
    "    df = df.copy()\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)  \n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    ros = RandomOverSampler(random_state=33)\n",
    "\n",
    "    smote = SMOTE(random_state=33)\n",
    "\n",
    "    pipeline = Pipeline([('ros', ros), ('smote', smote)])\n",
    "\n",
    "    X_resampled, y_resampled = pipeline.fit_resample(X, y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=33)    \n",
    "    tree_classifier = DecisionTreeClassifier(random_state=random_state)\n",
    "    \n",
    "    cross_val_scores = cross_val_score(tree_classifier, X_train, y_train, cv=kf, scoring='accuracy')\n",
    "    error_rates = 1 - cross_val_scores\n",
    "    \n",
    "    tree_classifier.fit(X_train, y_train)\n",
    "    y_pred = tree_classifier.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'Cross-Validation Accuracy': cross_val_scores.mean(),\n",
    "        'Cross-Validation Error Rate': error_rates.mean(),\n",
    "        'Test Accuracy': accuracy,\n",
    "        'Test Error Rate': 1 - accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "datasets = {\n",
    "    'df_raw': df_raw,\n",
    "    'df_char_raw': df_char_raw,\n",
    "    'df_quality_raw': df_quality_raw\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    results[name] = evaluate_decision_tree(df)\n",
    "\n",
    "comparison_df = pd.DataFrame(results).transpose()\n",
    "\n",
    "print(comparison_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Cross-Validation Accuracy Cross-Validation Error Rate  \\\n",
      "df_raw                          0.905882                   0.0941176   \n",
      "df_char_raw                     0.905882                   0.0941176   \n",
      "df_quality_raw                  0.815686                    0.184314   \n",
      "\n",
      "               Test Accuracy Test Error Rate Precision    Recall  F1 Score  \\\n",
      "df_raw              0.931373       0.0686275   0.93961  0.931373  0.922151   \n",
      "df_char_raw         0.931373       0.0686275   0.93961  0.931373  0.922151   \n",
      "df_quality_raw      0.872549        0.127451  0.876166  0.872549  0.861582   \n",
      "\n",
      "                                                 Confusion Matrix  \n",
      "df_raw          [[14, 0, 0, 0, 0], [0, 20, 0, 0, 0], [3, 2, 7,...  \n",
      "df_char_raw     [[14, 0, 0, 0, 0], [0, 20, 0, 0, 0], [3, 2, 7,...  \n",
      "df_quality_raw  [[12, 1, 1, 0, 0], [0, 20, 0, 0, 0], [4, 2, 6,...  \n"
     ]
    }
   ],
   "source": [
    "datasets = {\n",
    "    'df_raw': df_raw,\n",
    "    'df_char_raw': df_char_raw,\n",
    "    'df_quality_raw': df_quality_raw\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "metric_name = 'Euclidean'\n",
    "metric_func = lambda X_train, X_test: np.sqrt(((X_train.values[:, np.newaxis] - X_test.values) ** 2).sum(axis=2))\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=33)\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    cv_accuracies = []\n",
    "\n",
    "    ros = RandomOverSampler(random_state=33)\n",
    "    smote = SMOTE(random_state=33)\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('ros', ros),\n",
    "        ('smote', smote)\n",
    "    ])\n",
    "\n",
    "    X_resampled, y_resampled = pipeline.fit_resample(X, y_encoded)\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_resampled):\n",
    "        X_train, X_test = X_resampled.iloc[train_index], X_resampled.iloc[test_index]\n",
    "        y_train, y_test = y_resampled[train_index], y_resampled[test_index]\n",
    "        \n",
    "        distances = metric_func(X_train, X_test)\n",
    "        \n",
    "        nearest_indices = np.argmin(distances, axis=0)\n",
    "        \n",
    "        predicted_labels = y_train[nearest_indices]\n",
    "        \n",
    "        cv_accuracy = accuracy_score(y_test, predicted_labels)\n",
    "        \n",
    "        cv_accuracies.append(cv_accuracy)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=33)\n",
    "    \n",
    "    distances = metric_func(X_train, X_test)\n",
    "    \n",
    "    nearest_indices = np.argmin(distances, axis=0)\n",
    "    \n",
    "    predicted_labels = y_train[nearest_indices]\n",
    "    \n",
    "    test_accuracy = accuracy_score(y_test, predicted_labels)\n",
    "    precision = precision_score(y_test, predicted_labels, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, predicted_labels, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, predicted_labels, average='weighted', zero_division=0)\n",
    "    confusion_matrix_result = confusion_matrix(y_test, predicted_labels)\n",
    "\n",
    "    results[name] = {\n",
    "        'Cross-Validation Accuracy': np.mean(cv_accuracies),\n",
    "        'Cross-Validation Error Rate': np.mean([1 - acc for acc in cv_accuracies]),\n",
    "        'Test Accuracy': test_accuracy,\n",
    "        'Test Error Rate': 1 - test_accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': confusion_matrix_result\n",
    "    }\n",
    "\n",
    "results_df = pd.DataFrame(results).transpose()\n",
    "\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Cross-Validation Accuracy Cross-Validation Error Rate  \\\n",
      "df_raw                          0.948492                   0.0515076   \n",
      "df_char_raw                     0.948492                   0.0515076   \n",
      "df_quality_raw                  0.586965                    0.413035   \n",
      "\n",
      "               Test Accuracy Test Error Rate Precision    Recall  F1 Score  \\\n",
      "df_raw              0.454545        0.545455  0.411315  0.454545  0.431564   \n",
      "df_char_raw         0.454545        0.545455  0.411315  0.454545  0.431564   \n",
      "df_quality_raw      0.362637        0.637363   0.37798  0.362637  0.363931   \n",
      "\n",
      "                                                 Confusion Matrix  \n",
      "df_raw          [[6, 1, 0, 2, 3], [2, 11, 0, 0, 4], [1, 3, 0, ...  \n",
      "df_char_raw     [[6, 1, 0, 2, 3], [2, 11, 0, 0, 4], [1, 3, 0, ...  \n",
      "df_quality_raw  [[4, 2, 1, 1, 5], [4, 7, 0, 3, 5], [0, 3, 3, 3...  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def unify_features(test_sample, train_data, train_labels):\n",
    "    test_features = test_sample.values\n",
    "    matching_indices = np.all(train_data == test_features, axis=1)\n",
    "    if matching_indices.any():\n",
    "        matching_labels = train_labels[matching_indices]\n",
    "        return np.unique(matching_labels)\n",
    "    return [\"unknown\"]\n",
    "\n",
    "def evaluate_unification(df, random_state=33):\n",
    "    df = df.copy()\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    cv_accuracies = []\n",
    "    test_accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    confusion_matrices = []\n",
    "    \n",
    "    ros = RandomOverSampler(random_state=33)\n",
    "\n",
    "    smote = SMOTE(random_state=33)\n",
    "\n",
    "    pipeline = Pipeline([('ros', ros), ('smote', smote)])\n",
    "\n",
    "    X_resampled, y_resampled = pipeline.fit_resample(X, y_encoded)\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_resampled):\n",
    "        X_train, X_test = X_resampled.iloc[train_index], X_resampled.iloc[test_index]\n",
    "        y_train, y_test = y_resampled[train_index], y_resampled[test_index]\n",
    "        \n",
    "        predicted_labels = []\n",
    "        for i in range(X_test.shape[0]):\n",
    "            label = unify_features(X_test.iloc[i], X_train, y_train)\n",
    "            predicted_labels.append(label)\n",
    "        \n",
    "        predicted_labels = [item for sublist in predicted_labels for item in sublist]\n",
    "        \n",
    "        valid_indices = [i for i, label in enumerate(predicted_labels) if label != \"unknown\" and i < len(y_test)]\n",
    "        if not valid_indices:\n",
    "            continue\n",
    "        \n",
    "        valid_predictions = np.array([predicted_labels[i] for i in valid_indices])\n",
    "\n",
    "        valid_truth = np.array([y_test[i] for i in valid_indices])\n",
    "        \n",
    "        cv_accuracy = accuracy_score(valid_truth, valid_predictions)\n",
    "       \n",
    "        cv_accuracies.append(cv_accuracy)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=33)\n",
    "    predicted_labels = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        label = unify_features(X_test.iloc[i], X_train, y_train)\n",
    "        predicted_labels.append(label)\n",
    "    predicted_labels = [item for sublist in predicted_labels for item in sublist]\n",
    "        \n",
    "    valid_indices = [i for i, label in enumerate(predicted_labels) if label != \"unknown\" and i < len(y_test)]\n",
    "        \n",
    "    valid_predictions = np.array([predicted_labels[i] for i in valid_indices])\n",
    "    valid_truth = np.array([y_test[i] for i in valid_indices])\n",
    "        \n",
    "    precision = precision_score(valid_truth, valid_predictions, average='weighted', zero_division=0)\n",
    "    recall = recall_score(valid_truth, valid_predictions, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(valid_truth, valid_predictions, average='weighted', zero_division=0)\n",
    "    confusion_matrix_result = confusion_matrix(valid_truth, valid_predictions)    \n",
    "    test_accuracy = accuracy_score(valid_truth, valid_predictions)\n",
    "\n",
    "    return {\n",
    "        'Cross-Validation Accuracy': np.mean(cv_accuracies),\n",
    "        'Cross-Validation Error Rate': np.mean([1 - acc for acc in cv_accuracies]),\n",
    "        'Test Accuracy': test_accuracy,\n",
    "        'Test Error Rate': 1 - test_accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': confusion_matrix_result\n",
    "    }\n",
    "\n",
    "\n",
    "datasets = {\n",
    "    'df_raw': df_raw,\n",
    "    'df_char_raw': df_char_raw,\n",
    "    'df_quality_raw': df_quality_raw\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    results[name] = evaluate_unification(df)\n",
    "\n",
    "results_df = pd.DataFrame(results).transpose()\n",
    "\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIN data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Cross-Validation Accuracy Cross-Validation Error Rate  \\\n",
      "df_bin                          0.661759                    0.338241   \n",
      "df_char_bin                     0.487654                    0.512346   \n",
      "df_quality_bin                   0.42171                     0.57829   \n",
      "\n",
      "               Test Accuracy Test Error Rate Precision    Recall  F1 Score  \\\n",
      "df_bin              0.764706        0.235294  0.792974  0.764706  0.753066   \n",
      "df_char_bin         0.568627        0.431373  0.725897  0.568627   0.57731   \n",
      "df_quality_bin       0.45098         0.54902  0.503912   0.45098  0.459663   \n",
      "\n",
      "                                                 Confusion Matrix  \n",
      "df_bin          [[11, 1, 2, 0, 0], [0, 20, 0, 0, 0], [4, 2, 6,...  \n",
      "df_char_bin     [[8, 4, 2, 0, 0], [0, 20, 0, 0, 0], [2, 4, 7, ...  \n",
      "df_quality_bin  [[6, 1, 5, 2, 0], [0, 12, 6, 0, 2], [1, 1, 5, ...  \n"
     ]
    }
   ],
   "source": [
    "def evaluate_knn(df, random_state=33):\n",
    "    df = df.copy()\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    \n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    for col in X:\n",
    "        X[col] = label_encoder.fit_transform(X[col])\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)  # Ensure y_encoded is 1D\n",
    "\n",
    "    ros = RandomOverSampler(random_state=33)\n",
    "    smote = SMOTE(random_state=33)\n",
    "\n",
    "    pipeline = Pipeline([('ros', ros), ('smote', smote)])\n",
    "\n",
    "    X_resampled, y_resampled = pipeline.fit_resample(X, y_encoded)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=33)\n",
    "\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "        \n",
    "    knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "    \n",
    "    cross_val_scores = cross_val_score(knn_classifier, X_train, y_train, cv=kf, scoring='accuracy')\n",
    "    error_rates = 1 - cross_val_scores\n",
    "    \n",
    "    knn_classifier.fit(X_train, y_train)\n",
    "    y_pred = knn_classifier.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'Cross-Validation Accuracy': cross_val_scores.mean(),\n",
    "        'Cross-Validation Error Rate': error_rates.mean(),\n",
    "        'Test Accuracy': accuracy,\n",
    "        'Test Error Rate': 1 - accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "datasets = {\n",
    "    'df_bin': df_bin,\n",
    "    'df_char_bin': df_char_bin,\n",
    "    'df_quality_bin': df_quality_bin\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    results[name] = evaluate_knn(df)\n",
    "\n",
    "comparison_df = pd.DataFrame(results).transpose()\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Cross-Validation Accuracy Cross-Validation Error Rate  \\\n",
      "df_bin                          0.806444                    0.193556   \n",
      "df_char_bin                     0.580939                    0.419061   \n",
      "df_quality_bin                  0.507438                    0.492562   \n",
      "\n",
      "               Test Accuracy Test Error Rate Precision    Recall  F1 Score  \\\n",
      "df_bin              0.813725        0.186275  0.848702  0.813725  0.807594   \n",
      "df_char_bin         0.637255        0.362745  0.698977  0.637255  0.617408   \n",
      "df_quality_bin      0.558824        0.441176  0.611434  0.558824  0.494522   \n",
      "\n",
      "                                                 Confusion Matrix  \n",
      "df_bin          [[12, 1, 1, 0, 0], [0, 20, 0, 0, 0], [3, 1, 9,...  \n",
      "df_char_bin     [[8, 4, 0, 2, 0], [0, 20, 0, 0, 0], [4, 3, 3, ...  \n",
      "df_quality_bin  [[11, 1, 0, 2, 0], [0, 20, 0, 0, 0], [3, 4, 2,...  \n"
     ]
    }
   ],
   "source": [
    "def evaluate_decision_tree(df, random_state=33):\n",
    "    df = df.copy()\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    \n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    for col in X:\n",
    "        X[col] = label_encoder.fit_transform(X[col])\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)  # Ensure y_encoded is 1D\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    ros = RandomOverSampler(random_state=33)\n",
    "\n",
    "    # SMOTE\n",
    "    smote = SMOTE(random_state=33)\n",
    "\n",
    "    pipeline = Pipeline([('ros', ros), ('smote', smote)])\n",
    "\n",
    "    X_resampled, y_resampled = pipeline.fit_resample(X, y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=33)    \n",
    "    tree_classifier = DecisionTreeClassifier(random_state=random_state)\n",
    "    \n",
    "    cross_val_scores = cross_val_score(tree_classifier, X_train, y_train, cv=kf, scoring='accuracy')\n",
    "    error_rates = 1 - cross_val_scores\n",
    "    \n",
    "    tree_classifier.fit(X_train, y_train)\n",
    "    y_pred = tree_classifier.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'Cross-Validation Accuracy': cross_val_scores.mean(),\n",
    "        'Cross-Validation Error Rate': error_rates.mean(),\n",
    "        'Test Accuracy': accuracy,\n",
    "        'Test Error Rate': 1 - accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "# Evaluate all three datasets\n",
    "datasets = {\n",
    "    'df_bin': df_bin,\n",
    "    'df_char_bin': df_char_bin,\n",
    "    'df_quality_bin': df_quality_bin\n",
    "}\n",
    "results = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    results[name] = evaluate_decision_tree(df)\n",
    "\n",
    "comparison_df = pd.DataFrame(results).transpose()\n",
    "\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Cross-Validation Accuracy Cross-Validation Error Rate  \\\n",
      "df_bin                          0.637255                    0.362745   \n",
      "df_char_bin                     0.333333                    0.666667   \n",
      "df_quality_bin                   0.35098                     0.64902   \n",
      "\n",
      "               Test Accuracy Test Error Rate Precision    Recall  F1 Score  \\\n",
      "df_bin              0.803922        0.196078  0.833787  0.803922  0.794862   \n",
      "df_char_bin         0.578431        0.421569  0.607732  0.578431  0.571409   \n",
      "df_quality_bin      0.431373        0.568627  0.453515  0.431373  0.432584   \n",
      "\n",
      "                                                 Confusion Matrix  \n",
      "df_bin          [[12, 1, 1, 0, 0], [0, 20, 0, 0, 0], [3, 2, 7,...  \n",
      "df_char_bin     [[5, 2, 3, 2, 2], [0, 15, 0, 5, 0], [0, 3, 6, ...  \n",
      "df_quality_bin  [[7, 0, 6, 0, 1], [0, 11, 0, 2, 7], [2, 2, 3, ...  \n"
     ]
    }
   ],
   "source": [
    "datasets = {\n",
    "    'df_bin': df_bin,\n",
    "    'df_char_bin': df_char_bin,\n",
    "    'df_quality_bin': df_quality_bin\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "metric_name = 'Euclidean'\n",
    "metric_func = lambda X_train, X_test: np.sqrt(((X_train.values[:, np.newaxis] - X_test.values) ** 2).sum(axis=2))\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=33)\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    for col in X:\n",
    "        X[col] = label_encoder.fit_transform(X[col])\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)  # Ensure y_encoded is 1D\n",
    "    \n",
    "    cv_accuracies = []\n",
    "\n",
    "    ros = RandomOverSampler(random_state=33)\n",
    "    smote = SMOTE(random_state=33)\n",
    "    pipeline = Pipeline([('ros', ros), ('smote', smote)])\n",
    "\n",
    "    X_resampled, y_resampled = pipeline.fit_resample(X, y_encoded)\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_resampled):\n",
    "        X_train, X_test = X_resampled.iloc[train_index], X_resampled.iloc[test_index]\n",
    "        y_train, y_test = y_resampled[train_index], y_resampled[test_index]\n",
    "        \n",
    "        distances = metric_func(X_train, X_test)\n",
    "        \n",
    "        nearest_indices = np.argmin(distances, axis=0)\n",
    "        \n",
    "        predicted_labels = y_train[nearest_indices]\n",
    "        \n",
    "        cv_accuracy = accuracy_score(y_test, predicted_labels)\n",
    "        \n",
    "        cv_accuracies.append(cv_accuracy)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=33)\n",
    "    \n",
    "    distances = metric_func(X_train, X_test)\n",
    "    \n",
    "    nearest_indices = np.argmin(distances, axis=0)\n",
    "    \n",
    "    predicted_labels = y_train[nearest_indices]\n",
    "    \n",
    "    test_accuracy = accuracy_score(y_test, predicted_labels)\n",
    "    precision = precision_score(y_test, predicted_labels, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, predicted_labels, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, predicted_labels, average='weighted', zero_division=0)\n",
    "    confusion_matrix_result = confusion_matrix(y_test, predicted_labels)\n",
    "\n",
    "    results[name] = {\n",
    "        'Cross-Validation Accuracy': np.mean(cv_accuracies),\n",
    "        'Cross-Validation Error Rate': np.mean([1 - acc for acc in cv_accuracies]),\n",
    "        'Test Accuracy': test_accuracy,\n",
    "        'Test Error Rate': 1 - test_accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': confusion_matrix_result\n",
    "    }\n",
    "\n",
    "results_df = pd.DataFrame(results).transpose()\n",
    "\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Cross-Validation Accuracy Cross-Validation Error Rate  \\\n",
      "df_bin                          0.206199                    0.793801   \n",
      "df_char_bin                     0.193499                    0.806501   \n",
      "df_quality_bin                  0.196772                    0.803228   \n",
      "\n",
      "               Test Accuracy Test Error Rate Precision    Recall  F1 Score  \\\n",
      "df_bin              0.222222        0.777778  0.262511  0.222222  0.233824   \n",
      "df_char_bin         0.196078        0.803922  0.206998  0.196078  0.190664   \n",
      "df_quality_bin      0.166667        0.833333  0.170434  0.166667  0.161078   \n",
      "\n",
      "                                                 Confusion Matrix  \n",
      "df_bin          [[3, 2, 6, 2, 0], [6, 4, 4, 3, 3], [3, 4, 2, 3...  \n",
      "df_char_bin     [[3, 2, 3, 2, 4], [5, 1, 4, 3, 7], [3, 3, 6, 1...  \n",
      "df_quality_bin  [[4, 0, 4, 1, 5], [4, 2, 7, 3, 4], [2, 0, 4, 4...  \n"
     ]
    }
   ],
   "source": [
    "def unify_features(test_sample, train_data, train_labels):\n",
    "    test_features = test_sample.values\n",
    "    matching_indices = np.all(train_data == test_features, axis=1)\n",
    "    if matching_indices.any():\n",
    "        matching_labels = train_labels[matching_indices]\n",
    "        return np.unique(matching_labels)\n",
    "    return [\"unknown\"]\n",
    "\n",
    "def evaluate_unification(df, random_state=33):\n",
    "    df = df.copy()\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    for col in X:\n",
    "        X[col] = label_encoder.fit_transform(X[col])\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)  # Ensure y_encoded is 1D\n",
    "    \n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    cv_accuracies = []\n",
    "    test_accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    confusion_matrices = []\n",
    "    \n",
    "    ros = RandomOverSampler(random_state=33)\n",
    "\n",
    "    smote = SMOTE(random_state=33)\n",
    "\n",
    "    pipeline = Pipeline([('ros', ros), ('smote', smote)])\n",
    "\n",
    "    X_resampled, y_resampled = pipeline.fit_resample(X, y_encoded)\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_resampled):\n",
    "        X_train, X_test = X_resampled.iloc[train_index], X_resampled.iloc[test_index]\n",
    "        y_train, y_test = y_resampled[train_index], y_resampled[test_index]\n",
    "        \n",
    "        predicted_labels = []\n",
    "        for i in range(X_test.shape[0]):\n",
    "            label = unify_features(X_test.iloc[i], X_train, y_train)\n",
    "            predicted_labels.append(label)\n",
    "        \n",
    "        predicted_labels = [item for sublist in predicted_labels for item in sublist]\n",
    "        \n",
    "        valid_indices = [i for i, label in enumerate(predicted_labels) if label != \"unknown\" and i < len(y_test)]\n",
    "        if not valid_indices:\n",
    "            continue\n",
    "        \n",
    "        valid_predictions = np.array([predicted_labels[i] for i in valid_indices])\n",
    "\n",
    "        valid_truth = np.array([y_test[i] for i in valid_indices])\n",
    "        \n",
    "        cv_accuracy = accuracy_score(valid_truth, valid_predictions)\n",
    "       \n",
    "        cv_accuracies.append(cv_accuracy)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=33)\n",
    "    predicted_labels = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        label = unify_features(X_test.iloc[i], X_train, y_train)\n",
    "        predicted_labels.append(label)\n",
    "    predicted_labels = [item for sublist in predicted_labels for item in sublist]\n",
    "        \n",
    "    valid_indices = [i for i, label in enumerate(predicted_labels) if label != \"unknown\" and i < len(y_test)]\n",
    "        \n",
    "    valid_predictions = np.array([predicted_labels[i] for i in valid_indices])\n",
    "    valid_truth = np.array([y_test[i] for i in valid_indices])\n",
    "        \n",
    "    precision = precision_score(valid_truth, valid_predictions, average='weighted', zero_division=0)\n",
    "    recall = recall_score(valid_truth, valid_predictions, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(valid_truth, valid_predictions, average='weighted', zero_division=0)\n",
    "    confusion_matrix_result = confusion_matrix(valid_truth, valid_predictions)    \n",
    "    test_accuracy = accuracy_score(valid_truth, valid_predictions)\n",
    "\n",
    "    return {\n",
    "        'Cross-Validation Accuracy': np.mean(cv_accuracies),\n",
    "        'Cross-Validation Error Rate': np.mean([1 - acc for acc in cv_accuracies]),\n",
    "        'Test Accuracy': test_accuracy,\n",
    "        'Test Error Rate': 1 - test_accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': confusion_matrix_result\n",
    "    }\n",
    "\n",
    "\n",
    "datasets = {\n",
    "    'df_bin': df_bin,\n",
    "    'df_char_bin': df_char_bin,\n",
    "    'df_quality_bin': df_quality_bin\n",
    "}\n",
    "\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    results[name] = evaluate_unification(df)\n",
    "\n",
    "results_df = pd.DataFrame(results).transpose()\n",
    "\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalised = pd.read_csv(\"NormalisedDataset.csv\")\n",
    "df_normalised_quality = df_normalised[['Completeness_normalized', 'Conciseness_normalized',  'ClassImbRatio_normalized', 'ClassOverlapPerc_normalized',\n",
    "       'OutlierPerc_normalized',  'LabelIssuesPerc_normalized', 'FeatureAlgo_normalized']]\n",
    "df_normalised_char = df_normalised[[ 'cor.mean_normalized', 'cov.mean_normalized',\n",
    "       'eigenvalues.mean_normalized', 'g_mean.mean_normalized',\n",
    "       'h_mean.mean_normalized', 'iq_range.mean_normalized',\n",
    "       'kurtosis.mean_normalized', 'mad.mean_normalized',\n",
    "       'max.mean_normalized', 'mean.mean_normalized', 'median.mean_normalized',\n",
    "       'min.mean_normalized', 'nr_cor_attr_normalized', 'nr_norm_normalized',\n",
    "       'nr_outliers_normalized', 'range.mean_normalized', 'sd.mean_normalized',\n",
    "       'skewness.mean_normalized', 'sparsity.mean_normalized',\n",
    "       't_mean.mean_normalized', 'var.mean_normalized',\n",
    "        'attr_to_inst_normalized',\n",
    "       'inst_to_attr_normalized', 'nr_attr_normalized', 'nr_bin_normalized',\n",
    "       'nr_inst_normalized', 'nr_num_normalized', 'attr_conc.mean_normalized',\n",
    "       'attr_ent.mean_normalized', 'nUnique_normalized', 'ena_normalized', 'snr.mean_normalized',\n",
    "       'cEntropy_normalized', 'FeatureAlgo_normalized']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Cross-Validation Accuracy Cross-Validation Error Rate  \\\n",
      "df_normalised                          0.730866                    0.269134   \n",
      "df_normalised_char                     0.644435                    0.355565   \n",
      "df_normalised_quality                  0.442312                    0.557688   \n",
      "\n",
      "                      Test Accuracy Test Error Rate Precision    Recall  \\\n",
      "df_normalised              0.759615        0.240385  0.784856  0.759615   \n",
      "df_normalised_char         0.682692        0.317308  0.744291  0.682692   \n",
      "df_normalised_quality      0.403846        0.596154  0.565061  0.403846   \n",
      "\n",
      "                       F1 Score  \\\n",
      "df_normalised          0.761207   \n",
      "df_normalised_char     0.688986   \n",
      "df_normalised_quality  0.388166   \n",
      "\n",
      "                                                        Confusion Matrix  \n",
      "df_normalised          [[12, 1, 3, 0, 0], [0, 13, 0, 0, 0], [5, 2, 10...  \n",
      "df_normalised_char     [[11, 2, 2, 0, 1], [0, 13, 0, 0, 0], [6, 3, 8,...  \n",
      "df_normalised_quality  [[7, 4, 1, 3, 1], [3, 10, 0, 0, 0], [5, 4, 6, ...  \n"
     ]
    }
   ],
   "source": [
    "def evaluate_knn(df, random_state=33):\n",
    "    df = df.copy()\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)  # Ensure y_encoded is 1D\n",
    "\n",
    "    ros = RandomOverSampler(random_state=33)\n",
    "    smote = SMOTE(random_state=33)\n",
    "\n",
    "    pipeline = Pipeline([('ros', ros), ('smote', smote)])\n",
    "\n",
    "    X_resampled, y_resampled = pipeline.fit_resample(X, y_encoded)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=33)\n",
    "\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "        \n",
    "    knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "    \n",
    "    cross_val_scores = cross_val_score(knn_classifier, X_train, y_train, cv=kf, scoring='accuracy')\n",
    "    error_rates = 1 - cross_val_scores\n",
    "    \n",
    "    knn_classifier.fit(X_train, y_train)\n",
    "    y_pred = knn_classifier.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'Cross-Validation Accuracy': cross_val_scores.mean(),\n",
    "        'Cross-Validation Error Rate': error_rates.mean(),\n",
    "        'Test Accuracy': accuracy,\n",
    "        'Test Error Rate': 1 - accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "datasets = {\n",
    "    'df_normalised': df_normalised,\n",
    "    'df_normalised_char': df_normalised_char,\n",
    "    'df_normalised_quality': df_normalised_quality\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    results[name] = evaluate_knn(df)\n",
    "\n",
    "comparison_df = pd.DataFrame(results).transpose()\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Cross-Validation Accuracy Cross-Validation Error Rate  \\\n",
      "df_normalised                          0.819707                    0.180293   \n",
      "df_normalised_char                     0.733133                    0.266867   \n",
      "df_normalised_quality                  0.550516                    0.449484   \n",
      "\n",
      "                      Test Accuracy Test Error Rate Precision    Recall  \\\n",
      "df_normalised              0.846154        0.153846  0.860741  0.846154   \n",
      "df_normalised_char         0.778846        0.221154  0.834188  0.778846   \n",
      "df_normalised_quality      0.490385        0.509615  0.687627  0.490385   \n",
      "\n",
      "                       F1 Score  \\\n",
      "df_normalised          0.842221   \n",
      "df_normalised_char     0.787035   \n",
      "df_normalised_quality  0.498323   \n",
      "\n",
      "                                                        Confusion Matrix  \n",
      "df_normalised          [[14, 1, 1, 0, 0], [0, 13, 0, 0, 0], [4, 1, 10...  \n",
      "df_normalised_char     [[13, 2, 1, 0, 0], [0, 13, 0, 0, 0], [5, 1, 11...  \n",
      "df_normalised_quality  [[9, 4, 0, 0, 3], [0, 13, 0, 0, 0], [6, 7, 4, ...  \n"
     ]
    }
   ],
   "source": [
    "#df_raw decision\n",
    "\n",
    "def evaluate_decision_tree(df, random_state=33):\n",
    "    df = df.copy()\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    \n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)  # Ensure y_encoded is 1D\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    ros = RandomOverSampler(random_state=33)\n",
    "\n",
    "    smote = SMOTE(random_state=33)\n",
    "\n",
    "    pipeline = Pipeline([('ros', ros), ('smote', smote)])\n",
    "\n",
    "    X_resampled, y_resampled = pipeline.fit_resample(X, y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=33)    \n",
    "    tree_classifier = DecisionTreeClassifier(random_state=random_state)\n",
    "    \n",
    "    cross_val_scores = cross_val_score(tree_classifier, X_train, y_train, cv=kf, scoring='accuracy')\n",
    "    error_rates = 1 - cross_val_scores\n",
    "    \n",
    "    tree_classifier.fit(X_train, y_train)\n",
    "    y_pred = tree_classifier.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'Cross-Validation Accuracy': cross_val_scores.mean(),\n",
    "        'Cross-Validation Error Rate': error_rates.mean(),\n",
    "        'Test Accuracy': accuracy,\n",
    "        'Test Error Rate': 1 - accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "datasets = {\n",
    "    'df_normalised': df_normalised,\n",
    "    'df_normalised_char': df_normalised_char,\n",
    "    'df_normalised_quality': df_normalised_quality\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    results[name] = evaluate_decision_tree(df)\n",
    "\n",
    "comparison_df = pd.DataFrame(results).transpose()\n",
    "\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Cross-Validation Accuracy Cross-Validation Error Rate  \\\n",
      "df_normalised                          0.728846                    0.271154   \n",
      "df_normalised_char                     0.630769                    0.369231   \n",
      "df_normalised_quality                  0.394231                    0.605769   \n",
      "\n",
      "                      Test Accuracy Test Error Rate Precision    Recall  \\\n",
      "df_normalised              0.769231        0.230769  0.777686  0.769231   \n",
      "df_normalised_char         0.798077        0.201923  0.820765  0.798077   \n",
      "df_normalised_quality      0.403846        0.596154  0.613018  0.403846   \n",
      "\n",
      "                       F1 Score  \\\n",
      "df_normalised          0.766673   \n",
      "df_normalised_char     0.798219   \n",
      "df_normalised_quality  0.407522   \n",
      "\n",
      "                                                        Confusion Matrix  \n",
      "df_normalised          [[14, 1, 1, 0, 0], [0, 10, 0, 3, 0], [5, 2, 9,...  \n",
      "df_normalised_char     [[11, 1, 3, 1, 0], [0, 10, 0, 3, 0], [2, 1, 13...  \n",
      "df_normalised_quality  [[8, 1, 7, 0, 0], [0, 2, 8, 3, 0], [4, 1, 12, ...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/d19125691/.local/lib/python3.6/site-packages/pandas/core/frame.py:4327: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  downcast=downcast,\n",
      "/home/d19125691/.local/lib/python3.6/site-packages/pandas/core/frame.py:4327: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  downcast=downcast,\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "\n",
    "datasets = {\n",
    "    'df_normalised': df_normalised,\n",
    "    'df_normalised_char': df_normalised_char,\n",
    "    'df_normalised_quality': df_normalised_quality\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "metric_name = 'Euclidean'\n",
    "metric_func = lambda X_train, X_test: np.sqrt(((X_train.values[:, np.newaxis] - X_test.values) ** 2).sum(axis=2))\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=33)\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    \n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)  # Ensure y_encoded is 1D\n",
    "    \n",
    "    cv_accuracies = []\n",
    "\n",
    "    ros = RandomOverSampler(random_state=33)\n",
    "    smote = SMOTE(random_state=33)\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('ros', ros),\n",
    "        ('smote', smote)\n",
    "    ])\n",
    "\n",
    "    X_resampled, y_resampled = pipeline.fit_resample(X, y_encoded)\n",
    "    \n",
    "    \n",
    "    for train_index, test_index in kf.split(X_resampled):\n",
    "        X_train, X_test = X_resampled.iloc[train_index], X_resampled.iloc[test_index]\n",
    "        y_train, y_test = y_resampled[train_index], y_resampled[test_index]\n",
    "        \n",
    "        distances = metric_func(X_train, X_test)\n",
    "        \n",
    "        nearest_indices = np.argmin(distances, axis=0)\n",
    "        \n",
    "        predicted_labels = y_train[nearest_indices]\n",
    "        \n",
    "        cv_accuracy = accuracy_score(y_test, predicted_labels)\n",
    "        \n",
    "        cv_accuracies.append(cv_accuracy)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=33)\n",
    "    \n",
    "    distances = metric_func(X_train, X_test)\n",
    "    \n",
    "    nearest_indices = np.argmin(distances, axis=0)\n",
    "    \n",
    "    predicted_labels = y_train[nearest_indices]\n",
    "    \n",
    "    test_accuracy = accuracy_score(y_test, predicted_labels)\n",
    "    precision = precision_score(y_test, predicted_labels, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, predicted_labels, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, predicted_labels, average='weighted', zero_division=0)\n",
    "    confusion_matrix_result = confusion_matrix(y_test, predicted_labels)\n",
    "\n",
    "    results[name] = {\n",
    "        'Cross-Validation Accuracy': np.mean(cv_accuracies),\n",
    "        'Cross-Validation Error Rate': np.mean([1 - acc for acc in cv_accuracies]),\n",
    "        'Test Accuracy': test_accuracy,\n",
    "        'Test Error Rate': 1 - test_accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': confusion_matrix_result\n",
    "    }\n",
    "\n",
    "results_df = pd.DataFrame(results).transpose()\n",
    "\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Cross-Validation Accuracy Cross-Validation Error Rate  \\\n",
      "df_normalised                          0.333918                    0.666082   \n",
      "df_normalised_char                     0.200077                    0.799923   \n",
      "df_normalised_quality                  0.187718                    0.812282   \n",
      "\n",
      "                      Test Accuracy Test Error Rate Precision    Recall  \\\n",
      "df_normalised                  0.25            0.75  0.259081      0.25   \n",
      "df_normalised_char              0.2             0.8  0.205202       0.2   \n",
      "df_normalised_quality      0.262136        0.737864  0.267688  0.262136   \n",
      "\n",
      "                       F1 Score  \\\n",
      "df_normalised          0.252381   \n",
      "df_normalised_char     0.201614   \n",
      "df_normalised_quality  0.263796   \n",
      "\n",
      "                                                        Confusion Matrix  \n",
      "df_normalised          [[3, 2, 3, 3, 3], [2, 3, 1, 2, 3], [3, 1, 4, 4...  \n",
      "df_normalised_char     [[3, 1, 2, 7, 2], [3, 3, 3, 1, 3], [4, 1, 1, 4...  \n",
      "df_normalised_quality  [[5, 3, 1, 2, 5], [2, 2, 3, 3, 3], [3, 2, 2, 6...  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def unify_features(test_sample, train_data, train_labels):\n",
    "    test_features = test_sample.values\n",
    "    matching_indices = np.all(train_data == test_features, axis=1)\n",
    "    if matching_indices.any():\n",
    "        matching_labels = train_labels[matching_indices]\n",
    "        return np.unique(matching_labels)\n",
    "    return [\"unknown\"]\n",
    "\n",
    "def evaluate_unification(df, random_state=33):\n",
    "    df = df.copy()\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "   \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)  # Ensure y_encoded is 1D\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    cv_accuracies = []\n",
    "    test_accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    confusion_matrices = []\n",
    "    \n",
    "    ros = RandomOverSampler(random_state=33)\n",
    "\n",
    "    smote = SMOTE(random_state=33)\n",
    "\n",
    "    pipeline = Pipeline([('ros', ros), ('smote', smote)])\n",
    "\n",
    "    X_resampled, y_resampled = pipeline.fit_resample(X, y_encoded)\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_resampled):\n",
    "        X_train, X_test = X_resampled.iloc[train_index], X_resampled.iloc[test_index]\n",
    "        y_train, y_test = y_resampled[train_index], y_resampled[test_index]\n",
    "        \n",
    "        predicted_labels = []\n",
    "        for i in range(X_test.shape[0]):\n",
    "            label = unify_features(X_test.iloc[i], X_train, y_train)\n",
    "            predicted_labels.append(label)\n",
    "        \n",
    "        predicted_labels = [item for sublist in predicted_labels for item in sublist]\n",
    "        \n",
    "        valid_indices = [i for i, label in enumerate(predicted_labels) if label != \"unknown\" and i < len(y_test)]\n",
    "        if not valid_indices:\n",
    "            continue\n",
    "        \n",
    "        valid_predictions = np.array([predicted_labels[i] for i in valid_indices])\n",
    "\n",
    "        valid_truth = np.array([y_test[i] for i in valid_indices])\n",
    "        \n",
    "        cv_accuracy = accuracy_score(valid_truth, valid_predictions)\n",
    "       \n",
    "        cv_accuracies.append(cv_accuracy)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=33)\n",
    "    predicted_labels = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        label = unify_features(X_test.iloc[i], X_train, y_train)\n",
    "        predicted_labels.append(label)\n",
    "    predicted_labels = [item for sublist in predicted_labels for item in sublist]\n",
    "        \n",
    "    valid_indices = [i for i, label in enumerate(predicted_labels) if label != \"unknown\" and i < len(y_test)]\n",
    "        \n",
    "    valid_predictions = np.array([predicted_labels[i] for i in valid_indices])\n",
    "    valid_truth = np.array([y_test[i] for i in valid_indices])\n",
    "        \n",
    "    precision = precision_score(valid_truth, valid_predictions, average='weighted', zero_division=0)\n",
    "    recall = recall_score(valid_truth, valid_predictions, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(valid_truth, valid_predictions, average='weighted', zero_division=0)\n",
    "    confusion_matrix_result = confusion_matrix(valid_truth, valid_predictions)    \n",
    "    test_accuracy = accuracy_score(valid_truth, valid_predictions)\n",
    "\n",
    "    return {\n",
    "        'Cross-Validation Accuracy': np.mean(cv_accuracies),\n",
    "        'Cross-Validation Error Rate': np.mean([1 - acc for acc in cv_accuracies]),\n",
    "        'Test Accuracy': test_accuracy,\n",
    "        'Test Error Rate': 1 - test_accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': confusion_matrix_result\n",
    "    }\n",
    "\n",
    "\n",
    "datasets = {\n",
    "    'df_normalised': df_normalised,\n",
    "    'df_normalised_char': df_normalised_char,\n",
    "    'df_normalised_quality': df_normalised_quality\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    results[name] = evaluate_unification(df)\n",
    "\n",
    "results_df = pd.DataFrame(results).transpose()\n",
    "\n",
    "print(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".deeplearningClass",
   "language": "python",
   "name": ".deeplearningclass"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "551cc31cf9c5f57e891734d9457a3bee3a72ddf11065444d1381168b3dccfbde"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
