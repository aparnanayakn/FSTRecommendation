{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#df1 = pd.read_csv(\"./optimalalphabeta/dearr_cont/dearr0.1_0.1.csv\")\n",
    "df1 = pd.read_csv(\"bordascores_top3.csv\")\n",
    "df2 = pd.read_csv(\"df_mfeatures.csv\")\n",
    "df3 = pd.read_csv(\"BinnedMetaFeatures.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['top3_labels'] = list(\n",
    "    map(list, zip(df1['Top1'], df1['Top2'], df1['Top3']))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chisquare    57\n",
       "GR           45\n",
       "fcbf         26\n",
       "relief       24\n",
       "MI           17\n",
       "cfs           2\n",
       "relieff       2\n",
       "Name: Top1, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df1['Top1'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1[['Dataset','top3_labels']]\n",
    "df1 = df1.rename(columns={'Dataset': 'File', 'top3_labels': 'FeatureAlgo'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = df1.merge(df2, on='File').merge(df3, on='File')\n",
    "merged_df.to_csv(\"KnowledgeBase.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"KnowledgeBase.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"KnowledgeBase.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char_raw = df[[ 'cor.mean', 'cov.mean', 'eigenvalues.mean', 'g_mean.mean', 'h_mean.mean', 'iq_range.mean', \n",
    " 'kurtosis.mean', 'mad.mean', 'max.mean', 'mean.mean', 'median.mean', 'min.mean', 'nr_cor_attr', 'nr_norm', 'nr_outliers', 'range.mean', 'sd.mean', 'skewness.mean', 'sparsity.mean',\n",
    " 't_mean.mean', 'var.mean', 'attr_to_inst', 'inst_to_attr', 'nr_attr', 'nr_bin', 'nr_inst', 'nr_num','attr_conc.mean', \n",
    " 'attr_ent.mean', 'nUnique', 'ena', 'snr.mean', 'cEntropy', ]]\n",
    "\n",
    "df_quality_raw = df[['Completeness', 'Conciseness', 'ClassImbRatio', 'ClassOverlapPerc', 'OutlierPerc', 'LabelIssuesPerc', 'FeatureAlgo']]\n",
    "\n",
    "df_raw = df[['Completeness', 'Conciseness', 'cor.mean', 'cov.mean', 'eigenvalues.mean', 'g_mean.mean', 'h_mean.mean', 'iq_range.mean', \n",
    " 'kurtosis.mean', 'mad.mean', 'max.mean', 'mean.mean', 'median.mean', 'min.mean', 'nr_cor_attr', 'nr_norm', 'nr_outliers', 'range.mean', 'sd.mean', 'skewness.mean', 'sparsity.mean',\n",
    " 't_mean.mean', 'var.mean', 'ClassImbRatio', 'ClassOverlapPerc', 'OutlierPerc', 'attr_to_inst', 'inst_to_attr', 'nr_attr', 'nr_bin', 'nr_inst', 'nr_num','attr_conc.mean', \n",
    " 'attr_ent.mean', 'LabelIssuesPerc','nUnique', 'ena', 'snr.mean', 'cEntropy', 'FeatureAlgo']]\n",
    "\n",
    "\n",
    "df_char_bin = df[['cor.mean_bins', 'cov.mean_bins',\n",
    "       'eigenvalues.mean_bins', 'g_mean.mean_bins', 'h_mean.mean_bins',\n",
    "       'iq_range.mean_bins', 'kurtosis.mean_bins', 'mad.mean_bins',\n",
    "       'max.mean_bins', 'mean.mean_bins', 'median.mean_bins', 'min.mean_bins',\n",
    "       'nr_cor_attr_bins', 'nr_norm_bins', 'nr_outliers_bins',\n",
    "       'range.mean_bins', 'sd.mean_bins', 'skewness.mean_bins',\n",
    "       'sparsity.mean_bins', 't_mean.mean_bins', 'var.mean_bins',\n",
    "       'attr_to_inst_bins', 'inst_to_attr_bins',\n",
    "       'nr_attr_bins', 'nr_inst_bins', 'nr_num_bins', 'nr_bin_bins',\n",
    "       'attr_conc.mean_bins', 'attr_ent.mean_bins', 'ena_bins',\n",
    "       'snr.mean_bins', 'cEntropy_bins','FeatureAlgo']]\n",
    "\n",
    "df_quality_bin = df[['Completeness_bins',\n",
    "       'Conciseness_bins', 'LabelIssues_bins', 'ClassImbRatio_bins', 'OutlierPerc_bins',\n",
    "       'ClassOverlapPerc_bins','FeatureAlgo']]\n",
    "\n",
    "df_bin = df[['Completeness_bins',\n",
    "       'Conciseness_bins', 'cor.mean_bins', 'cov.mean_bins',\n",
    "       'eigenvalues.mean_bins', 'g_mean.mean_bins', 'h_mean.mean_bins',\n",
    "       'iq_range.mean_bins', 'kurtosis.mean_bins', 'mad.mean_bins',\n",
    "       'max.mean_bins', 'mean.mean_bins', 'median.mean_bins', 'min.mean_bins',\n",
    "       'nr_cor_attr_bins', 'nr_norm_bins', 'nr_outliers_bins',\n",
    "       'range.mean_bins', 'sd.mean_bins', 'skewness.mean_bins',\n",
    "       'sparsity.mean_bins', 't_mean.mean_bins', 'var.mean_bins',\n",
    "       'LabelIssues_bins', 'ClassImbRatio_bins', 'OutlierPerc_bins',\n",
    "       'ClassOverlapPerc_bins', 'attr_to_inst_bins', 'inst_to_attr_bins',\n",
    "       'nr_attr_bins', 'nr_inst_bins', 'nr_num_bins', 'nr_bin_bins',\n",
    "       'attr_conc.mean_bins', 'attr_ent.mean_bins', 'ena_bins',\n",
    "       'snr.mean_bins', 'cEntropy_bins', 'FeatureAlgo']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAW DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      ['chisquare', 'relief', 'fcbf']\n",
      "1      ['chisquare', 'relief', 'fcbf']\n",
      "2        ['chisquare', 'relief', 'GR']\n",
      "3      ['chisquare', 'relief', 'fcbf']\n",
      "4      ['chisquare', 'relief', 'fcbf']\n",
      "                    ...               \n",
      "168        ['fcbf', 'chisquare', 'GR']\n",
      "169            ['relief', 'cfs', 'GR']\n",
      "170        ['chisquare', 'fcbf', 'GR']\n",
      "171            ['cfs', 'relief', 'GR']\n",
      "172              ['MI', 'fcbf', 'cfs']\n",
      "Name: FeatureAlgo, Length: 173, dtype: object\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y must have at least two dimensions for multi-output regression but has only one.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-155-ce874e708c4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiOutputClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/multioutput.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \"\"\"\n\u001b[0;32m--> 368\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/multioutput.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             raise ValueError(\"y must have at least two dimensions for \"\n\u001b[0m\u001b[1;32m    169\u001b[0m                              \"multi-output regression but has only one.\")\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: y must have at least two dimensions for multi-output regression but has only one."
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "df.fillna(0, inplace=True)\n",
    "    \n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "print(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "classifier = MultiOutputClassifier(LogisticRegression(max_iter=1000))\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "predictions = classifier.predict(X_test)\n",
    "\n",
    "accuracy = classifier.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4. 12.  8. 12. 10.]\n",
      " [ 8. 20. 11.  7.  9.]\n",
      " [14. 18.  3.  3.  6.]\n",
      " ...\n",
      " [10. 11.  9. 18. 10.]\n",
      " [ 8.  8.  7. 16. 11.]\n",
      " [ 1. 22. 12. 11.  2.]]\n"
     ]
    }
   ],
   "source": [
    "X, y = make_multilabel_classification(n_samples=1000, n_features=5, n_classes=4, n_labels=3, random_state=42)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[\"['GR', 'MI', 'cfs']\" \"['GR', 'MI', 'chisquare']\" \"['GR', 'MI', 'fcbf']\"\n",
      " \"['GR', 'MI', 'relief']\" \"['GR', 'MI', 'relieff']\"\n",
      " \"['GR', 'cfs', 'relieff']\" \"['GR', 'chisquare', 'MI']\"\n",
      " \"['GR', 'chisquare', 'cfs']\" \"['GR', 'chisquare', 'relief']\"\n",
      " \"['GR', 'fcbf', 'MI']\" \"['GR', 'fcbf', 'cfs']\" \"['GR', 'relief', 'MI']\"\n",
      " \"['GR', 'relief', 'cfs']\" \"['GR', 'relief', 'chisquare']\"\n",
      " \"['GR', 'relief', 'fcbf']\" \"['MI', 'GR', 'chisquare']\"\n",
      " \"['MI', 'GR', 'fcbf']\" \"['MI', 'GR', 'relieff']\" \"['MI', 'cfs', 'GR']\"\n",
      " \"['MI', 'cfs', 'relieff']\" \"['MI', 'chisquare', 'GR']\"\n",
      " \"['MI', 'fcbf', 'chisquare']\" \"['MI', 'fcbf', 'relief']\"\n",
      " \"['MI', 'mrmr', 'fcbf']\" \"['cfs', 'chisquare', 'MI']\"\n",
      " \"['cfs', 'relief', 'GR']\" \"['chisquare', 'GR', 'MI']\"\n",
      " \"['chisquare', 'GR', 'cfs']\" \"['chisquare', 'GR', 'fcbf']\"\n",
      " \"['chisquare', 'GR', 'relief']\" \"['chisquare', 'MI', 'GR']\"\n",
      " \"['chisquare', 'MI', 'fcbf']\" \"['chisquare', 'MI', 'relief']\"\n",
      " \"['chisquare', 'fcbf', 'GR']\" \"['chisquare', 'fcbf', 'MI']\"\n",
      " \"['chisquare', 'fcbf', 'cfs']\" \"['chisquare', 'fcbf', 'relief']\"\n",
      " \"['chisquare', 'mrmr', 'MI']\" \"['chisquare', 'mrmr', 'fcbf']\"\n",
      " \"['chisquare', 'relief', 'GR']\" \"['chisquare', 'relief', 'MI']\"\n",
      " \"['chisquare', 'relief', 'fcbf']\" \"['fcbf', 'GR', 'MI']\"\n",
      " \"['fcbf', 'GR', 'cfs']\" \"['fcbf', 'GR', 'relief']\" \"['fcbf', 'MI', 'GR']\"\n",
      " \"['fcbf', 'MI', 'cfs']\" \"['fcbf', 'MI', 'relief']\"\n",
      " \"['fcbf', 'cfs', 'relieff']\" \"['fcbf', 'chisquare', 'GR']\"\n",
      " \"['fcbf', 'chisquare', 'cfs']\" \"['fcbf', 'relieff', 'chisquare']\"\n",
      " \"['fcbf', 'relieff', 'relief']\" \"['relief', 'GR', 'cfs']\"\n",
      " \"['relief', 'GR', 'chisquare']\" \"['relief', 'MI', 'GR']\"\n",
      " \"['relief', 'MI', 'chisquare']\" \"['relief', 'MI', 'fcbf']\"\n",
      " \"['relief', 'cfs', 'GR']\" \"['relief', 'cfs', 'chisquare']\"\n",
      " \"['relief', 'cfs', 'fcbf']\" \"['relief', 'chisquare', 'GR']\"\n",
      " \"['relief', 'chisquare', 'MI']\" \"['relief', 'chisquare', 'fcbf']\"\n",
      " \"['relief', 'chisquare', 'relieff']\" \"['relief', 'fcbf', 'GR']\"\n",
      " \"['relief', 'fcbf', 'chisquare']\" \"['relieff', 'GR', 'multisurf']\"\n",
      " \"['relieff', 'MI', 'fcbf']\"]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'y_true' contains labels not in parameter 'labels'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-fa7a6e68732d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_knn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0mcomparison_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-147-fa7a6e68732d>\u001b[0m in \u001b[0;36mevaluate_knn\u001b[0;34m(df, random_state, top_k)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mknn_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     top_k_acc = top_k_accuracy_score(\n\u001b[0;32m---> 50\u001b[0;31m         y_test, y_pred_probs, k=top_k, labels = knn_classifier.classes_)\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mtop_k_accuracy_score\u001b[0;34m(y_true, y_score, k, normalize, sample_weight, labels)\u001b[0m\n\u001b[1;32m   1689\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdiff1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m             raise ValueError(\n\u001b[0;32m-> 1691\u001b[0;31m                 \u001b[0;34m\"'y_true' contains labels not in parameter 'labels'.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1692\u001b[0m             )\n\u001b[1;32m   1693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'y_true' contains labels not in parameter 'labels'."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    top_k_accuracy_score,\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# k-NN evaluation with Top-k accuracy\n",
    "def evaluate_knn(df, random_state=33, top_k=3):\n",
    "    df = df.copy()\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    \n",
    "   # label_encoder = LabelEncoder()\n",
    "   # y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "    \n",
    "    # Cross-validation on training data\n",
    "    cross_val_scores = cross_val_score(\n",
    "        knn_classifier, X_train, y_train, cv=kf, scoring='accuracy'\n",
    "    )\n",
    "    error_rates = 1 - cross_val_scores\n",
    "    \n",
    "    # Train final model\n",
    "    knn_classifier.fit(X_train, y_train)\n",
    "    y_pred = knn_classifier.predict(X_test)\n",
    "    \n",
    "    # Probabilities for top-k\n",
    "    y_pred_probs = knn_classifier.predict_proba(X_test)\n",
    "\n",
    "    top_k_acc = top_k_accuracy_score(\n",
    "        y_test, y_pred_probs, k=top_k, labels = knn_classifier.classes_)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'Cross-Validation Accuracy': cross_val_scores.mean(),\n",
    "        'Cross-Validation Error Rate': error_rates.mean(),\n",
    "        'Test Accuracy': accuracy,\n",
    "        'Test Error Rate': 1 - accuracy,\n",
    "        f'Top-{top_k} Accuracy': top_k_acc,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "datasets = {\n",
    "    'df_raw': df_raw,\n",
    "    'df_char_raw': df_char_raw,\n",
    "    'df_quality_raw': df_quality_raw\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    results[name] = evaluate_knn(df, top_k=3)\n",
    "\n",
    "comparison_df = pd.DataFrame(results).transpose()\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Cross-Validation Accuracy Cross-Validation Error Rate  \\\n",
      "df_raw                          0.398413                    0.601587   \n",
      "df_char_raw                     0.397884                    0.602116   \n",
      "df_quality_raw                   0.37672                     0.62328   \n",
      "\n",
      "               Test Accuracy Test Error Rate Precision    Recall  F1 Score  \\\n",
      "df_raw              0.485714        0.514286  0.455159  0.485714  0.469775   \n",
      "df_char_raw         0.485714        0.514286  0.456277  0.485714   0.46724   \n",
      "df_quality_raw           0.4             0.6  0.414314       0.4  0.401677   \n",
      "\n",
      "                                                 Confusion Matrix  \n",
      "df_raw          [[2, 0, 2, 4, 0], [1, 0, 1, 0, 0], [3, 0, 10, ...  \n",
      "df_char_raw     [[1, 1, 2, 4, 0], [0, 0, 2, 0, 0], [2, 0, 10, ...  \n",
      "df_quality_raw  [[2, 2, 3, 1, 0], [2, 0, 0, 0, 0], [3, 0, 9, 3...  \n"
     ]
    }
   ],
   "source": [
    "#decision\n",
    "\n",
    "def evaluate_decision_tree(df, random_state=33):\n",
    "    df = df.copy()\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    \n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)  \n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=random_state)\n",
    "    \n",
    "    tree_classifier = DecisionTreeClassifier(random_state=random_state)\n",
    "    \n",
    "    cross_val_scores = cross_val_score(tree_classifier, X_train, y_train, cv=kf, scoring='accuracy')\n",
    "    error_rates = 1 - cross_val_scores\n",
    "    \n",
    "    tree_classifier.fit(X_train, y_train)\n",
    "    y_pred = tree_classifier.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'Cross-Validation Accuracy': cross_val_scores.mean(),\n",
    "        'Cross-Validation Error Rate': error_rates.mean(),\n",
    "        'Test Accuracy': accuracy,\n",
    "        'Test Error Rate': 1 - accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "datasets = {\n",
    "    'df_raw': df_raw,\n",
    "    'df_char_raw': df_char_raw,\n",
    "    'df_quality_raw': df_quality_raw\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    results[name] = evaluate_decision_tree(df)\n",
    "\n",
    "comparison_df = pd.DataFrame(results).transpose()\n",
    "\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Cross-Validation Accuracy Cross-Validation Error Rate  \\\n",
      "df_raw                          0.421681                    0.578319   \n",
      "df_char_raw                     0.421681                    0.578319   \n",
      "df_quality_raw                  0.358151                    0.641849   \n",
      "\n",
      "               Test Accuracy Test Error Rate Precision    Recall  F1 Score  \\\n",
      "df_raw              0.485714        0.514286  0.462857  0.485714  0.473016   \n",
      "df_char_raw         0.485714        0.514286  0.462857  0.485714  0.473016   \n",
      "df_quality_raw      0.285714        0.714286  0.302198  0.285714  0.290476   \n",
      "\n",
      "                                                 Confusion Matrix  \n",
      "df_raw          [[3, 0, 2, 2, 1], [0, 0, 1, 1, 0], [3, 0, 9, 3...  \n",
      "df_char_raw     [[3, 0, 2, 2, 1], [0, 0, 1, 1, 0], [3, 0, 9, 3...  \n",
      "df_quality_raw  [[0, 3, 1, 2, 2], [0, 0, 1, 1, 0], [4, 1, 7, 2...  \n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "datasets = {\n",
    "    'df_raw': df_raw,\n",
    "    'df_char_raw': df_char_raw,\n",
    "    'df_quality_raw': df_quality_raw\n",
    "}\n",
    "\n",
    "metric_name = 'Euclidean'\n",
    "metric_func = lambda X_train, X_test: np.sqrt(((X_train.values[:, np.newaxis] - X_test.values) ** 2).sum(axis=2))\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=33)\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    cv_accuracies = []\n",
    "    test_accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    confusion_matrices = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y_encoded[train_index], y_encoded[test_index]\n",
    "        \n",
    "        distances = metric_func(X_train, X_test)\n",
    "        \n",
    "        nearest_indices = np.argmin(distances, axis=0)\n",
    "        \n",
    "        predicted_labels = y_train[nearest_indices]\n",
    "        \n",
    "        cv_accuracy = accuracy_score(y_test, predicted_labels)\n",
    "        cv_error_rate = 1 - cv_accuracy\n",
    "        \n",
    "        cv_accuracies.append(cv_accuracy)\n",
    "      \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=33)\n",
    "    \n",
    "    distances = metric_func(X_train, X_test)\n",
    "    nearest_indices = np.argmin(distances, axis=0)\n",
    "    nearest_labels = y_encoded[nearest_indices]\n",
    "    predicted_labels = y_train[nearest_indices] \n",
    "    precision = precision_score(y_test, predicted_labels, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, predicted_labels, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, predicted_labels, average='weighted', zero_division=0)\n",
    "    confusion_matrix_result = confusion_matrix(y_test, predicted_labels)\n",
    "        \n",
    "    test_accuracy =accuracy_score(y_test, predicted_labels)\n",
    "\n",
    "    results[name] = {\n",
    "        'Cross-Validation Accuracy': np.mean(cv_accuracies),\n",
    "        'Cross-Validation Error Rate': np.mean([1 - acc for acc in cv_accuracies]),\n",
    "        'Test Accuracy': (test_accuracy),\n",
    "        'Test Error Rate': (1 - test_accuracy),\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score':f1,\n",
    "        'Confusion Matrix': confusion_matrix_result\n",
    "    }\n",
    "\n",
    "results_df = pd.DataFrame(results).transpose()\n",
    "\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/d19125691/.local/lib/python3.6/site-packages/numpy/lib/function_base.py:393: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/home/d19125691/.local/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Cross-Validation Accuracy Cross-Validation Error Rate  \\\n",
      "df_raw                          0.333333                    0.666667   \n",
      "df_char_raw                     0.333333                    0.666667   \n",
      "df_quality_raw                  0.266667                    0.733333   \n",
      "\n",
      "               Test Accuracy Test Error Rate Precision    Recall  F1 Score  \\\n",
      "df_raw                   NaN             NaN         0         0         0   \n",
      "df_char_raw              NaN             NaN         0         0         0   \n",
      "df_quality_raw      0.333333        0.666667       0.5  0.333333  0.388889   \n",
      "\n",
      "                                                 Confusion Matrix  \n",
      "df_raw                                                         []  \n",
      "df_char_raw                                                    []  \n",
      "df_quality_raw  [[1, 0, 0, 0, 1], [0, 0, 0, 0, 0], [0, 1, 1, 0...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/d19125691/.local/lib/python3.6/site-packages/numpy/lib/function_base.py:393: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/home/d19125691/.local/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "def unify_features(test_sample, train_data, train_labels):\n",
    "    test_features = test_sample.values\n",
    "    matching_indices = np.all(train_data == test_features, axis=1)\n",
    "    if matching_indices.any():\n",
    "        matching_labels = train_labels[matching_indices]\n",
    "        return np.unique(matching_labels)\n",
    "    return [\"unknown\"]\n",
    "\n",
    "def evaluate_unification(df, random_state=33):\n",
    "    df = df.copy()\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    cv_accuracies = []\n",
    "    test_accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    confusion_matrices = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y_encoded[train_index], y_encoded[test_index]\n",
    "        \n",
    "        predicted_labels = []\n",
    "        for i in range(X_test.shape[0]):\n",
    "            label = unify_features(X_test.iloc[i], X_train, y_train)\n",
    "            predicted_labels.append(label)\n",
    "        \n",
    "        predicted_labels = [item for sublist in predicted_labels for item in sublist]\n",
    "        \n",
    "        valid_indices = [i for i, label in enumerate(predicted_labels) if label != \"unknown\" and i < len(y_test)]\n",
    "        \n",
    "        if not valid_indices:\n",
    "            continue\n",
    "        \n",
    "        valid_predictions = np.array([predicted_labels[i] for i in valid_indices])\n",
    "        valid_truth = np.array([y_test[i] for i in valid_indices])\n",
    "        \n",
    "        cv_accuracy = accuracy_score(valid_truth, valid_predictions)\n",
    "       \n",
    "        cv_accuracies.append(cv_accuracy)\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=2)\n",
    "\n",
    "    predicted_labels = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        label = unify_features(X_test.iloc[i], X_train, y_train)\n",
    "        predicted_labels.append(label)\n",
    "    predicted_labels = [item for sublist in predicted_labels for item in sublist]\n",
    "        \n",
    "    valid_indices = [i for i, label in enumerate(predicted_labels) if label != \"unknown\" and i < len(y_test)]\n",
    "        \n",
    "    valid_predictions = np.array([predicted_labels[i] for i in valid_indices])\n",
    "    valid_truth = np.array([y_test[i] for i in valid_indices])\n",
    "        \n",
    "    precision = precision_score(valid_truth, valid_predictions, average='weighted', zero_division=0)\n",
    "    recall = recall_score(valid_truth, valid_predictions, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(valid_truth, valid_predictions, average='weighted', zero_division=0)\n",
    "    confusion_matrix_result = confusion_matrix(valid_truth, valid_predictions)    \n",
    "    test_accuracy = accuracy_score(valid_truth, valid_predictions)\n",
    "\n",
    "    return {\n",
    "        'Cross-Validation Accuracy': np.mean(cv_accuracies),\n",
    "        'Cross-Validation Error Rate': np.mean([1 - acc for acc in cv_accuracies]),\n",
    "        'Test Accuracy': test_accuracy,\n",
    "        'Test Error Rate': 1 - test_accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': confusion_matrix_result\n",
    "    }\n",
    "\n",
    "datasets = {\n",
    "    'df_raw': df_raw,\n",
    "    'df_char_raw': df_char_raw,\n",
    "    'df_quality_raw': df_quality_raw\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    results[name] = evaluate_unification(df)\n",
    "\n",
    "results_df = pd.DataFrame(results).transpose()\n",
    "\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIN data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Cross-Validation Accuracy Cross-Validation Error Rate  \\\n",
      "df_bin                          0.312169                    0.687831   \n",
      "df_char_bin                     0.398942                    0.601058   \n",
      "df_quality_bin                  0.296825                    0.703175   \n",
      "\n",
      "               Test Accuracy Test Error Rate Precision    Recall  F1 Score  \\\n",
      "df_bin              0.314286        0.685714  0.334524  0.314286  0.304762   \n",
      "df_char_bin         0.342857        0.657143  0.370238  0.342857  0.335007   \n",
      "df_quality_bin      0.342857        0.657143  0.241558  0.342857  0.281633   \n",
      "\n",
      "                                                 Confusion Matrix  \n",
      "df_bin          [[5, 1, 7, 1, 0], [2, 0, 0, 0, 0], [6, 0, 4, 0...  \n",
      "df_char_bin     [[7, 0, 6, 1, 0], [2, 0, 0, 0, 0], [3, 3, 4, 0...  \n",
      "df_quality_bin  [[9, 0, 4, 1, 0], [2, 0, 0, 0, 0], [6, 1, 3, 0...  \n"
     ]
    }
   ],
   "source": [
    "def evaluate_knn(df, random_state=33):\n",
    "    df = df.copy()\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    for col in X:\n",
    "        X[col] = label_encoder.fit_transform(X[col])\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)  \n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=random_state)\n",
    "    \n",
    "    knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "    \n",
    "    cross_val_scores = cross_val_score(knn_classifier, X_train, y_train, cv=kf, scoring='accuracy')\n",
    "    error_rates = 1 - cross_val_scores\n",
    "    \n",
    "    knn_classifier.fit(X_train, y_train)\n",
    "    y_pred = knn_classifier.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'Cross-Validation Accuracy': cross_val_scores.mean(),\n",
    "        'Cross-Validation Error Rate': error_rates.mean(),\n",
    "        'Test Accuracy': accuracy,\n",
    "        'Test Error Rate': 1 - accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "datasets = {\n",
    "    'df_bin': df_bin,\n",
    "    'df_char_bin': df_char_bin,\n",
    "    'df_quality_bin': df_quality_bin\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    results[name] = evaluate_knn(df)\n",
    "\n",
    "comparison_df = pd.DataFrame(results).transpose()\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Cross-Validation Accuracy Cross-Validation Error Rate  \\\n",
      "df_bin                          0.296561                    0.703439   \n",
      "df_char_bin                     0.384392                    0.615608   \n",
      "df_quality_bin                  0.282275                    0.717725   \n",
      "\n",
      "               Test Accuracy Test Error Rate Precision    Recall  F1 Score  \\\n",
      "df_bin              0.285714        0.714286  0.293537  0.285714   0.28325   \n",
      "df_char_bin         0.342857        0.657143  0.253968  0.342857  0.285714   \n",
      "df_quality_bin      0.257143        0.742857  0.194748  0.257143  0.220773   \n",
      "\n",
      "                                                 Confusion Matrix  \n",
      "df_bin          [[4, 0, 7, 3, 0], [2, 0, 0, 0, 0], [4, 2, 4, 0...  \n",
      "df_char_bin     [[5, 0, 7, 2, 0], [2, 0, 0, 0, 0], [2, 0, 7, 0...  \n",
      "df_quality_bin  [[6, 0, 7, 1, 0], [2, 0, 0, 0, 0], [6, 1, 3, 0...  \n"
     ]
    }
   ],
   "source": [
    "def evaluate_decision_tree(df, random_state=33):\n",
    "    df = df.copy()\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    for col in X:\n",
    "        X[col] = label_encoder.fit_transform(X[col])\n",
    "        \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)  \n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=random_state)\n",
    "    \n",
    "    tree_classifier = DecisionTreeClassifier(random_state=random_state)\n",
    "    \n",
    "    cross_val_scores = cross_val_score(tree_classifier, X_train, y_train, cv=kf, scoring='accuracy')\n",
    "    error_rates = 1 - cross_val_scores\n",
    "    \n",
    "    tree_classifier.fit(X_train, y_train)\n",
    "    y_pred = tree_classifier.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'Cross-Validation Accuracy': cross_val_scores.mean(),\n",
    "        'Cross-Validation Error Rate': error_rates.mean(),\n",
    "        'Test Accuracy': accuracy,\n",
    "        'Test Error Rate': 1 - accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "datasets = {\n",
    "    'df_bin': df_bin,\n",
    "    'df_char_bin': df_char_bin,\n",
    "    'df_quality_bin': df_quality_bin\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    results[name] = evaluate_decision_tree(df)\n",
    "\n",
    "comparison_df = pd.DataFrame(results).transpose()\n",
    "\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Cross-Validation Accuracy Cross-Validation Error Rate  \\\n",
      "df_bin                          0.387395                    0.612605   \n",
      "df_char_bin                     0.468403                    0.531597   \n",
      "df_quality_bin                  0.380168                    0.619832   \n",
      "\n",
      "               Test Accuracy Test Error Rate Precision    Recall  F1 Score  \\\n",
      "df_bin              0.485714        0.514286  0.561429  0.485714  0.520809   \n",
      "df_char_bin         0.428571        0.571429  0.462585  0.428571  0.444555   \n",
      "df_quality_bin      0.514286        0.485714   0.47619  0.514286  0.490964   \n",
      "\n",
      "                                                 Confusion Matrix  \n",
      "df_bin          [[2, 1, 3, 0, 0], [0, 0, 0, 1, 0], [2, 2, 15, ...  \n",
      "df_char_bin     [[1, 0, 4, 0, 1], [0, 0, 1, 0, 0], [4, 0, 14, ...  \n",
      "df_quality_bin  [[0, 0, 5, 0, 1], [0, 0, 0, 0, 1], [1, 0, 17, ...  \n"
     ]
    }
   ],
   "source": [
    "datasets = {\n",
    "    'df_bin': df_bin,\n",
    "    'df_char_bin': df_char_bin,\n",
    "    'df_quality_bin': df_quality_bin\n",
    "}\n",
    "results = {}\n",
    "\n",
    "metric_name = 'Euclidean'\n",
    "metric_func = lambda X_train, X_test: np.sqrt(((X_train.values[:, np.newaxis] - X_test.values) ** 2).sum(axis=2))\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=33)\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    for col in X:\n",
    "        X[col] = label_encoder.fit_transform(X[col])\n",
    "        \n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    cv_accuracies = []\n",
    "    test_accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    confusion_matrices = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y_encoded[train_index], y_encoded[test_index]\n",
    "        \n",
    "        distances = metric_func(X_train, X_test)\n",
    "        \n",
    "        nearest_indices = np.argmin(distances, axis=0)\n",
    "        \n",
    "        predicted_labels = y_train[nearest_indices]\n",
    "        \n",
    "        cv_accuracy = accuracy_score(y_test, predicted_labels)\n",
    "        cv_error_rate = 1 - cv_accuracy\n",
    "        \n",
    "        cv_accuracies.append(cv_accuracy)\n",
    "      \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=33)\n",
    "    \n",
    "    distances = metric_func(X_train, X_test)\n",
    "    nearest_indices = np.argmin(distances, axis=0)\n",
    "    nearest_labels = y_encoded[nearest_indices]\n",
    "    predicted_labels = y_train[nearest_indices] \n",
    "    precision = precision_score(y_test, predicted_labels, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, predicted_labels, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, predicted_labels, average='weighted', zero_division=0)\n",
    "    confusion_matrix_result = confusion_matrix(y_test, predicted_labels)\n",
    "        \n",
    "    test_accuracy =accuracy_score(y_test, predicted_labels)\n",
    "\n",
    "    results[name] = {\n",
    "        'Cross-Validation Accuracy': np.mean(cv_accuracies),\n",
    "        'Cross-Validation Error Rate': np.mean([1 - acc for acc in cv_accuracies]),\n",
    "        'Test Accuracy': (test_accuracy),\n",
    "        'Test Error Rate': (1 - test_accuracy),\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score':f1,\n",
    "        'Confusion Matrix': confusion_matrix_result\n",
    "    }\n",
    "\n",
    "results_df = pd.DataFrame(results).transpose()\n",
    "\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Cross-Validation Accuracy Cross-Validation Error Rate  \\\n",
      "df_bin                          0.395833                    0.604167   \n",
      "df_char_bin                     0.309051                    0.690949   \n",
      "df_quality_bin                   0.30827                     0.69173   \n",
      "\n",
      "               Test Accuracy Test Error Rate Precision    Recall  F1 Score  \\\n",
      "df_bin              0.421053        0.578947  0.336842  0.421053  0.367222   \n",
      "df_char_bin         0.363636        0.636364  0.483838  0.363636   0.40035   \n",
      "df_quality_bin      0.205882        0.794118  0.370915  0.205882  0.251859   \n",
      "\n",
      "                                                 Confusion Matrix  \n",
      "df_bin          [[1, 0, 1, 0, 0], [0, 0, 1, 0, 0], [4, 0, 6, 1...  \n",
      "df_char_bin     [[1, 0, 2, 0, 0], [1, 0, 1, 0, 0], [6, 0, 7, 4...  \n",
      "df_quality_bin  [[1, 0, 1, 0, 2], [0, 0, 1, 1, 0], [4, 2, 5, 5...  \n"
     ]
    }
   ],
   "source": [
    "def unify_features(test_sample, train_data, train_labels):\n",
    "    test_features = test_sample.values\n",
    "    matching_indices = np.all(train_data == test_features, axis=1)\n",
    "    if matching_indices.any():\n",
    "        matching_labels = train_labels[matching_indices]\n",
    "        return np.unique(matching_labels)\n",
    "    return [\"unknown\"]\n",
    "\n",
    "def evaluate_unification(df, random_state=33):\n",
    "    df = df.copy()\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    cv_accuracies = []\n",
    "    test_accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    confusion_matrices = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y_encoded[train_index], y_encoded[test_index]\n",
    "        \n",
    "        predicted_labels = []\n",
    "        for i in range(X_test.shape[0]):\n",
    "            label = unify_features(X_test.iloc[i], X_train, y_train)\n",
    "            predicted_labels.append(label)\n",
    "        \n",
    "        predicted_labels = [item for sublist in predicted_labels for item in sublist]\n",
    "        \n",
    "        valid_indices = [i for i, label in enumerate(predicted_labels) if label != \"unknown\" and i < len(y_test)]\n",
    "        \n",
    "        if not valid_indices:\n",
    "            continue\n",
    "        \n",
    "        valid_predictions = np.array([predicted_labels[i] for i in valid_indices])\n",
    "        valid_truth = np.array([y_test[i] for i in valid_indices])\n",
    "        \n",
    "        cv_accuracy = accuracy_score(valid_truth, valid_predictions)\n",
    "       \n",
    "        cv_accuracies.append(cv_accuracy)\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=2)\n",
    "    predicted_labels = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        label = unify_features(X_test.iloc[i], X_train, y_train)\n",
    "        predicted_labels.append(label)\n",
    "    predicted_labels = [item for sublist in predicted_labels for item in sublist]\n",
    "        \n",
    "    valid_indices = [i for i, label in enumerate(predicted_labels) if label != \"unknown\" and i < len(y_test)]\n",
    "        \n",
    "    valid_predictions = np.array([predicted_labels[i] for i in valid_indices])\n",
    "    valid_truth = np.array([y_test[i] for i in valid_indices])\n",
    "        \n",
    "    precision = precision_score(valid_truth, valid_predictions, average='weighted', zero_division=0)\n",
    "    recall = recall_score(valid_truth, valid_predictions, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(valid_truth, valid_predictions, average='weighted', zero_division=0)\n",
    "    confusion_matrix_result = confusion_matrix(valid_truth, valid_predictions)    \n",
    "    test_accuracy = accuracy_score(valid_truth, valid_predictions)\n",
    "\n",
    "    return {\n",
    "        'Cross-Validation Accuracy': np.mean(cv_accuracies),\n",
    "        'Cross-Validation Error Rate': np.mean([1 - acc for acc in cv_accuracies]),\n",
    "        'Test Accuracy': test_accuracy,\n",
    "        'Test Error Rate': 1 - test_accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': confusion_matrix_result\n",
    "    }\n",
    "\n",
    "datasets = {\n",
    "    'df_bin': df_bin,\n",
    "    'df_char_bin': df_char_bin,\n",
    "    'df_quality_bin': df_quality_bin\n",
    "}\n",
    "results = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    results[name] = evaluate_unification(df)\n",
    "\n",
    "results_df = pd.DataFrame(results).transpose()\n",
    "\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_normalised = pd.read_csv(\"NormalisedDataset.csv\")\n",
    "df_normalised_quality = df_normalised[['Completeness_normalized', 'Conciseness_normalized',  'ClassImbRatio_normalized', 'ClassOverlapPerc_normalized',\n",
    "       'OutlierPerc_normalized',  'LabelIssuesPerc_normalized', 'FeatureAlgo_normalized']]\n",
    "df_normalised_char = df_normalised[[ 'cor.mean_normalized', 'cov.mean_normalized',\n",
    "       'eigenvalues.mean_normalized', 'g_mean.mean_normalized',\n",
    "       'h_mean.mean_normalized', 'iq_range.mean_normalized',\n",
    "       'kurtosis.mean_normalized', 'mad.mean_normalized',\n",
    "       'max.mean_normalized', 'mean.mean_normalized', 'median.mean_normalized',\n",
    "       'min.mean_normalized', 'nr_cor_attr_normalized', 'nr_norm_normalized',\n",
    "       'nr_outliers_normalized', 'range.mean_normalized', 'sd.mean_normalized',\n",
    "       'skewness.mean_normalized', 'sparsity.mean_normalized',\n",
    "       't_mean.mean_normalized', 'var.mean_normalized',\n",
    "        'attr_to_inst_normalized',\n",
    "       'inst_to_attr_normalized', 'nr_attr_normalized', 'nr_bin_normalized',\n",
    "       'nr_inst_normalized', 'nr_num_normalized', 'attr_conc.mean_normalized',\n",
    "       'attr_ent.mean_normalized', 'nUnique_normalized', 'ena_normalized', 'snr.mean_normalized',\n",
    "       'cEntropy_normalized', 'FeatureAlgo_normalized']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_normalised.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Cross-Validation Accuracy Cross-Validation Error Rate  \\\n",
      "df_normalised                          0.506878                    0.493122   \n",
      "df_normalised_char                      0.53545                     0.46455   \n",
      "df_normalised_quality                  0.543386                    0.456614   \n",
      "\n",
      "                      Test Accuracy Test Error Rate Precision    Recall  \\\n",
      "df_normalised              0.571429        0.428571  0.423963  0.571429   \n",
      "df_normalised_char              0.6             0.4  0.503061       0.6   \n",
      "df_normalised_quality      0.628571        0.371429  0.438095  0.628571   \n",
      "\n",
      "                       F1 Score  \\\n",
      "df_normalised          0.486772   \n",
      "df_normalised_char     0.546779   \n",
      "df_normalised_quality  0.516327   \n",
      "\n",
      "                                                        Confusion Matrix  \n",
      "df_normalised          [[0, 0, 7, 0, 0], [0, 0, 1, 0, 0], [3, 0, 20, ...  \n",
      "df_normalised_char     [[2, 0, 5, 0, 0], [0, 0, 1, 0, 0], [4, 0, 19, ...  \n",
      "df_normalised_quality  [[0, 0, 7, 0, 0], [1, 0, 0, 0, 0], [1, 0, 22, ...  \n"
     ]
    }
   ],
   "source": [
    "def evaluate_knn(df, random_state=33):\n",
    "    df = df.copy()\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)  \n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=random_state)\n",
    "    \n",
    "    knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "    \n",
    "    cross_val_scores = cross_val_score(knn_classifier, X_train, y_train, cv=kf, scoring='accuracy')\n",
    "    error_rates = 1 - cross_val_scores\n",
    "    \n",
    "    knn_classifier.fit(X_train, y_train)\n",
    "    y_pred = knn_classifier.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'Cross-Validation Accuracy': cross_val_scores.mean(),\n",
    "        'Cross-Validation Error Rate': error_rates.mean(),\n",
    "        'Test Accuracy': accuracy,\n",
    "        'Test Error Rate': 1 - accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "datasets = {\n",
    "    'df_normalised': df_normalised,\n",
    "    'df_normalised_char': df_normalised_char,\n",
    "    'df_normalised_quality': df_normalised_quality\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    results[name] = evaluate_knn(df)\n",
    "\n",
    "comparison_df = pd.DataFrame(results).transpose()\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Cross-Validation Accuracy Cross-Validation Error Rate  \\\n",
      "df_normalised                          0.464286                    0.535714   \n",
      "df_normalised_char                     0.441534                    0.558466   \n",
      "df_normalised_quality                  0.521164                    0.478836   \n",
      "\n",
      "                      Test Accuracy Test Error Rate Precision    Recall  \\\n",
      "df_normalised                   0.6             0.4  0.580952       0.6   \n",
      "df_normalised_char         0.628571        0.371429  0.574429  0.628571   \n",
      "df_normalised_quality      0.628571        0.371429      0.51  0.628571   \n",
      "\n",
      "                       F1 Score  \\\n",
      "df_normalised          0.589286   \n",
      "df_normalised_char     0.600238   \n",
      "df_normalised_quality  0.557118   \n",
      "\n",
      "                                                        Confusion Matrix  \n",
      "df_normalised          [[3, 1, 3, 0, 0], [0, 0, 0, 1, 0], [5, 0, 18, ...  \n",
      "df_normalised_char     [[3, 1, 3, 0, 0], [0, 0, 1, 0, 0], [3, 0, 19, ...  \n",
      "df_normalised_quality  [[1, 0, 6, 0, 0], [1, 0, 0, 0, 0], [2, 0, 21, ...  \n"
     ]
    }
   ],
   "source": [
    "def evaluate_decision_tree(df, random_state=33):\n",
    "    df = df.copy()\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)  \n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=random_state)\n",
    "    \n",
    "    tree_classifier = DecisionTreeClassifier(random_state=random_state)\n",
    "    \n",
    "    cross_val_scores = cross_val_score(tree_classifier, X_train, y_train, cv=kf, scoring='accuracy')\n",
    "    error_rates = 1 - cross_val_scores\n",
    "    \n",
    "    tree_classifier.fit(X_train, y_train)\n",
    "    y_pred = tree_classifier.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'Cross-Validation Accuracy': cross_val_scores.mean(),\n",
    "        'Cross-Validation Error Rate': error_rates.mean(),\n",
    "        'Test Accuracy': accuracy,\n",
    "        'Test Error Rate': 1 - accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "datasets = {\n",
    "    'df_normalised': df_normalised,\n",
    "    'df_normalised_char': df_normalised_char,\n",
    "    'df_normalised_quality': df_normalised_quality\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    results[name] = evaluate_decision_tree(df)\n",
    "\n",
    "comparison_df = pd.DataFrame(results).transpose()\n",
    "\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Cross-Validation Accuracy Cross-Validation Error Rate  \\\n",
      "df_normalised                          0.438992                    0.561008   \n",
      "df_normalised_char                     0.496807                    0.503193   \n",
      "df_normalised_quality                  0.467395                    0.532605   \n",
      "\n",
      "                      Test Accuracy Test Error Rate Precision    Recall  \\\n",
      "df_normalised              0.628571        0.371429  0.596296  0.628571   \n",
      "df_normalised_char         0.628571        0.371429  0.603077  0.628571   \n",
      "df_normalised_quality      0.428571        0.571429  0.428571  0.428571   \n",
      "\n",
      "                       F1 Score  \\\n",
      "df_normalised               0.6   \n",
      "df_normalised_char     0.610573   \n",
      "df_normalised_quality  0.428571   \n",
      "\n",
      "                                                        Confusion Matrix  \n",
      "df_normalised          [[1, 1, 5, 0, 0], [0, 0, 0, 1, 0], [1, 1, 20, ...  \n",
      "df_normalised_char     [[2, 1, 4, 0, 0], [0, 0, 1, 0, 0], [2, 1, 19, ...  \n",
      "df_normalised_quality  [[0, 0, 5, 1, 1], [0, 0, 0, 0, 1], [1, 0, 15, ...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/d19125691/.local/lib/python3.6/site-packages/pandas/core/frame.py:4327: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  downcast=downcast,\n",
      "/home/d19125691/.local/lib/python3.6/site-packages/pandas/core/frame.py:4327: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  downcast=downcast,\n"
     ]
    }
   ],
   "source": [
    "datasets = {\n",
    "    'df_normalised': df_normalised,\n",
    "    'df_normalised_char': df_normalised_char,\n",
    "    'df_normalised_quality': df_normalised_quality\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "metric_name = 'Euclidean'\n",
    "metric_func = lambda X_train, X_test: np.sqrt(((X_train.values[:, np.newaxis] - X_test.values) ** 2).sum(axis=2))\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=33)\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    cv_accuracies = []\n",
    "    test_accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    confusion_matrices = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y_encoded[train_index], y_encoded[test_index]\n",
    "        \n",
    "        distances = metric_func(X_train, X_test)\n",
    "        \n",
    "        nearest_indices = np.argmin(distances, axis=0)\n",
    "        \n",
    "        predicted_labels = y_train[nearest_indices]\n",
    "        \n",
    "        cv_accuracy = accuracy_score(y_test, predicted_labels)\n",
    "        cv_error_rate = 1 - cv_accuracy\n",
    "        \n",
    "        cv_accuracies.append(cv_accuracy)\n",
    "      \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=33)\n",
    "    \n",
    "    distances = metric_func(X_train, X_test)\n",
    "    nearest_indices = np.argmin(distances, axis=0)\n",
    "    nearest_labels = y_encoded[nearest_indices]\n",
    "    predicted_labels = y_train[nearest_indices] \n",
    "    precision = precision_score(y_test, predicted_labels, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, predicted_labels, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, predicted_labels, average='weighted', zero_division=0)\n",
    "    confusion_matrix_result = confusion_matrix(y_test, predicted_labels)\n",
    "        \n",
    "    test_accuracy =accuracy_score(y_test, predicted_labels)\n",
    "\n",
    "    results[name] = {\n",
    "        'Cross-Validation Accuracy': np.mean(cv_accuracies),\n",
    "        'Cross-Validation Error Rate': np.mean([1 - acc for acc in cv_accuracies]),\n",
    "        'Test Accuracy': (test_accuracy),\n",
    "        'Test Error Rate': (1 - test_accuracy),\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score':f1,\n",
    "        'Confusion Matrix': confusion_matrix_result\n",
    "    }\n",
    "\n",
    "results_df = pd.DataFrame(results).transpose()\n",
    "\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Cross-Validation Accuracy Cross-Validation Error Rate  \\\n",
      "df_normalised                          0.465325                    0.534675   \n",
      "df_normalised_char                     0.300733                    0.699267   \n",
      "df_normalised_quality                  0.236842                    0.763158   \n",
      "\n",
      "                      Test Accuracy Test Error Rate Precision    Recall  \\\n",
      "df_normalised              0.307692        0.692308  0.289377  0.307692   \n",
      "df_normalised_char         0.333333        0.666667  0.409722  0.333333   \n",
      "df_normalised_quality      0.242424        0.757576  0.443182  0.242424   \n",
      "\n",
      "                       F1 Score  \\\n",
      "df_normalised          0.284615   \n",
      "df_normalised_char     0.358674   \n",
      "df_normalised_quality  0.306138   \n",
      "\n",
      "                                                        Confusion Matrix  \n",
      "df_normalised          [[1, 0, 0, 0, 0], [0, 0, 1, 0, 0], [2, 0, 3, 2...  \n",
      "df_normalised_char     [[1, 0, 1, 1, 0], [0, 0, 1, 0, 0], [4, 0, 5, 0...  \n",
      "df_normalised_quality  [[1, 2, 1, 1, 0], [1, 0, 0, 0, 1], [2, 3, 7, 4...  \n"
     ]
    }
   ],
   "source": [
    "def unify_features(test_sample, train_data, train_labels):\n",
    "    test_features = test_sample.values\n",
    "    matching_indices = np.all(train_data == test_features, axis=1)\n",
    "    if matching_indices.any():\n",
    "        matching_labels = train_labels[matching_indices]\n",
    "        return np.unique(matching_labels)\n",
    "    return [\"unknown\"]\n",
    "\n",
    "def evaluate_unification(df, random_state=33):\n",
    "    df = df.copy()\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    cv_accuracies = []\n",
    "    test_accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    confusion_matrices = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y_encoded[train_index], y_encoded[test_index]\n",
    "        \n",
    "        predicted_labels = []\n",
    "        for i in range(X_test.shape[0]):\n",
    "            label = unify_features(X_test.iloc[i], X_train, y_train)\n",
    "            predicted_labels.append(label)\n",
    "        \n",
    "        predicted_labels = [item for sublist in predicted_labels for item in sublist]\n",
    "        \n",
    "        valid_indices = [i for i, label in enumerate(predicted_labels) if label != \"unknown\" and i < len(y_test)]\n",
    "        \n",
    "        if not valid_indices:\n",
    "            continue\n",
    "        \n",
    "        valid_predictions = np.array([predicted_labels[i] for i in valid_indices])\n",
    "        valid_truth = np.array([y_test[i] for i in valid_indices])\n",
    "        \n",
    "        cv_accuracy = accuracy_score(valid_truth, valid_predictions)\n",
    "       \n",
    "        cv_accuracies.append(cv_accuracy)\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=2)\n",
    "    predicted_labels = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        label = unify_features(X_test.iloc[i], X_train, y_train)\n",
    "        predicted_labels.append(label)\n",
    "    predicted_labels = [item for sublist in predicted_labels for item in sublist]\n",
    "        \n",
    "    valid_indices = [i for i, label in enumerate(predicted_labels) if label != \"unknown\" and i < len(y_test)]\n",
    "        \n",
    "    valid_predictions = np.array([predicted_labels[i] for i in valid_indices])\n",
    "    valid_truth = np.array([y_test[i] for i in valid_indices])\n",
    "        \n",
    "    precision = precision_score(valid_truth, valid_predictions, average='weighted', zero_division=0)\n",
    "    recall = recall_score(valid_truth, valid_predictions, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(valid_truth, valid_predictions, average='weighted', zero_division=0)\n",
    "    confusion_matrix_result = confusion_matrix(valid_truth, valid_predictions)    \n",
    "    test_accuracy = accuracy_score(valid_truth, valid_predictions)\n",
    "\n",
    "    return {\n",
    "        'Cross-Validation Accuracy': np.mean(cv_accuracies),\n",
    "        'Cross-Validation Error Rate': np.mean([1 - acc for acc in cv_accuracies]),\n",
    "        'Test Accuracy': test_accuracy,\n",
    "        'Test Error Rate': 1 - test_accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': confusion_matrix_result\n",
    "    }\n",
    "\n",
    "\n",
    "datasets = {\n",
    "    'df_normalised': df_normalised,\n",
    "    'df_normalised_char': df_normalised_char,\n",
    "    'df_normalised_quality': df_normalised_quality\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    results[name] = evaluate_unification(df)\n",
    "\n",
    "results_df = pd.DataFrame(results).transpose()\n",
    "\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".deeplearningClass",
   "language": "python",
   "name": ".deeplearningclass"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68ad885796f6f0b806db95b15cf2a015a244c9adabe8d6cffa3fc143090837a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
